{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part II: Logistic Regression using Gradient Descent [35 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df = pd.read_csv('penguins_preprocessed.csv')\n",
    "penguin_df = penguin_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>calorie requirement</th>\n",
       "      <th>average sleep duration</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>gender</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.828324</td>\n",
       "      <td>11</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375305</td>\n",
       "      <td>14</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996480</td>\n",
       "      <td>11</td>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.180070</td>\n",
       "      <td>8</td>\n",
       "      <td>0.487062</td>\n",
       "      <td>0.585514</td>\n",
       "      <td>0.436693</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.343894</td>\n",
       "      <td>8</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  island  calorie requirement  average sleep duration  \\\n",
       "0        0       2             0.828324                      11   \n",
       "1        0       2             0.375305                      14   \n",
       "2        0       2             0.996480                      11   \n",
       "3        0       2             0.180070                       8   \n",
       "4        0       2             0.343894                       8   \n",
       "\n",
       "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  gender  \\\n",
       "0        0.254545       0.666667           0.152542     0.291667       1   \n",
       "1        0.269091       0.511905           0.237288     0.305556       0   \n",
       "2        0.298182       0.583333           0.389831     0.152778       0   \n",
       "3        0.487062       0.585514           0.436693     0.375000       1   \n",
       "4        0.167273       0.738095           0.355932     0.208333       0   \n",
       "\n",
       "     year  \n",
       "0  2007.0  \n",
       "1  2007.0  \n",
       "2  2007.0  \n",
       "3  2007.0  \n",
       "4  2007.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguin_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = penguin_df[['gender']] #defining the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g','average sleep duration'] \n",
    "#selecting the feature columns as per analysis from correlation matrix\n",
    "X = penguin_df[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (344, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X', X.shape) #X is in the shape of ð‘ x ð‘‘ where ð‘ is number of data samples and ð‘‘ is number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y (344, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Y', Y.shape) #Y is in the shape of ð‘ x 1 where ð‘ is number of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_df = pd.concat([X, Y], axis=1) #combines feature matrix and target variable into a single data frame\n",
    "data_shuffled = pd.concat([X, Y], axis=1).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#shuffles the dataset and randomizes the order of rows to avoid ordering bias during training, making model more robust\n",
    "\n",
    "#Splitting the dataset into training and testing in the ratio of 80:20\n",
    "split_index = int(0.8 * len(data_shuffled))\n",
    "train_data = data_shuffled[:split_index]\n",
    "test_data = data_shuffled[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the features(X) and target(Y) for both the training and testing sets\n",
    "X_train = train_data[feature_columns]  #selecting feature columns from training set to get X_train\n",
    "Y_train = train_data[['gender']]     #selecting the 'gender' column for Y_train\n",
    "X_test = test_data[feature_columns]    #selecting feature columns from testing set to get X_test\n",
    "Y_test = test_data[['gender']]        #selecting the 'gender' column for Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (275, 5)\n",
      "Shape of Y_train: (275, 1)\n",
      "Shape of X_test: (69, 5)\n",
      "Shape of Y_test: (69, 1)\n"
     ]
    }
   ],
   "source": [
    "#Printing the shapes of the resulting datasets\n",
    "print(f\"Shape of X_train: {X_train.shape}\")  \n",
    "print(f\"Shape of Y_train: {Y_train.shape}\") \n",
    "print(f\"Shape of X_test: {X_test.shape}\")   \n",
    "print(f\"Shape of Y_test: {Y_test.shape}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting target vector into 1D array\n",
    "Y_train = Y_train.values.flatten()\n",
    "Y_test = Y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for logistic regression, it contains various functions like sigmoid, loss (cost) function, gradient descent etc\n",
    "class logistic_regression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=100000): #initializing values \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.losses = []\n",
    "\n",
    "    #Sigmoid function - ouput interpreted as a probability in binary classification tasks like logistic regression\n",
    "    def sigmoid(self, z): #z = w^T * X + b, z is the linear combination of feature vector and model parameters(weights and bias)\n",
    "        return 1 / (1 + np.exp(-z)) #function maps real number in the range of (0,1)\n",
    "\n",
    "    #Cost/Loss function (binary cross-entropy loss)\n",
    "    def cost(self, y, y_hat):\n",
    "        m = y.shape[0] #determines number of samples in the dataset\n",
    "        return - (1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "    #Gradient Descent - optimizes values of weights and bias during the training process\n",
    "    def gradient_descent(self, X, y, y_hat):\n",
    "        m = X.shape[0]\n",
    "        dw = (1 / m) * np.dot(X.T, (y_hat - y))  #gradient of weights\n",
    "        db = (1 / m) * np.sum(y_hat - y)         #gradient of bias\n",
    "        return dw, db\n",
    "\n",
    "    #fit function to train the model\n",
    "    def fit(self, X, y):\n",
    "        #initializing values of weights and bias\n",
    "        n_features = X.shape[1] #will store the number of input features\n",
    "        self.weights = np.zeros(n_features) #weights assigned as an array of zeroes\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            #Calculating the value of z = w^T * X + b, this will be the input to the sigmoid function\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_hat = self.sigmoid(z)\n",
    "\n",
    "            #computing the loss\n",
    "            loss = self.cost(y, y_hat)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            #performing gradient descent to find out values of dw, db\n",
    "            dw, db = self.gradient_descent(X, y, y_hat)\n",
    "\n",
    "            #updating the weight and bias terms on the basis of the output\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            if i % 100 == 0: #prints the value of loss after every iteration\n",
    "                print(f\"Iteration {i}: Loss = {loss}\")\n",
    "\n",
    "    #predict function to return the binary result\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_hat = self.sigmoid(z)\n",
    "        y_pred = [1 if i >= 0.5 else 0 for i in y_hat]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate, iterations):\n",
    "    model = logistic_regression(learning_rate, iterations)\n",
    "    model.fit(X_train, Y_train)\n",
    "    #performing prediction on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    #calculating the value of accuracy\n",
    "    accuracy = np.mean(y_pred == Y_test) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}% for learning rate {learning_rate} and iterations {iterations}\")\n",
    "    #prints the value of accuracy of the model along with its learning rate and number of iterations\n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the values of accuracy and best model\n",
    "best_accuracy = 0\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying 3 different hyperparameters, changing values of learning rate and iterations\n",
    "hyperparameters = [\n",
    "    (1e-3, 100000),\n",
    "    (1e-2, 50000),\n",
    "    (1e-4, 200000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.6931471805599453\n",
      "Iteration 100: Loss = 0.6896181010227871\n",
      "Iteration 200: Loss = 0.6891363818462609\n",
      "Iteration 300: Loss = 0.6886638721912874\n",
      "Iteration 400: Loss = 0.6881932038520613\n",
      "Iteration 500: Loss = 0.6877243488957715\n",
      "Iteration 600: Loss = 0.6872572974596172\n",
      "Iteration 700: Loss = 0.686792039774711\n",
      "Iteration 800: Loss = 0.6863285661220283\n",
      "Iteration 900: Loss = 0.6858668668322168\n",
      "Iteration 1000: Loss = 0.6854069322855018\n",
      "Iteration 1100: Loss = 0.6849487529115865\n",
      "Iteration 1200: Loss = 0.6844923191895532\n",
      "Iteration 1300: Loss = 0.6840376216477566\n",
      "Iteration 1400: Loss = 0.6835846508637182\n",
      "Iteration 1500: Loss = 0.6831333974640162\n",
      "Iteration 1600: Loss = 0.6826838521241734\n",
      "Iteration 1700: Loss = 0.6822360055685417\n",
      "Iteration 1800: Loss = 0.6817898485701855\n",
      "Iteration 1900: Loss = 0.6813453719507606\n",
      "Iteration 2000: Loss = 0.6809025665803926\n",
      "Iteration 2100: Loss = 0.6804614233775516\n",
      "Iteration 2200: Loss = 0.6800219333089248\n",
      "Iteration 2300: Loss = 0.6795840873892878\n",
      "Iteration 2400: Loss = 0.6791478766813723\n",
      "Iteration 2500: Loss = 0.6787132922957325\n",
      "Iteration 2600: Loss = 0.6782803253906092\n",
      "Iteration 2700: Loss = 0.6778489671717913\n",
      "Iteration 2800: Loss = 0.6774192088924766\n",
      "Iteration 2900: Loss = 0.6769910418531291\n",
      "Iteration 3000: Loss = 0.6765644574013353\n",
      "Iteration 3100: Loss = 0.6761394469316588\n",
      "Iteration 3200: Loss = 0.6757160018854923\n",
      "Iteration 3300: Loss = 0.6752941137509083\n",
      "Iteration 3400: Loss = 0.6748737740625085\n",
      "Iteration 3500: Loss = 0.6744549744012708\n",
      "Iteration 3600: Loss = 0.6740377063943945\n",
      "Iteration 3700: Loss = 0.6736219617151458\n",
      "Iteration 3800: Loss = 0.6732077320826989\n",
      "Iteration 3900: Loss = 0.6727950092619781\n",
      "Iteration 4000: Loss = 0.6723837850634972\n",
      "Iteration 4100: Loss = 0.6719740513431972\n",
      "Iteration 4200: Loss = 0.6715658000022846\n",
      "Iteration 4300: Loss = 0.6711590229870659\n",
      "Iteration 4400: Loss = 0.6707537122887821\n",
      "Iteration 4500: Loss = 0.6703498599434428\n",
      "Iteration 4600: Loss = 0.669947458031657\n",
      "Iteration 4700: Loss = 0.6695464986784646\n",
      "Iteration 4800: Loss = 0.6691469740531663\n",
      "Iteration 4900: Loss = 0.6687488763691518\n",
      "Iteration 5000: Loss = 0.668352197883728\n",
      "Iteration 5100: Loss = 0.6679569308979454\n",
      "Iteration 5200: Loss = 0.6675630677564237\n",
      "Iteration 5300: Loss = 0.6671706008471772\n",
      "Iteration 5400: Loss = 0.6667795226014382\n",
      "Iteration 5500: Loss = 0.6663898254934811\n",
      "Iteration 5600: Loss = 0.6660015020404435\n",
      "Iteration 5700: Loss = 0.6656145448021485\n",
      "Iteration 5800: Loss = 0.6652289463809257\n",
      "Iteration 5900: Loss = 0.6648446994214311\n",
      "Iteration 6000: Loss = 0.6644617966104668\n",
      "Iteration 6100: Loss = 0.6640802306767994\n",
      "Iteration 6200: Loss = 0.6636999943909795\n",
      "Iteration 6300: Loss = 0.6633210805651579\n",
      "Iteration 6400: Loss = 0.662943482052904\n",
      "Iteration 6500: Loss = 0.6625671917490219\n",
      "Iteration 6600: Loss = 0.6621922025893665\n",
      "Iteration 6700: Loss = 0.6618185075506602\n",
      "Iteration 6800: Loss = 0.6614460996503069\n",
      "Iteration 6900: Loss = 0.6610749719462085\n",
      "Iteration 7000: Loss = 0.6607051175365782\n",
      "Iteration 7100: Loss = 0.660336529559756\n",
      "Iteration 7200: Loss = 0.6599692011940214\n",
      "Iteration 7300: Loss = 0.6596031256574084\n",
      "Iteration 7400: Loss = 0.6592382962075181\n",
      "Iteration 7500: Loss = 0.6588747061413323\n",
      "Iteration 7600: Loss = 0.6585123487950271\n",
      "Iteration 7700: Loss = 0.6581512175437846\n",
      "Iteration 7800: Loss = 0.6577913058016065\n",
      "Iteration 7900: Loss = 0.6574326070211269\n",
      "Iteration 8000: Loss = 0.6570751146934241\n",
      "Iteration 8100: Loss = 0.6567188223478334\n",
      "Iteration 8200: Loss = 0.6563637235517599\n",
      "Iteration 8300: Loss = 0.6560098119104899\n",
      "Iteration 8400: Loss = 0.6556570810670042\n",
      "Iteration 8500: Loss = 0.65530552470179\n",
      "Iteration 8600: Loss = 0.6549551365326534\n",
      "Iteration 8700: Loss = 0.6546059103145316\n",
      "Iteration 8800: Loss = 0.6542578398393059\n",
      "Iteration 8900: Loss = 0.6539109189356137\n",
      "Iteration 9000: Loss = 0.6535651414686622\n",
      "Iteration 9100: Loss = 0.6532205013400397\n",
      "Iteration 9200: Loss = 0.6528769924875297\n",
      "Iteration 9300: Loss = 0.6525346088849245\n",
      "Iteration 9400: Loss = 0.652193344541837\n",
      "Iteration 9500: Loss = 0.6518531935035157\n",
      "Iteration 9600: Loss = 0.6515141498506573\n",
      "Iteration 9700: Loss = 0.6511762076992219\n",
      "Iteration 9800: Loss = 0.6508393612002459\n",
      "Iteration 9900: Loss = 0.6505036045396576\n",
      "Iteration 10000: Loss = 0.6501689319380913\n",
      "Iteration 10100: Loss = 0.6498353376507031\n",
      "Iteration 10200: Loss = 0.6495028159669854\n",
      "Iteration 10300: Loss = 0.6491713612105829\n",
      "Iteration 10400: Loss = 0.6488409677391095\n",
      "Iteration 10500: Loss = 0.6485116299439632\n",
      "Iteration 10600: Loss = 0.6481833422501442\n",
      "Iteration 10700: Loss = 0.6478560991160713\n",
      "Iteration 10800: Loss = 0.6475298950333997\n",
      "Iteration 10900: Loss = 0.6472047245268391\n",
      "Iteration 11000: Loss = 0.6468805821539716\n",
      "Iteration 11100: Loss = 0.646557462505071\n",
      "Iteration 11200: Loss = 0.6462353602029215\n",
      "Iteration 11300: Loss = 0.6459142699026379\n",
      "Iteration 11400: Loss = 0.6455941862914856\n",
      "Iteration 11500: Loss = 0.645275104088701\n",
      "Iteration 11600: Loss = 0.6449570180453129\n",
      "Iteration 11700: Loss = 0.6446399229439637\n",
      "Iteration 11800: Loss = 0.6443238135987318\n",
      "Iteration 11900: Loss = 0.6440086848549541\n",
      "Iteration 12000: Loss = 0.6436945315890494\n",
      "Iteration 12100: Loss = 0.6433813487083416\n",
      "Iteration 12200: Loss = 0.643069131150884\n",
      "Iteration 12300: Loss = 0.6427578738852848\n",
      "Iteration 12400: Loss = 0.6424475719105308\n",
      "Iteration 12500: Loss = 0.642138220255815\n",
      "Iteration 12600: Loss = 0.6418298139803618\n",
      "Iteration 12700: Loss = 0.6415223481732548\n",
      "Iteration 12800: Loss = 0.6412158179532639\n",
      "Iteration 12900: Loss = 0.6409102184686737\n",
      "Iteration 13000: Loss = 0.6406055448971125\n",
      "Iteration 13100: Loss = 0.6403017924453812\n",
      "Iteration 13200: Loss = 0.6399989563492836\n",
      "Iteration 13300: Loss = 0.6396970318734572\n",
      "Iteration 13400: Loss = 0.6393960143112042\n",
      "Iteration 13500: Loss = 0.6390958989843237\n",
      "Iteration 13600: Loss = 0.638796681242944\n",
      "Iteration 13700: Loss = 0.6384983564653551\n",
      "Iteration 13800: Loss = 0.6382009200578442\n",
      "Iteration 13900: Loss = 0.6379043674545288\n",
      "Iteration 14000: Loss = 0.637608694117192\n",
      "Iteration 14100: Loss = 0.637313895535119\n",
      "Iteration 14200: Loss = 0.6370199672249325\n",
      "Iteration 14300: Loss = 0.6367269047304309\n",
      "Iteration 14400: Loss = 0.6364347036224254\n",
      "Iteration 14500: Loss = 0.6361433594985787\n",
      "Iteration 14600: Loss = 0.6358528679832441\n",
      "Iteration 14700: Loss = 0.6355632247273058\n",
      "Iteration 14800: Loss = 0.6352744254080182\n",
      "Iteration 14900: Loss = 0.6349864657288488\n",
      "Iteration 15000: Loss = 0.634699341419319\n",
      "Iteration 15100: Loss = 0.6344130482348471\n",
      "Iteration 15200: Loss = 0.6341275819565915\n",
      "Iteration 15300: Loss = 0.6338429383912946\n",
      "Iteration 15400: Loss = 0.6335591133711281\n",
      "Iteration 15500: Loss = 0.6332761027535379\n",
      "Iteration 15600: Loss = 0.6329939024210901\n",
      "Iteration 15700: Loss = 0.632712508281319\n",
      "Iteration 15800: Loss = 0.6324319162665728\n",
      "Iteration 15900: Loss = 0.6321521223338639\n",
      "Iteration 16000: Loss = 0.6318731224647167\n",
      "Iteration 16100: Loss = 0.6315949126650172\n",
      "Iteration 16200: Loss = 0.6313174889648646\n",
      "Iteration 16300: Loss = 0.6310408474184213\n",
      "Iteration 16400: Loss = 0.630764984103765\n",
      "Iteration 16500: Loss = 0.6304898951227423\n",
      "Iteration 16600: Loss = 0.630215576600821\n",
      "Iteration 16700: Loss = 0.6299420246869444\n",
      "Iteration 16800: Loss = 0.6296692355533864\n",
      "Iteration 16900: Loss = 0.6293972053956068\n",
      "Iteration 17000: Loss = 0.6291259304321075\n",
      "Iteration 17100: Loss = 0.6288554069042901\n",
      "Iteration 17200: Loss = 0.6285856310763124\n",
      "Iteration 17300: Loss = 0.6283165992349481\n",
      "Iteration 17400: Loss = 0.6280483076894455\n",
      "Iteration 17500: Loss = 0.6277807527713867\n",
      "Iteration 17600: Loss = 0.62751393083455\n",
      "Iteration 17700: Loss = 0.6272478382547686\n",
      "Iteration 17800: Loss = 0.626982471429796\n",
      "Iteration 17900: Loss = 0.6267178267791658\n",
      "Iteration 18000: Loss = 0.626453900744057\n",
      "Iteration 18100: Loss = 0.6261906897871575\n",
      "Iteration 18200: Loss = 0.6259281903925301\n",
      "Iteration 18300: Loss = 0.6256663990654767\n",
      "Iteration 18400: Loss = 0.6254053123324056\n",
      "Iteration 18500: Loss = 0.6251449267406993\n",
      "Iteration 18600: Loss = 0.6248852388585809\n",
      "Iteration 18700: Loss = 0.6246262452749844\n",
      "Iteration 18800: Loss = 0.6243679425994229\n",
      "Iteration 18900: Loss = 0.6241103274618582\n",
      "Iteration 19000: Loss = 0.6238533965125733\n",
      "Iteration 19100: Loss = 0.6235971464220428\n",
      "Iteration 19200: Loss = 0.6233415738808048\n",
      "Iteration 19300: Loss = 0.6230866755993346\n",
      "Iteration 19400: Loss = 0.6228324483079183\n",
      "Iteration 19500: Loss = 0.622578888756526\n",
      "Iteration 19600: Loss = 0.6223259937146889\n",
      "Iteration 19700: Loss = 0.6220737599713733\n",
      "Iteration 19800: Loss = 0.6218221843348575\n",
      "Iteration 19900: Loss = 0.6215712636326098\n",
      "Iteration 20000: Loss = 0.6213209947111654\n",
      "Iteration 20100: Loss = 0.6210713744360057\n",
      "Iteration 20200: Loss = 0.6208223996914372\n",
      "Iteration 20300: Loss = 0.6205740673804713\n",
      "Iteration 20400: Loss = 0.6203263744247058\n",
      "Iteration 20500: Loss = 0.6200793177642053\n",
      "Iteration 20600: Loss = 0.6198328943573845\n",
      "Iteration 20700: Loss = 0.6195871011808893\n",
      "Iteration 20800: Loss = 0.6193419352294819\n",
      "Iteration 20900: Loss = 0.6190973935159239\n",
      "Iteration 21000: Loss = 0.6188534730708617\n",
      "Iteration 21100: Loss = 0.6186101709427116\n",
      "Iteration 21200: Loss = 0.6183674841975465\n",
      "Iteration 21300: Loss = 0.6181254099189821\n",
      "Iteration 21400: Loss = 0.617883945208065\n",
      "Iteration 21500: Loss = 0.617643087183161\n",
      "Iteration 21600: Loss = 0.6174028329798429\n",
      "Iteration 21700: Loss = 0.6171631797507818\n",
      "Iteration 21800: Loss = 0.6169241246656361\n",
      "Iteration 21900: Loss = 0.6166856649109422\n",
      "Iteration 22000: Loss = 0.6164477976900075\n",
      "Iteration 22100: Loss = 0.6162105202228009\n",
      "Iteration 22200: Loss = 0.6159738297458464\n",
      "Iteration 22300: Loss = 0.6157377235121173\n",
      "Iteration 22400: Loss = 0.615502198790929\n",
      "Iteration 22500: Loss = 0.6152672528678352\n",
      "Iteration 22600: Loss = 0.615032883044522\n",
      "Iteration 22700: Loss = 0.614799086638705\n",
      "Iteration 22800: Loss = 0.6145658609840257\n",
      "Iteration 22900: Loss = 0.6143332034299491\n",
      "Iteration 23000: Loss = 0.6141011113416612\n",
      "Iteration 23100: Loss = 0.6138695820999683\n",
      "Iteration 23200: Loss = 0.6136386131011958\n",
      "Iteration 23300: Loss = 0.6134082017570883\n",
      "Iteration 23400: Loss = 0.6131783454947101\n",
      "Iteration 23500: Loss = 0.6129490417563461\n",
      "Iteration 23600: Loss = 0.6127202879994046\n",
      "Iteration 23700: Loss = 0.6124920816963177\n",
      "Iteration 23800: Loss = 0.6122644203344467\n",
      "Iteration 23900: Loss = 0.6120373014159838\n",
      "Iteration 24000: Loss = 0.6118107224578571\n",
      "Iteration 24100: Loss = 0.6115846809916357\n",
      "Iteration 24200: Loss = 0.6113591745634341\n",
      "Iteration 24300: Loss = 0.6111342007338196\n",
      "Iteration 24400: Loss = 0.610909757077718\n",
      "Iteration 24500: Loss = 0.6106858411843207\n",
      "Iteration 24600: Loss = 0.6104624506569931\n",
      "Iteration 24700: Loss = 0.6102395831131827\n",
      "Iteration 24800: Loss = 0.6100172361843278\n",
      "Iteration 24900: Loss = 0.6097954075157674\n",
      "Iteration 25000: Loss = 0.6095740947666516\n",
      "Iteration 25100: Loss = 0.6093532956098515\n",
      "Iteration 25200: Loss = 0.6091330077318711\n",
      "Iteration 25300: Loss = 0.608913228832759\n",
      "Iteration 25400: Loss = 0.6086939566260204\n",
      "Iteration 25500: Loss = 0.6084751888385312\n",
      "Iteration 25600: Loss = 0.6082569232104502\n",
      "Iteration 25700: Loss = 0.6080391574951338\n",
      "Iteration 25800: Loss = 0.6078218894590511\n",
      "Iteration 25900: Loss = 0.6076051168816982\n",
      "Iteration 26000: Loss = 0.6073888375555142\n",
      "Iteration 26100: Loss = 0.6071730492857986\n",
      "Iteration 26200: Loss = 0.6069577498906262\n",
      "Iteration 26300: Loss = 0.606742937200766\n",
      "Iteration 26400: Loss = 0.6065286090595987\n",
      "Iteration 26500: Loss = 0.606314763323035\n",
      "Iteration 26600: Loss = 0.6061013978594346\n",
      "Iteration 26700: Loss = 0.6058885105495256\n",
      "Iteration 26800: Loss = 0.6056760992863253\n",
      "Iteration 26900: Loss = 0.6054641619750595\n",
      "Iteration 27000: Loss = 0.6052526965330847\n",
      "Iteration 27100: Loss = 0.6050417008898088\n",
      "Iteration 27200: Loss = 0.6048311729866146\n",
      "Iteration 27300: Loss = 0.6046211107767803\n",
      "Iteration 27400: Loss = 0.6044115122254049\n",
      "Iteration 27500: Loss = 0.60420237530933\n",
      "Iteration 27600: Loss = 0.6039936980170649\n",
      "Iteration 27700: Loss = 0.6037854783487114\n",
      "Iteration 27800: Loss = 0.6035777143158882\n",
      "Iteration 27900: Loss = 0.6033704039416571\n",
      "Iteration 28000: Loss = 0.6031635452604485\n",
      "Iteration 28100: Loss = 0.6029571363179891\n",
      "Iteration 28200: Loss = 0.6027511751712279\n",
      "Iteration 28300: Loss = 0.6025456598882643\n",
      "Iteration 28400: Loss = 0.6023405885482762\n",
      "Iteration 28500: Loss = 0.6021359592414481\n",
      "Iteration 28600: Loss = 0.6019317700689006\n",
      "Iteration 28700: Loss = 0.6017280191426199\n",
      "Iteration 28800: Loss = 0.601524704585387\n",
      "Iteration 28900: Loss = 0.6013218245307089\n",
      "Iteration 29000: Loss = 0.6011193771227493\n",
      "Iteration 29100: Loss = 0.6009173605162598\n",
      "Iteration 29200: Loss = 0.6007157728765116\n",
      "Iteration 29300: Loss = 0.6005146123792281\n",
      "Iteration 29400: Loss = 0.6003138772105171\n",
      "Iteration 29500: Loss = 0.6001135655668045\n",
      "Iteration 29600: Loss = 0.5999136756547674\n",
      "Iteration 29700: Loss = 0.5997142056912685\n",
      "Iteration 29800: Loss = 0.5995151539032904\n",
      "Iteration 29900: Loss = 0.5993165185278704\n",
      "Iteration 30000: Loss = 0.5991182978120362\n",
      "Iteration 30100: Loss = 0.5989204900127411\n",
      "Iteration 30200: Loss = 0.5987230933968011\n",
      "Iteration 30300: Loss = 0.5985261062408306\n",
      "Iteration 30400: Loss = 0.5983295268311802\n",
      "Iteration 30500: Loss = 0.5981333534638738\n",
      "Iteration 30600: Loss = 0.5979375844445458\n",
      "Iteration 30700: Loss = 0.5977422180883818\n",
      "Iteration 30800: Loss = 0.597547252720054\n",
      "Iteration 30900: Loss = 0.5973526866736635\n",
      "Iteration 31000: Loss = 0.5971585182926771\n",
      "Iteration 31100: Loss = 0.5969647459298699\n",
      "Iteration 31200: Loss = 0.5967713679472635\n",
      "Iteration 31300: Loss = 0.5965783827160679\n",
      "Iteration 31400: Loss = 0.5963857886166221\n",
      "Iteration 31500: Loss = 0.5961935840383363\n",
      "Iteration 31600: Loss = 0.5960017673796334\n",
      "Iteration 31700: Loss = 0.5958103370478917\n",
      "Iteration 31800: Loss = 0.5956192914593864\n",
      "Iteration 31900: Loss = 0.595428629039235\n",
      "Iteration 32000: Loss = 0.5952383482213386\n",
      "Iteration 32100: Loss = 0.595048447448327\n",
      "Iteration 32200: Loss = 0.5948589251715024\n",
      "Iteration 32300: Loss = 0.5946697798507846\n",
      "Iteration 32400: Loss = 0.5944810099546558\n",
      "Iteration 32500: Loss = 0.594292613960105\n",
      "Iteration 32600: Loss = 0.5941045903525759\n",
      "Iteration 32700: Loss = 0.5939169376259107\n",
      "Iteration 32800: Loss = 0.593729654282298\n",
      "Iteration 32900: Loss = 0.5935427388322193\n",
      "Iteration 33000: Loss = 0.5933561897943959\n",
      "Iteration 33100: Loss = 0.5931700056957372\n",
      "Iteration 33200: Loss = 0.5929841850712877\n",
      "Iteration 33300: Loss = 0.5927987264641759\n",
      "Iteration 33400: Loss = 0.5926136284255626\n",
      "Iteration 33500: Loss = 0.5924288895145903\n",
      "Iteration 33600: Loss = 0.5922445082983314\n",
      "Iteration 33700: Loss = 0.5920604833517397\n",
      "Iteration 33800: Loss = 0.5918768132575982\n",
      "Iteration 33900: Loss = 0.5916934966064716\n",
      "Iteration 34000: Loss = 0.5915105319966553\n",
      "Iteration 34100: Loss = 0.5913279180341269\n",
      "Iteration 34200: Loss = 0.5911456533324982\n",
      "Iteration 34300: Loss = 0.590963736512966\n",
      "Iteration 34400: Loss = 0.5907821662042644\n",
      "Iteration 34500: Loss = 0.5906009410426168\n",
      "Iteration 34600: Loss = 0.5904200596716894\n",
      "Iteration 34700: Loss = 0.5902395207425434\n",
      "Iteration 34800: Loss = 0.5900593229135885\n",
      "Iteration 34900: Loss = 0.589879464850536\n",
      "Iteration 35000: Loss = 0.5896999452263543\n",
      "Iteration 35100: Loss = 0.5895207627212204\n",
      "Iteration 35200: Loss = 0.5893419160224774\n",
      "Iteration 35300: Loss = 0.5891634038245877\n",
      "Iteration 35400: Loss = 0.5889852248290873\n",
      "Iteration 35500: Loss = 0.588807377744544\n",
      "Iteration 35600: Loss = 0.58862986128651\n",
      "Iteration 35700: Loss = 0.5884526741774806\n",
      "Iteration 35800: Loss = 0.5882758151468486\n",
      "Iteration 35900: Loss = 0.588099282930862\n",
      "Iteration 36000: Loss = 0.5879230762725808\n",
      "Iteration 36100: Loss = 0.5877471939218338\n",
      "Iteration 36200: Loss = 0.5875716346351766\n",
      "Iteration 36300: Loss = 0.5873963971758491\n",
      "Iteration 36400: Loss = 0.5872214803137337\n",
      "Iteration 36500: Loss = 0.5870468828253137\n",
      "Iteration 36600: Loss = 0.5868726034936318\n",
      "Iteration 36700: Loss = 0.5866986411082495\n",
      "Iteration 36800: Loss = 0.5865249944652055\n",
      "Iteration 36900: Loss = 0.5863516623669758\n",
      "Iteration 37000: Loss = 0.5861786436224334\n",
      "Iteration 37100: Loss = 0.5860059370468083\n",
      "Iteration 37200: Loss = 0.5858335414616477\n",
      "Iteration 37300: Loss = 0.5856614556947763\n",
      "Iteration 37400: Loss = 0.5854896785802584\n",
      "Iteration 37500: Loss = 0.5853182089583567\n",
      "Iteration 37600: Loss = 0.5851470456754962\n",
      "Iteration 37700: Loss = 0.5849761875842243\n",
      "Iteration 37800: Loss = 0.5848056335431728\n",
      "Iteration 37900: Loss = 0.5846353824170207\n",
      "Iteration 38000: Loss = 0.5844654330764559\n",
      "Iteration 38100: Loss = 0.5842957843981388\n",
      "Iteration 38200: Loss = 0.5841264352646633\n",
      "Iteration 38300: Loss = 0.5839573845645228\n",
      "Iteration 38400: Loss = 0.5837886311920709\n",
      "Iteration 38500: Loss = 0.5836201740474867\n",
      "Iteration 38600: Loss = 0.5834520120367384\n",
      "Iteration 38700: Loss = 0.5832841440715465\n",
      "Iteration 38800: Loss = 0.5831165690693498\n",
      "Iteration 38900: Loss = 0.5829492859532688\n",
      "Iteration 39000: Loss = 0.5827822936520707\n",
      "Iteration 39100: Loss = 0.5826155911001354\n",
      "Iteration 39200: Loss = 0.5824491772374195\n",
      "Iteration 39300: Loss = 0.5822830510094229\n",
      "Iteration 39400: Loss = 0.5821172113671544\n",
      "Iteration 39500: Loss = 0.5819516572670972\n",
      "Iteration 39600: Loss = 0.5817863876711761\n",
      "Iteration 39700: Loss = 0.5816214015467228\n",
      "Iteration 39800: Loss = 0.5814566978664439\n",
      "Iteration 39900: Loss = 0.5812922756083869\n",
      "Iteration 40000: Loss = 0.5811281337559079\n",
      "Iteration 40100: Loss = 0.5809642712976384\n",
      "Iteration 40200: Loss = 0.5808006872274537\n",
      "Iteration 40300: Loss = 0.5806373805444401\n",
      "Iteration 40400: Loss = 0.5804743502528632\n",
      "Iteration 40500: Loss = 0.5803115953621363\n",
      "Iteration 40600: Loss = 0.580149114886788\n",
      "Iteration 40700: Loss = 0.5799869078464325\n",
      "Iteration 40800: Loss = 0.5798249732657368\n",
      "Iteration 40900: Loss = 0.5796633101743909\n",
      "Iteration 41000: Loss = 0.5795019176070768\n",
      "Iteration 41100: Loss = 0.5793407946034377\n",
      "Iteration 41200: Loss = 0.5791799402080484\n",
      "Iteration 41300: Loss = 0.5790193534703842\n",
      "Iteration 41400: Loss = 0.5788590334447924\n",
      "Iteration 41500: Loss = 0.5786989791904614\n",
      "Iteration 41600: Loss = 0.5785391897713916\n",
      "Iteration 41700: Loss = 0.5783796642563667\n",
      "Iteration 41800: Loss = 0.5782204017189235\n",
      "Iteration 41900: Loss = 0.5780614012373243\n",
      "Iteration 42000: Loss = 0.5779026618945269\n",
      "Iteration 42100: Loss = 0.5777441827781573\n",
      "Iteration 42200: Loss = 0.5775859629804806\n",
      "Iteration 42300: Loss = 0.5774280015983729\n",
      "Iteration 42400: Loss = 0.5772702977332943\n",
      "Iteration 42500: Loss = 0.5771128504912595\n",
      "Iteration 42600: Loss = 0.5769556589828118\n",
      "Iteration 42700: Loss = 0.576798722322995\n",
      "Iteration 42800: Loss = 0.5766420396313264\n",
      "Iteration 42900: Loss = 0.5764856100317692\n",
      "Iteration 43000: Loss = 0.576329432652707\n",
      "Iteration 43100: Loss = 0.5761735066269152\n",
      "Iteration 43200: Loss = 0.5760178310915367\n",
      "Iteration 43300: Loss = 0.5758624051880542\n",
      "Iteration 43400: Loss = 0.5757072280622642\n",
      "Iteration 43500: Loss = 0.5755522988642515\n",
      "Iteration 43600: Loss = 0.575397616748363\n",
      "Iteration 43700: Loss = 0.5752431808731826\n",
      "Iteration 43800: Loss = 0.5750889904015053\n",
      "Iteration 43900: Loss = 0.5749350445003119\n",
      "Iteration 44000: Loss = 0.5747813423407444\n",
      "Iteration 44100: Loss = 0.5746278830980805\n",
      "Iteration 44200: Loss = 0.5744746659517095\n",
      "Iteration 44300: Loss = 0.5743216900851071\n",
      "Iteration 44400: Loss = 0.5741689546858109\n",
      "Iteration 44500: Loss = 0.574016458945397\n",
      "Iteration 44600: Loss = 0.5738642020594547\n",
      "Iteration 44700: Loss = 0.5737121832275633\n",
      "Iteration 44800: Loss = 0.5735604016532687\n",
      "Iteration 44900: Loss = 0.5734088565440586\n",
      "Iteration 45000: Loss = 0.5732575471113395\n",
      "Iteration 45100: Loss = 0.5731064725704141\n",
      "Iteration 45200: Loss = 0.5729556321404572\n",
      "Iteration 45300: Loss = 0.5728050250444932\n",
      "Iteration 45400: Loss = 0.5726546505093726\n",
      "Iteration 45500: Loss = 0.5725045077657505\n",
      "Iteration 45600: Loss = 0.5723545960480627\n",
      "Iteration 45700: Loss = 0.5722049145945043\n",
      "Iteration 45800: Loss = 0.5720554626470066\n",
      "Iteration 45900: Loss = 0.5719062394512155\n",
      "Iteration 46000: Loss = 0.5717572442564698\n",
      "Iteration 46100: Loss = 0.5716084763157786\n",
      "Iteration 46200: Loss = 0.5714599348858005\n",
      "Iteration 46300: Loss = 0.5713116192268206\n",
      "Iteration 46400: Loss = 0.5711635286027312\n",
      "Iteration 46500: Loss = 0.5710156622810091\n",
      "Iteration 46600: Loss = 0.5708680195326941\n",
      "Iteration 46700: Loss = 0.5707205996323698\n",
      "Iteration 46800: Loss = 0.570573401858141\n",
      "Iteration 46900: Loss = 0.570426425491614\n",
      "Iteration 47000: Loss = 0.5702796698178756\n",
      "Iteration 47100: Loss = 0.5701331341254728\n",
      "Iteration 47200: Loss = 0.5699868177063925\n",
      "Iteration 47300: Loss = 0.5698407198560412\n",
      "Iteration 47400: Loss = 0.5696948398732256\n",
      "Iteration 47500: Loss = 0.5695491770601315\n",
      "Iteration 47600: Loss = 0.5694037307223054\n",
      "Iteration 47700: Loss = 0.5692585001686338\n",
      "Iteration 47800: Loss = 0.569113484711324\n",
      "Iteration 47900: Loss = 0.5689686836658849\n",
      "Iteration 48000: Loss = 0.5688240963511082\n",
      "Iteration 48100: Loss = 0.5686797220890476\n",
      "Iteration 48200: Loss = 0.5685355602050017\n",
      "Iteration 48300: Loss = 0.5683916100274943\n",
      "Iteration 48400: Loss = 0.5682478708882551\n",
      "Iteration 48500: Loss = 0.5681043421222024\n",
      "Iteration 48600: Loss = 0.5679610230674234\n",
      "Iteration 48700: Loss = 0.5678179130651557\n",
      "Iteration 48800: Loss = 0.5676750114597706\n",
      "Iteration 48900: Loss = 0.5675323175987532\n",
      "Iteration 49000: Loss = 0.5673898308326851\n",
      "Iteration 49100: Loss = 0.5672475505152265\n",
      "Iteration 49200: Loss = 0.5671054760030986\n",
      "Iteration 49300: Loss = 0.5669636066560649\n",
      "Iteration 49400: Loss = 0.5668219418369151\n",
      "Iteration 49500: Loss = 0.5666804809114468\n",
      "Iteration 49600: Loss = 0.5665392232484479\n",
      "Iteration 49700: Loss = 0.5663981682196799\n",
      "Iteration 49800: Loss = 0.5662573151998607\n",
      "Iteration 49900: Loss = 0.5661166635666474\n",
      "Iteration 50000: Loss = 0.5659762127006196\n",
      "Iteration 50100: Loss = 0.5658359619852622\n",
      "Iteration 50200: Loss = 0.5656959108069496\n",
      "Iteration 50300: Loss = 0.5655560585549279\n",
      "Iteration 50400: Loss = 0.5654164046212996\n",
      "Iteration 50500: Loss = 0.5652769484010064\n",
      "Iteration 50600: Loss = 0.5651376892918134\n",
      "Iteration 50700: Loss = 0.5649986266942927\n",
      "Iteration 50800: Loss = 0.5648597600118077\n",
      "Iteration 50900: Loss = 0.5647210886504967\n",
      "Iteration 51000: Loss = 0.5645826120192574\n",
      "Iteration 51100: Loss = 0.5644443295297312\n",
      "Iteration 51200: Loss = 0.5643062405962871\n",
      "Iteration 51300: Loss = 0.5641683446360066\n",
      "Iteration 51400: Loss = 0.5640306410686686\n",
      "Iteration 51500: Loss = 0.5638931293167326\n",
      "Iteration 51600: Loss = 0.5637558088053257\n",
      "Iteration 51700: Loss = 0.5636186789622252\n",
      "Iteration 51800: Loss = 0.5634817392178451\n",
      "Iteration 51900: Loss = 0.5633449890052206\n",
      "Iteration 52000: Loss = 0.5632084277599931\n",
      "Iteration 52100: Loss = 0.5630720549203959\n",
      "Iteration 52200: Loss = 0.5629358699272394\n",
      "Iteration 52300: Loss = 0.5627998722238958\n",
      "Iteration 52400: Loss = 0.5626640612562858\n",
      "Iteration 52500: Loss = 0.5625284364728633\n",
      "Iteration 52600: Loss = 0.5623929973246018\n",
      "Iteration 52700: Loss = 0.5622577432649799\n",
      "Iteration 52800: Loss = 0.5621226737499665\n",
      "Iteration 52900: Loss = 0.5619877882380082\n",
      "Iteration 53000: Loss = 0.5618530861900143\n",
      "Iteration 53100: Loss = 0.5617185670693431\n",
      "Iteration 53200: Loss = 0.5615842303417887\n",
      "Iteration 53300: Loss = 0.5614500754755669\n",
      "Iteration 53400: Loss = 0.5613161019413014\n",
      "Iteration 53500: Loss = 0.5611823092120107\n",
      "Iteration 53600: Loss = 0.561048696763095\n",
      "Iteration 53700: Loss = 0.5609152640723216\n",
      "Iteration 53800: Loss = 0.5607820106198136\n",
      "Iteration 53900: Loss = 0.5606489358880344\n",
      "Iteration 54000: Loss = 0.5605160393617774\n",
      "Iteration 54100: Loss = 0.5603833205281504\n",
      "Iteration 54200: Loss = 0.5602507788765642\n",
      "Iteration 54300: Loss = 0.5601184138987195\n",
      "Iteration 54400: Loss = 0.559986225088594\n",
      "Iteration 54500: Loss = 0.5598542119424296\n",
      "Iteration 54600: Loss = 0.5597223739587205\n",
      "Iteration 54700: Loss = 0.5595907106381998\n",
      "Iteration 54800: Loss = 0.5594592214838273\n",
      "Iteration 54900: Loss = 0.5593279060007781\n",
      "Iteration 55000: Loss = 0.5591967636964291\n",
      "Iteration 55100: Loss = 0.5590657940803476\n",
      "Iteration 55200: Loss = 0.5589349966642783\n",
      "Iteration 55300: Loss = 0.5588043709621326\n",
      "Iteration 55400: Loss = 0.5586739164899752\n",
      "Iteration 55500: Loss = 0.5585436327660136\n",
      "Iteration 55600: Loss = 0.5584135193105852\n",
      "Iteration 55700: Loss = 0.5582835756461462\n",
      "Iteration 55800: Loss = 0.5581538012972592\n",
      "Iteration 55900: Loss = 0.5580241957905832\n",
      "Iteration 56000: Loss = 0.5578947586548596\n",
      "Iteration 56100: Loss = 0.5577654894209033\n",
      "Iteration 56200: Loss = 0.5576363876215897\n",
      "Iteration 56300: Loss = 0.557507452791844\n",
      "Iteration 56400: Loss = 0.5573786844686296\n",
      "Iteration 56500: Loss = 0.5572500821909377\n",
      "Iteration 56600: Loss = 0.5571216454997754\n",
      "Iteration 56700: Loss = 0.5569933739381552\n",
      "Iteration 56800: Loss = 0.5568652670510834\n",
      "Iteration 56900: Loss = 0.5567373243855501\n",
      "Iteration 57000: Loss = 0.5566095454905178\n",
      "Iteration 57100: Loss = 0.5564819299169106\n",
      "Iteration 57200: Loss = 0.5563544772176041\n",
      "Iteration 57300: Loss = 0.556227186947414\n",
      "Iteration 57400: Loss = 0.5561000586630861\n",
      "Iteration 57500: Loss = 0.5559730919232857\n",
      "Iteration 57600: Loss = 0.5558462862885871\n",
      "Iteration 57700: Loss = 0.5557196413214633\n",
      "Iteration 57800: Loss = 0.555593156586276\n",
      "Iteration 57900: Loss = 0.5554668316492641\n",
      "Iteration 58000: Loss = 0.5553406660785355\n",
      "Iteration 58100: Loss = 0.5552146594440558\n",
      "Iteration 58200: Loss = 0.5550888113176381\n",
      "Iteration 58300: Loss = 0.5549631212729337\n",
      "Iteration 58400: Loss = 0.5548375888854217\n",
      "Iteration 58500: Loss = 0.5547122137323994\n",
      "Iteration 58600: Loss = 0.5545869953929727\n",
      "Iteration 58700: Loss = 0.5544619334480456\n",
      "Iteration 58800: Loss = 0.5543370274803113\n",
      "Iteration 58900: Loss = 0.5542122770742424\n",
      "Iteration 59000: Loss = 0.5540876818160809\n",
      "Iteration 59100: Loss = 0.5539632412938297\n",
      "Iteration 59200: Loss = 0.5538389550972418\n",
      "Iteration 59300: Loss = 0.5537148228178121\n",
      "Iteration 59400: Loss = 0.5535908440487676\n",
      "Iteration 59500: Loss = 0.5534670183850582\n",
      "Iteration 59600: Loss = 0.5533433454233471\n",
      "Iteration 59700: Loss = 0.5532198247620026\n",
      "Iteration 59800: Loss = 0.5530964560010879\n",
      "Iteration 59900: Loss = 0.5529732387423529\n",
      "Iteration 60000: Loss = 0.5528501725892246\n",
      "Iteration 60100: Loss = 0.5527272571467989\n",
      "Iteration 60200: Loss = 0.5526044920218309\n",
      "Iteration 60300: Loss = 0.5524818768227269\n",
      "Iteration 60400: Loss = 0.5523594111595346\n",
      "Iteration 60500: Loss = 0.5522370946439354\n",
      "Iteration 60600: Loss = 0.5521149268892356\n",
      "Iteration 60700: Loss = 0.5519929075103569\n",
      "Iteration 60800: Loss = 0.551871036123829\n",
      "Iteration 60900: Loss = 0.5517493123477806\n",
      "Iteration 61000: Loss = 0.55162773580193\n",
      "Iteration 61100: Loss = 0.5515063061075791\n",
      "Iteration 61200: Loss = 0.5513850228876026\n",
      "Iteration 61300: Loss = 0.5512638857664408\n",
      "Iteration 61400: Loss = 0.5511428943700917\n",
      "Iteration 61500: Loss = 0.5510220483261017\n",
      "Iteration 61600: Loss = 0.5509013472635584\n",
      "Iteration 61700: Loss = 0.5507807908130824\n",
      "Iteration 61800: Loss = 0.5506603786068188\n",
      "Iteration 61900: Loss = 0.5505401102784294\n",
      "Iteration 62000: Loss = 0.5504199854630849\n",
      "Iteration 62100: Loss = 0.5503000037974571\n",
      "Iteration 62200: Loss = 0.5501801649197106\n",
      "Iteration 62300: Loss = 0.550060468469495\n",
      "Iteration 62400: Loss = 0.5499409140879378\n",
      "Iteration 62500: Loss = 0.5498215014176362\n",
      "Iteration 62600: Loss = 0.5497022301026491\n",
      "Iteration 62700: Loss = 0.5495830997884904\n",
      "Iteration 62800: Loss = 0.5494641101221204\n",
      "Iteration 62900: Loss = 0.549345260751939\n",
      "Iteration 63000: Loss = 0.5492265513277781\n",
      "Iteration 63100: Loss = 0.5491079815008935\n",
      "Iteration 63200: Loss = 0.5489895509239585\n",
      "Iteration 63300: Loss = 0.5488712592510562\n",
      "Iteration 63400: Loss = 0.5487531061376715\n",
      "Iteration 63500: Loss = 0.5486350912406849\n",
      "Iteration 63600: Loss = 0.5485172142183646\n",
      "Iteration 63700: Loss = 0.5483994747303594\n",
      "Iteration 63800: Loss = 0.5482818724376921\n",
      "Iteration 63900: Loss = 0.5481644070027512\n",
      "Iteration 64000: Loss = 0.5480470780892853\n",
      "Iteration 64100: Loss = 0.5479298853623954\n",
      "Iteration 64200: Loss = 0.5478128284885274\n",
      "Iteration 64300: Loss = 0.5476959071354663\n",
      "Iteration 64400: Loss = 0.5475791209723286\n",
      "Iteration 64500: Loss = 0.5474624696695554\n",
      "Iteration 64600: Loss = 0.5473459528989059\n",
      "Iteration 64700: Loss = 0.5472295703334507\n",
      "Iteration 64800: Loss = 0.5471133216475645\n",
      "Iteration 64900: Loss = 0.5469972065169207\n",
      "Iteration 65000: Loss = 0.546881224618483\n",
      "Iteration 65100: Loss = 0.5467653756304998\n",
      "Iteration 65200: Loss = 0.5466496592324978\n",
      "Iteration 65300: Loss = 0.5465340751052752\n",
      "Iteration 65400: Loss = 0.5464186229308954\n",
      "Iteration 65500: Loss = 0.5463033023926798\n",
      "Iteration 65600: Loss = 0.5461881131752022\n",
      "Iteration 65700: Loss = 0.5460730549642827\n",
      "Iteration 65800: Loss = 0.5459581274469799\n",
      "Iteration 65900: Loss = 0.5458433303115864\n",
      "Iteration 66000: Loss = 0.5457286632476211\n",
      "Iteration 66100: Loss = 0.5456141259458241\n",
      "Iteration 66200: Loss = 0.5454997180981495\n",
      "Iteration 66300: Loss = 0.5453854393977599\n",
      "Iteration 66400: Loss = 0.5452712895390202\n",
      "Iteration 66500: Loss = 0.5451572682174912\n",
      "Iteration 66600: Loss = 0.5450433751299244\n",
      "Iteration 66700: Loss = 0.5449296099742548\n",
      "Iteration 66800: Loss = 0.5448159724495953\n",
      "Iteration 66900: Loss = 0.544702462256232\n",
      "Iteration 67000: Loss = 0.5445890790956163\n",
      "Iteration 67100: Loss = 0.5444758226703603\n",
      "Iteration 67200: Loss = 0.5443626926842308\n",
      "Iteration 67300: Loss = 0.5442496888421433\n",
      "Iteration 67400: Loss = 0.5441368108501563\n",
      "Iteration 67500: Loss = 0.5440240584154657\n",
      "Iteration 67600: Loss = 0.5439114312463986\n",
      "Iteration 67700: Loss = 0.5437989290524086\n",
      "Iteration 67800: Loss = 0.5436865515440691\n",
      "Iteration 67900: Loss = 0.5435742984330681\n",
      "Iteration 68000: Loss = 0.5434621694322033\n",
      "Iteration 68100: Loss = 0.5433501642553754\n",
      "Iteration 68200: Loss = 0.5432382826175834\n",
      "Iteration 68300: Loss = 0.5431265242349191\n",
      "Iteration 68400: Loss = 0.5430148888245611\n",
      "Iteration 68500: Loss = 0.5429033761047699\n",
      "Iteration 68600: Loss = 0.5427919857948823\n",
      "Iteration 68700: Loss = 0.5426807176153065\n",
      "Iteration 68800: Loss = 0.542569571287516\n",
      "Iteration 68900: Loss = 0.5424585465340447\n",
      "Iteration 69000: Loss = 0.542347643078482\n",
      "Iteration 69100: Loss = 0.5422368606454674\n",
      "Iteration 69200: Loss = 0.5421261989606845\n",
      "Iteration 69300: Loss = 0.5420156577508569\n",
      "Iteration 69400: Loss = 0.5419052367437428\n",
      "Iteration 69500: Loss = 0.5417949356681294\n",
      "Iteration 69600: Loss = 0.5416847542538288\n",
      "Iteration 69700: Loss = 0.5415746922316714\n",
      "Iteration 69800: Loss = 0.541464749333503\n",
      "Iteration 69900: Loss = 0.5413549252921774\n",
      "Iteration 70000: Loss = 0.5412452198415537\n",
      "Iteration 70100: Loss = 0.54113563271649\n",
      "Iteration 70200: Loss = 0.5410261636528386\n",
      "Iteration 70300: Loss = 0.5409168123874415\n",
      "Iteration 70400: Loss = 0.5408075786581257\n",
      "Iteration 70500: Loss = 0.5406984622036978\n",
      "Iteration 70600: Loss = 0.5405894627639395\n",
      "Iteration 70700: Loss = 0.5404805800796031\n",
      "Iteration 70800: Loss = 0.5403718138924064\n",
      "Iteration 70900: Loss = 0.5402631639450282\n",
      "Iteration 71000: Loss = 0.5401546299811031\n",
      "Iteration 71100: Loss = 0.5400462117452183\n",
      "Iteration 71200: Loss = 0.5399379089829065\n",
      "Iteration 71300: Loss = 0.5398297214406436\n",
      "Iteration 71400: Loss = 0.5397216488658433\n",
      "Iteration 71500: Loss = 0.5396136910068522\n",
      "Iteration 71600: Loss = 0.5395058476129458\n",
      "Iteration 71700: Loss = 0.5393981184343238\n",
      "Iteration 71800: Loss = 0.5392905032221054\n",
      "Iteration 71900: Loss = 0.5391830017283248\n",
      "Iteration 72000: Loss = 0.5390756137059279\n",
      "Iteration 72100: Loss = 0.5389683389087664\n",
      "Iteration 72200: Loss = 0.5388611770915939\n",
      "Iteration 72300: Loss = 0.5387541280100622\n",
      "Iteration 72400: Loss = 0.5386471914207164\n",
      "Iteration 72500: Loss = 0.5385403670809902\n",
      "Iteration 72600: Loss = 0.5384336547492029\n",
      "Iteration 72700: Loss = 0.5383270541845536\n",
      "Iteration 72800: Loss = 0.5382205651471184\n",
      "Iteration 72900: Loss = 0.5381141873978448\n",
      "Iteration 73000: Loss = 0.5380079206985489\n",
      "Iteration 73100: Loss = 0.5379017648119101\n",
      "Iteration 73200: Loss = 0.5377957195014679\n",
      "Iteration 73300: Loss = 0.5376897845316171\n",
      "Iteration 73400: Loss = 0.5375839596676038\n",
      "Iteration 73500: Loss = 0.5374782446755216\n",
      "Iteration 73600: Loss = 0.5373726393223079\n",
      "Iteration 73700: Loss = 0.5372671433757384\n",
      "Iteration 73800: Loss = 0.5371617566044253\n",
      "Iteration 73900: Loss = 0.5370564787778116\n",
      "Iteration 74000: Loss = 0.5369513096661673\n",
      "Iteration 74100: Loss = 0.5368462490405868\n",
      "Iteration 74200: Loss = 0.5367412966729831\n",
      "Iteration 74300: Loss = 0.5366364523360857\n",
      "Iteration 74400: Loss = 0.5365317158034354\n",
      "Iteration 74500: Loss = 0.536427086849381\n",
      "Iteration 74600: Loss = 0.5363225652490758\n",
      "Iteration 74700: Loss = 0.5362181507784732\n",
      "Iteration 74800: Loss = 0.536113843214323\n",
      "Iteration 74900: Loss = 0.5360096423341679\n",
      "Iteration 75000: Loss = 0.5359055479163402\n",
      "Iteration 75100: Loss = 0.5358015597399567\n",
      "Iteration 75200: Loss = 0.5356976775849163\n",
      "Iteration 75300: Loss = 0.5355939012318961\n",
      "Iteration 75400: Loss = 0.5354902304623469\n",
      "Iteration 75500: Loss = 0.535386665058491\n",
      "Iteration 75600: Loss = 0.5352832048033168\n",
      "Iteration 75700: Loss = 0.5351798494805771\n",
      "Iteration 75800: Loss = 0.535076598874784\n",
      "Iteration 75900: Loss = 0.534973452771206\n",
      "Iteration 76000: Loss = 0.534870410955865\n",
      "Iteration 76100: Loss = 0.5347674732155312\n",
      "Iteration 76200: Loss = 0.5346646393377215\n",
      "Iteration 76300: Loss = 0.5345619091106945\n",
      "Iteration 76400: Loss = 0.5344592823234479\n",
      "Iteration 76500: Loss = 0.5343567587657148\n",
      "Iteration 76600: Loss = 0.5342543382279604\n",
      "Iteration 76700: Loss = 0.5341520205013783\n",
      "Iteration 76800: Loss = 0.5340498053778874\n",
      "Iteration 76900: Loss = 0.5339476926501284\n",
      "Iteration 77000: Loss = 0.5338456821114603\n",
      "Iteration 77100: Loss = 0.5337437735559578\n",
      "Iteration 77200: Loss = 0.5336419667784069\n",
      "Iteration 77300: Loss = 0.5335402615743021\n",
      "Iteration 77400: Loss = 0.533438657739844\n",
      "Iteration 77500: Loss = 0.5333371550719342\n",
      "Iteration 77600: Loss = 0.5332357533681739\n",
      "Iteration 77700: Loss = 0.5331344524268593\n",
      "Iteration 77800: Loss = 0.5330332520469793\n",
      "Iteration 77900: Loss = 0.5329321520282116\n",
      "Iteration 78000: Loss = 0.5328311521709204\n",
      "Iteration 78100: Loss = 0.5327302522761523\n",
      "Iteration 78200: Loss = 0.5326294521456341\n",
      "Iteration 78300: Loss = 0.5325287515817685\n",
      "Iteration 78400: Loss = 0.5324281503876325\n",
      "Iteration 78500: Loss = 0.5323276483669729\n",
      "Iteration 78600: Loss = 0.5322272453242041\n",
      "Iteration 78700: Loss = 0.5321269410644046\n",
      "Iteration 78800: Loss = 0.5320267353933146\n",
      "Iteration 78900: Loss = 0.531926628117332\n",
      "Iteration 79000: Loss = 0.5318266190435106\n",
      "Iteration 79100: Loss = 0.5317267079795556\n",
      "Iteration 79200: Loss = 0.5316268947338221\n",
      "Iteration 79300: Loss = 0.5315271791153111\n",
      "Iteration 79400: Loss = 0.5314275609336677\n",
      "Iteration 79500: Loss = 0.5313280399991768\n",
      "Iteration 79600: Loss = 0.531228616122761\n",
      "Iteration 79700: Loss = 0.5311292891159776\n",
      "Iteration 79800: Loss = 0.531030058791016\n",
      "Iteration 79900: Loss = 0.5309309249606938\n",
      "Iteration 80000: Loss = 0.5308318874384556\n",
      "Iteration 80100: Loss = 0.5307329460383688\n",
      "Iteration 80200: Loss = 0.5306341005751211\n",
      "Iteration 80300: Loss = 0.5305353508640183\n",
      "Iteration 80400: Loss = 0.5304366967209804\n",
      "Iteration 80500: Loss = 0.53033813796254\n",
      "Iteration 80600: Loss = 0.5302396744058392\n",
      "Iteration 80700: Loss = 0.530141305868626\n",
      "Iteration 80800: Loss = 0.530043032169253\n",
      "Iteration 80900: Loss = 0.5299448531266736\n",
      "Iteration 81000: Loss = 0.5298467685604398\n",
      "Iteration 81100: Loss = 0.5297487782906993\n",
      "Iteration 81200: Loss = 0.5296508821381928\n",
      "Iteration 81300: Loss = 0.5295530799242518\n",
      "Iteration 81400: Loss = 0.5294553714707956\n",
      "Iteration 81500: Loss = 0.529357756600328\n",
      "Iteration 81600: Loss = 0.5292602351359363\n",
      "Iteration 81700: Loss = 0.5291628069012875\n",
      "Iteration 81800: Loss = 0.5290654717206258\n",
      "Iteration 81900: Loss = 0.5289682294187703\n",
      "Iteration 82000: Loss = 0.5288710798211126\n",
      "Iteration 82100: Loss = 0.528774022753614\n",
      "Iteration 82200: Loss = 0.5286770580428025\n",
      "Iteration 82300: Loss = 0.5285801855157716\n",
      "Iteration 82400: Loss = 0.5284834050001762\n",
      "Iteration 82500: Loss = 0.5283867163242317\n",
      "Iteration 82600: Loss = 0.52829011931671\n",
      "Iteration 82700: Loss = 0.528193613806938\n",
      "Iteration 82800: Loss = 0.5280971996247954\n",
      "Iteration 82900: Loss = 0.528000876600711\n",
      "Iteration 83000: Loss = 0.5279046445656614\n",
      "Iteration 83100: Loss = 0.5278085033511684\n",
      "Iteration 83200: Loss = 0.5277124527892963\n",
      "Iteration 83300: Loss = 0.5276164927126493\n",
      "Iteration 83400: Loss = 0.5275206229543701\n",
      "Iteration 83500: Loss = 0.5274248433481361\n",
      "Iteration 83600: Loss = 0.5273291537281588\n",
      "Iteration 83700: Loss = 0.5272335539291799\n",
      "Iteration 83800: Loss = 0.5271380437864699\n",
      "Iteration 83900: Loss = 0.527042623135825\n",
      "Iteration 84000: Loss = 0.5269472918135658\n",
      "Iteration 84100: Loss = 0.5268520496565344\n",
      "Iteration 84200: Loss = 0.526756896502092\n",
      "Iteration 84300: Loss = 0.5266618321881171\n",
      "Iteration 84400: Loss = 0.5265668565530028\n",
      "Iteration 84500: Loss = 0.5264719694356549\n",
      "Iteration 84600: Loss = 0.5263771706754892\n",
      "Iteration 84700: Loss = 0.5262824601124302\n",
      "Iteration 84800: Loss = 0.5261878375869078\n",
      "Iteration 84900: Loss = 0.5260933029398558\n",
      "Iteration 85000: Loss = 0.5259988560127092\n",
      "Iteration 85100: Loss = 0.5259044966474029\n",
      "Iteration 85200: Loss = 0.5258102246863686\n",
      "Iteration 85300: Loss = 0.5257160399725331\n",
      "Iteration 85400: Loss = 0.5256219423493163\n",
      "Iteration 85500: Loss = 0.5255279316606284\n",
      "Iteration 85600: Loss = 0.5254340077508688\n",
      "Iteration 85700: Loss = 0.525340170464923\n",
      "Iteration 85800: Loss = 0.5252464196481612\n",
      "Iteration 85900: Loss = 0.5251527551464357\n",
      "Iteration 86000: Loss = 0.5250591768060793\n",
      "Iteration 86100: Loss = 0.5249656844739031\n",
      "Iteration 86200: Loss = 0.5248722779971943\n",
      "Iteration 86300: Loss = 0.5247789572237139\n",
      "Iteration 86400: Loss = 0.5246857220016955\n",
      "Iteration 86500: Loss = 0.5245925721798423\n",
      "Iteration 86600: Loss = 0.5244995076073261\n",
      "Iteration 86700: Loss = 0.5244065281337841\n",
      "Iteration 86800: Loss = 0.5243136336093182\n",
      "Iteration 86900: Loss = 0.5242208238844918\n",
      "Iteration 87000: Loss = 0.524128098810329\n",
      "Iteration 87100: Loss = 0.5240354582383118\n",
      "Iteration 87200: Loss = 0.5239429020203781\n",
      "Iteration 87300: Loss = 0.5238504300089205\n",
      "Iteration 87400: Loss = 0.5237580420567838\n",
      "Iteration 87500: Loss = 0.5236657380172631\n",
      "Iteration 87600: Loss = 0.5235735177441023\n",
      "Iteration 87700: Loss = 0.5234813810914916\n",
      "Iteration 87800: Loss = 0.5233893279140658\n",
      "Iteration 87900: Loss = 0.523297358066903\n",
      "Iteration 88000: Loss = 0.5232054714055221\n",
      "Iteration 88100: Loss = 0.5231136677858808\n",
      "Iteration 88200: Loss = 0.5230219470643742\n",
      "Iteration 88300: Loss = 0.5229303090978328\n",
      "Iteration 88400: Loss = 0.5228387537435211\n",
      "Iteration 88500: Loss = 0.5227472808591345\n",
      "Iteration 88600: Loss = 0.5226558903027988\n",
      "Iteration 88700: Loss = 0.522564581933068\n",
      "Iteration 88800: Loss = 0.5224733556089224\n",
      "Iteration 88900: Loss = 0.5223822111897662\n",
      "Iteration 89000: Loss = 0.5222911485354275\n",
      "Iteration 89100: Loss = 0.5222001675061542\n",
      "Iteration 89200: Loss = 0.5221092679626144\n",
      "Iteration 89300: Loss = 0.5220184497658926\n",
      "Iteration 89400: Loss = 0.5219277127774902\n",
      "Iteration 89500: Loss = 0.5218370568593218\n",
      "Iteration 89600: Loss = 0.5217464818737143\n",
      "Iteration 89700: Loss = 0.5216559876834053\n",
      "Iteration 89800: Loss = 0.5215655741515415\n",
      "Iteration 89900: Loss = 0.5214752411416763\n",
      "Iteration 90000: Loss = 0.5213849885177685\n",
      "Iteration 90100: Loss = 0.5212948161441809\n",
      "Iteration 90200: Loss = 0.5212047238856783\n",
      "Iteration 90300: Loss = 0.5211147116074261\n",
      "Iteration 90400: Loss = 0.5210247791749877\n",
      "Iteration 90500: Loss = 0.5209349264543247\n",
      "Iteration 90600: Loss = 0.5208451533117935\n",
      "Iteration 90700: Loss = 0.5207554596141442\n",
      "Iteration 90800: Loss = 0.5206658452285193\n",
      "Iteration 90900: Loss = 0.5205763100224522\n",
      "Iteration 91000: Loss = 0.5204868538638646\n",
      "Iteration 91100: Loss = 0.5203974766210661\n",
      "Iteration 91200: Loss = 0.5203081781627518\n",
      "Iteration 91300: Loss = 0.520218958358001\n",
      "Iteration 91400: Loss = 0.5201298170762759\n",
      "Iteration 91500: Loss = 0.5200407541874192\n",
      "Iteration 91600: Loss = 0.5199517695616537\n",
      "Iteration 91700: Loss = 0.5198628630695795\n",
      "Iteration 91800: Loss = 0.5197740345821735\n",
      "Iteration 91900: Loss = 0.5196852839707873\n",
      "Iteration 92000: Loss = 0.5195966111071457\n",
      "Iteration 92100: Loss = 0.5195080158633455\n",
      "Iteration 92200: Loss = 0.5194194981118535\n",
      "Iteration 92300: Loss = 0.5193310577255055\n",
      "Iteration 92400: Loss = 0.5192426945775043\n",
      "Iteration 92500: Loss = 0.5191544085414185\n",
      "Iteration 92600: Loss = 0.5190661994911812\n",
      "Iteration 92700: Loss = 0.5189780673010881\n",
      "Iteration 92800: Loss = 0.5188900118457962\n",
      "Iteration 92900: Loss = 0.5188020330003223\n",
      "Iteration 93000: Loss = 0.5187141306400418\n",
      "Iteration 93100: Loss = 0.5186263046406867\n",
      "Iteration 93200: Loss = 0.518538554878345\n",
      "Iteration 93300: Loss = 0.5184508812294581\n",
      "Iteration 93400: Loss = 0.5183632835708206\n",
      "Iteration 93500: Loss = 0.5182757617795779\n",
      "Iteration 93600: Loss = 0.5181883157332257\n",
      "Iteration 93700: Loss = 0.5181009453096073\n",
      "Iteration 93800: Loss = 0.5180136503869136\n",
      "Iteration 93900: Loss = 0.5179264308436812\n",
      "Iteration 94000: Loss = 0.51783928655879\n",
      "Iteration 94100: Loss = 0.5177522174114638\n",
      "Iteration 94200: Loss = 0.5176652232812672\n",
      "Iteration 94300: Loss = 0.5175783040481048\n",
      "Iteration 94400: Loss = 0.5174914595922205\n",
      "Iteration 94500: Loss = 0.517404689794195\n",
      "Iteration 94600: Loss = 0.517317994534945\n",
      "Iteration 94700: Loss = 0.5172313736957221\n",
      "Iteration 94800: Loss = 0.5171448271581112\n",
      "Iteration 94900: Loss = 0.5170583548040292\n",
      "Iteration 95000: Loss = 0.5169719565157236\n",
      "Iteration 95100: Loss = 0.5168856321757711\n",
      "Iteration 95200: Loss = 0.5167993816670768\n",
      "Iteration 95300: Loss = 0.5167132048728723\n",
      "Iteration 95400: Loss = 0.5166271016767147\n",
      "Iteration 95500: Loss = 0.5165410719624854\n",
      "Iteration 95600: Loss = 0.5164551156143882\n",
      "Iteration 95700: Loss = 0.5163692325169491\n",
      "Iteration 95800: Loss = 0.516283422555014\n",
      "Iteration 95900: Loss = 0.5161976856137483\n",
      "Iteration 96000: Loss = 0.5161120215786346\n",
      "Iteration 96100: Loss = 0.5160264303354724\n",
      "Iteration 96200: Loss = 0.5159409117703765\n",
      "Iteration 96300: Loss = 0.5158554657697755\n",
      "Iteration 96400: Loss = 0.515770092220411\n",
      "Iteration 96500: Loss = 0.5156847910093365\n",
      "Iteration 96600: Loss = 0.5155995620239152\n",
      "Iteration 96700: Loss = 0.5155144051518198\n",
      "Iteration 96800: Loss = 0.515429320281031\n",
      "Iteration 96900: Loss = 0.5153443072998362\n",
      "Iteration 97000: Loss = 0.5152593660968277\n",
      "Iteration 97100: Loss = 0.5151744965609031\n",
      "Iteration 97200: Loss = 0.5150896985812627\n",
      "Iteration 97300: Loss = 0.5150049720474086\n",
      "Iteration 97400: Loss = 0.514920316849144\n",
      "Iteration 97500: Loss = 0.514835732876571\n",
      "Iteration 97600: Loss = 0.5147512200200913\n",
      "Iteration 97700: Loss = 0.5146667781704026\n",
      "Iteration 97800: Loss = 0.5145824072184998\n",
      "Iteration 97900: Loss = 0.5144981070556723\n",
      "Iteration 98000: Loss = 0.5144138775735029\n",
      "Iteration 98100: Loss = 0.5143297186638676\n",
      "Iteration 98200: Loss = 0.5142456302189339\n",
      "Iteration 98300: Loss = 0.5141616121311597\n",
      "Iteration 98400: Loss = 0.514077664293292\n",
      "Iteration 98500: Loss = 0.5139937865983661\n",
      "Iteration 98600: Loss = 0.5139099789397046\n",
      "Iteration 98700: Loss = 0.5138262412109155\n",
      "Iteration 98800: Loss = 0.5137425733058919\n",
      "Iteration 98900: Loss = 0.5136589751188111\n",
      "Iteration 99000: Loss = 0.5135754465441326\n",
      "Iteration 99100: Loss = 0.513491987476597\n",
      "Iteration 99200: Loss = 0.5134085978112264\n",
      "Iteration 99300: Loss = 0.5133252774433219\n",
      "Iteration 99400: Loss = 0.5132420262684625\n",
      "Iteration 99500: Loss = 0.5131588441825052\n",
      "Iteration 99600: Loss = 0.5130757310815828\n",
      "Iteration 99700: Loss = 0.5129926868621033\n",
      "Iteration 99800: Loss = 0.5129097114207488\n",
      "Iteration 99900: Loss = 0.5128268046544746\n",
      "Accuracy: 81.16% for learning rate 0.001 and iterations 100000\n",
      "Iteration 0: Loss = 0.6931471805599453\n",
      "Iteration 100: Loss = 0.6854065382831849\n",
      "Iteration 200: Loss = 0.6809018188269114\n",
      "Iteration 300: Loss = 0.6765633937818768\n",
      "Iteration 400: Loss = 0.6723824401702012\n",
      "Iteration 500: Loss = 0.6683506032417378\n",
      "Iteration 600: Loss = 0.6644599808981523\n",
      "Iteration 700: Loss = 0.6607031067980735\n",
      "Iteration 800: Loss = 0.6570729325420905\n",
      "Iteration 900: Loss = 0.6535628092805132\n",
      "Iteration 1000: Loss = 0.650166469034558\n",
      "Iteration 1100: Loss = 0.6468780059736797\n",
      "Iteration 1200: Loss = 0.6436918578485203\n",
      "Iteration 1300: Loss = 0.6406027877404813\n",
      "Iteration 1400: Loss = 0.6376058662551883\n",
      "Iteration 1500: Loss = 0.6346964542578669\n",
      "Iteration 1600: Loss = 0.6318701862235864\n",
      "Iteration 1700: Loss = 0.6291229542541125\n",
      "Iteration 1800: Loss = 0.6264508927953221\n",
      "Iteration 1900: Loss = 0.623850364074417\n",
      "Iteration 2000: Loss = 0.6213179442641459\n",
      "Iteration 2100: Loss = 0.618850410371533\n",
      "Iteration 2200: Loss = 0.6164447278409141\n",
      "Iteration 2300: Loss = 0.6140980388550846\n",
      "Iteration 2400: Loss = 0.6118076513138058\n",
      "Iteration 2500: Loss = 0.609571028465557\n",
      "Iteration 2600: Loss = 0.6073857791660723\n",
      "Iteration 2700: Loss = 0.6052496487356509\n",
      "Iteration 2800: Loss = 0.6031605103863718\n",
      "Iteration 2900: Loss = 0.6011163571899951\n",
      "Iteration 3000: Loss = 0.5991152945574324\n",
      "Iteration 3100: Loss = 0.5971555332010826\n",
      "Iteration 3200: Loss = 0.5952353825520118\n",
      "Iteration 3300: Loss = 0.5933532446048247\n",
      "Iteration 3400: Loss = 0.5915076081640752\n",
      "Iteration 3500: Loss = 0.5896970434671754\n",
      "Iteration 3600: Loss = 0.5879201971599148\n",
      "Iteration 3700: Loss = 0.5861757876019015\n",
      "Iteration 3800: Loss = 0.584462600480439\n",
      "Iteration 3900: Loss = 0.5827794847125479\n",
      "Iteration 4000: Loss = 0.5811253486160208\n",
      "Iteration 4100: Loss = 0.57949915633154\n",
      "Iteration 4200: Loss = 0.5778999244789924\n",
      "Iteration 4300: Loss = 0.5763267190321872\n",
      "Iteration 4400: Loss = 0.5747786523971832\n",
      "Iteration 4500: Loss = 0.5732548806804103\n",
      "Iteration 4600: Loss = 0.5717546011336806\n",
      "Iteration 4700: Loss = 0.5702770497640444\n",
      "Iteration 4800: Loss = 0.5688214990972715\n",
      "Iteration 4900: Loss = 0.5673872560844947\n",
      "Iteration 5000: Loss = 0.565973660142276\n",
      "Iteration 5100: Loss = 0.5645800813170287\n",
      "Iteration 5200: Loss = 0.5632059185653555\n",
      "Iteration 5300: Loss = 0.5618505981424545\n",
      "Iteration 5400: Loss = 0.5605135720912897\n",
      "Iteration 5500: Loss = 0.559194316825736\n",
      "Iteration 5600: Loss = 0.5578923318013838\n",
      "Iteration 5700: Loss = 0.5566071382681317\n",
      "Iteration 5800: Loss = 0.5553382780991085\n",
      "Iteration 5900: Loss = 0.5540853126908449\n",
      "Iteration 6000: Loss = 0.5528478219299774\n",
      "Iteration 6100: Loss = 0.5516254032220905\n",
      "Iteration 6200: Loss = 0.5504176705786176\n",
      "Iteration 6300: Loss = 0.5492242537579978\n",
      "Iteration 6400: Loss = 0.5480447974575569\n",
      "Iteration 6500: Loss = 0.5468789605528236\n",
      "Iteration 6600: Loss = 0.5457264153812186\n",
      "Iteration 6700: Loss = 0.5445868470672686\n",
      "Iteration 6800: Loss = 0.5434599528866916\n",
      "Iteration 6900: Loss = 0.5423454416668801\n",
      "Iteration 7000: Loss = 0.5412430332214831\n",
      "Iteration 7100: Loss = 0.5401524578169402\n",
      "Iteration 7200: Loss = 0.539073455668965\n",
      "Iteration 7300: Loss = 0.5380057764671193\n",
      "Iteration 7400: Loss = 0.5369491789257325\n",
      "Iteration 7500: Loss = 0.535903430359548\n",
      "Iteration 7600: Loss = 0.5348683062825818\n",
      "Iteration 7700: Loss = 0.5338435900287767\n",
      "Iteration 7800: Loss = 0.5328290723931367\n",
      "Iteration 7900: Loss = 0.5318245512921046\n",
      "Iteration 8000: Loss = 0.5308298314420327\n",
      "Iteration 8100: Loss = 0.5298447240546695\n",
      "Iteration 8200: Loss = 0.5288690465486531\n",
      "Iteration 8300: Loss = 0.5279026222760731\n",
      "Iteration 8400: Loss = 0.5269452802632149\n",
      "Iteration 8500: Loss = 0.5259968549646653\n",
      "Iteration 8600: Loss = 0.525057186030005\n",
      "Iteration 8700: Loss = 0.5241261180823641\n",
      "Iteration 8800: Loss = 0.5232035005081634\n",
      "Iteration 8900: Loss = 0.522289187257406\n",
      "Iteration 9000: Loss = 0.5213830366539219\n",
      "Iteration 9100: Loss = 0.5204849112150094\n",
      "Iteration 9200: Loss = 0.5195946774799459\n",
      "Iteration 9300: Loss = 0.5187122058468775\n",
      "Iteration 9400: Loss = 0.5178373704176263\n",
      "Iteration 9500: Loss = 0.516970048849978\n",
      "Iteration 9600: Loss = 0.516110122217045\n",
      "Iteration 9700: Loss = 0.5152574748733207\n",
      "Iteration 9800: Loss = 0.5144119943270649\n",
      "Iteration 9900: Loss = 0.5135735711186787\n",
      "Iteration 10000: Loss = 0.5127420987047552\n",
      "Iteration 10100: Loss = 0.5119174733475002\n",
      "Iteration 10200: Loss = 0.5110995940092445\n",
      "Iteration 10300: Loss = 0.510288362251778\n",
      "Iteration 10400: Loss = 0.5094836821402589\n",
      "Iteration 10500: Loss = 0.508685460151458\n",
      "Iteration 10600: Loss = 0.5078936050861173\n",
      "Iteration 10700: Loss = 0.5071080279852115\n",
      "Iteration 10800: Loss = 0.5063286420499158\n",
      "Iteration 10900: Loss = 0.5055553625650908\n",
      "Iteration 11000: Loss = 0.504788106826111\n",
      "Iteration 11100: Loss = 0.5040267940688653\n",
      "Iteration 11200: Loss = 0.503271345402778\n",
      "Iteration 11300: Loss = 0.5025216837466954\n",
      "Iteration 11400: Loss = 0.5017777337675028\n",
      "Iteration 11500: Loss = 0.5010394218213339\n",
      "Iteration 11600: Loss = 0.5003066758972519\n",
      "Iteration 11700: Loss = 0.4995794255632772\n",
      "Iteration 11800: Loss = 0.4988576019146562\n",
      "Iteration 11900: Loss = 0.498141137524257\n",
      "Iteration 12000: Loss = 0.497429966394998\n",
      "Iteration 12100: Loss = 0.4967240239142083\n",
      "Iteration 12200: Loss = 0.49602324680983273\n",
      "Iteration 12300: Loss = 0.4953275731083941\n",
      "Iteration 12400: Loss = 0.4946369420946311\n",
      "Iteration 12500: Loss = 0.4939512942727354\n",
      "Iteration 12600: Loss = 0.49327057132911456\n",
      "Iteration 12700: Loss = 0.4925947160966105\n",
      "Iteration 12800: Loss = 0.4919236725201083\n",
      "Iteration 12900: Loss = 0.49125738562347204\n",
      "Iteration 13000: Loss = 0.4905958014777493\n",
      "Iteration 13100: Loss = 0.4899388671705859\n",
      "Iteration 13200: Loss = 0.4892865307767997\n",
      "Iteration 13300: Loss = 0.4886387413300599\n",
      "Iteration 13400: Loss = 0.4879954487956248\n",
      "Iteration 13500: Loss = 0.48735660404409187\n",
      "Iteration 13600: Loss = 0.4867221588261151\n",
      "Iteration 13700: Loss = 0.48609206574804914\n",
      "Iteration 13800: Loss = 0.48546627824848065\n",
      "Iteration 13900: Loss = 0.48484475057560594\n",
      "Iteration 14000: Loss = 0.4842274377654241\n",
      "Iteration 14100: Loss = 0.4836142956207065\n",
      "Iteration 14200: Loss = 0.483005280690713\n",
      "Iteration 14300: Loss = 0.4824003502516215\n",
      "Iteration 14400: Loss = 0.48179946228764287\n",
      "Iteration 14500: Loss = 0.481202575472793\n",
      "Iteration 14600: Loss = 0.48060964915329224\n",
      "Iteration 14700: Loss = 0.4800206433305706\n",
      "Iteration 14800: Loss = 0.4794355186448512\n",
      "Iteration 14900: Loss = 0.47885423635928853\n",
      "Iteration 15000: Loss = 0.4782767583446412\n",
      "Iteration 15100: Loss = 0.47770304706445577\n",
      "Iteration 15200: Loss = 0.4771330655607427\n",
      "Iteration 15300: Loss = 0.47656677744012343\n",
      "Iteration 15400: Loss = 0.4760041468604311\n",
      "Iteration 15500: Loss = 0.47544513851774706\n",
      "Iteration 15600: Loss = 0.4748897176338557\n",
      "Iteration 15700: Loss = 0.47433784994410166\n",
      "Iteration 15800: Loss = 0.4737895016856329\n",
      "Iteration 15900: Loss = 0.47324463958601737\n",
      "Iteration 16000: Loss = 0.47270323085221577\n",
      "Iteration 16100: Loss = 0.47216524315989833\n",
      "Iteration 16200: Loss = 0.4716306446430934\n",
      "Iteration 16300: Loss = 0.4710994038841539\n",
      "Iteration 16400: Loss = 0.47057148990403097\n",
      "Iteration 16500: Loss = 0.4700468721528416\n",
      "Iteration 16600: Loss = 0.4695255205007215\n",
      "Iteration 16700: Loss = 0.46900740522895146\n",
      "Iteration 16800: Loss = 0.4684924970213466\n",
      "Iteration 16900: Loss = 0.4679807669559005\n",
      "Iteration 17000: Loss = 0.4674721864966739\n",
      "Iteration 17100: Loss = 0.46696672748591855\n",
      "Iteration 17200: Loss = 0.46646436213642967\n",
      "Iteration 17300: Loss = 0.4659650630241171\n",
      "Iteration 17400: Loss = 0.4654688030807874\n",
      "Iteration 17500: Loss = 0.4649755555871312\n",
      "Iteration 17600: Loss = 0.4644852941659061\n",
      "Iteration 17700: Loss = 0.46399799277531034\n",
      "Iteration 17800: Loss = 0.46351362570253907\n",
      "Iteration 17900: Loss = 0.46303216755751814\n",
      "Iteration 18000: Loss = 0.4625535932668083\n",
      "Iteration 18100: Loss = 0.46207787806767453\n",
      "Iteration 18200: Loss = 0.4616049975023146\n",
      "Iteration 18300: Loss = 0.46113492741224194\n",
      "Iteration 18400: Loss = 0.46066764393281573\n",
      "Iteration 18500: Loss = 0.46020312348791736\n",
      "Iteration 18600: Loss = 0.4597413427847631\n",
      "Iteration 18700: Loss = 0.45928227880885225\n",
      "Iteration 18800: Loss = 0.4588259088190458\n",
      "Iteration 18900: Loss = 0.458372210342769\n",
      "Iteration 19000: Loss = 0.4579211611713367\n",
      "Iteration 19100: Loss = 0.45747273935539584\n",
      "Iteration 19200: Loss = 0.4570269232004815\n",
      "Iteration 19300: Loss = 0.45658369126268317\n",
      "Iteration 19400: Loss = 0.4561430223444167\n",
      "Iteration 19500: Loss = 0.455704895490301\n",
      "Iteration 19600: Loss = 0.4552692899831334\n",
      "Iteration 19700: Loss = 0.4548361853399615\n",
      "Iteration 19800: Loss = 0.4544055613082498\n",
      "Iteration 19900: Loss = 0.45397739786213587\n",
      "Iteration 20000: Loss = 0.45355167519877543\n",
      "Iteration 20100: Loss = 0.45312837373477255\n",
      "Iteration 20200: Loss = 0.4527074741026913\n",
      "Iteration 20300: Loss = 0.45228895714764955\n",
      "Iteration 20400: Loss = 0.4518728039239882\n",
      "Iteration 20500: Loss = 0.4514589956920174\n",
      "Iteration 20600: Loss = 0.45104751391483555\n",
      "Iteration 20700: Loss = 0.45063834025521865\n",
      "Iteration 20800: Loss = 0.4502314565725788\n",
      "Iteration 20900: Loss = 0.4498268449199893\n",
      "Iteration 21000: Loss = 0.4494244875412751\n",
      "Iteration 21100: Loss = 0.44902436686816566\n",
      "Iteration 21200: Loss = 0.4486264655175089\n",
      "Iteration 21300: Loss = 0.4482307662885453\n",
      "Iteration 21400: Loss = 0.4478372521602383\n",
      "Iteration 21500: Loss = 0.4474459062886618\n",
      "Iteration 21600: Loss = 0.4470567120044427\n",
      "Iteration 21700: Loss = 0.4466696528102537\n",
      "Iteration 21800: Loss = 0.44628471237836087\n",
      "Iteration 21900: Loss = 0.44590187454821895\n",
      "Iteration 22000: Loss = 0.44552112332411636\n",
      "Iteration 22100: Loss = 0.4451424428728669\n",
      "Iteration 22200: Loss = 0.4447658175215479\n",
      "Iteration 22300: Loss = 0.44439123175528267\n",
      "Iteration 22400: Loss = 0.44401867021506736\n",
      "Iteration 22500: Loss = 0.4436481176956393\n",
      "Iteration 22600: Loss = 0.44327955914338707\n",
      "Iteration 22700: Loss = 0.44291297965430054\n",
      "Iteration 22800: Loss = 0.44254836447196055\n",
      "Iteration 22900: Loss = 0.442185698985566\n",
      "Iteration 23000: Loss = 0.4418249687279983\n",
      "Iteration 23100: Loss = 0.44146615937392236\n",
      "Iteration 23200: Loss = 0.4411092567379226\n",
      "Iteration 23300: Loss = 0.44075424677267255\n",
      "Iteration 23400: Loss = 0.4404011155671397\n",
      "Iteration 23500: Loss = 0.4400498493448205\n",
      "Iteration 23600: Loss = 0.43970043446201007\n",
      "Iteration 23700: Loss = 0.43935285740610025\n",
      "Iteration 23800: Loss = 0.4390071047939098\n",
      "Iteration 23900: Loss = 0.438663163370043\n",
      "Iteration 24000: Loss = 0.4383210200052779\n",
      "Iteration 24100: Loss = 0.437980661694982\n",
      "Iteration 24200: Loss = 0.4376420755575557\n",
      "Iteration 24300: Loss = 0.4373052488329025\n",
      "Iteration 24400: Loss = 0.4369701688809255\n",
      "Iteration 24500: Loss = 0.4366368231800487\n",
      "Iteration 24600: Loss = 0.43630519932576484\n",
      "Iteration 24700: Loss = 0.4359752850292053\n",
      "Iteration 24800: Loss = 0.4356470681157361\n",
      "Iteration 24900: Loss = 0.43532053652357555\n",
      "Iteration 25000: Loss = 0.43499567830243535\n",
      "Iteration 25100: Loss = 0.43467248161218386\n",
      "Iteration 25200: Loss = 0.43435093472153036\n",
      "Iteration 25300: Loss = 0.4340310260067321\n",
      "Iteration 25400: Loss = 0.43371274395032017\n",
      "Iteration 25500: Loss = 0.43339607713984707\n",
      "Iteration 25600: Loss = 0.43308101426665374\n",
      "Iteration 25700: Loss = 0.43276754412465573\n",
      "Iteration 25800: Loss = 0.4324556556091487\n",
      "Iteration 25900: Loss = 0.43214533771563207\n",
      "Iteration 26000: Loss = 0.4318365795386518\n",
      "Iteration 26100: Loss = 0.43152937027065985\n",
      "Iteration 26200: Loss = 0.43122369920089165\n",
      "Iteration 26300: Loss = 0.4309195557142607\n",
      "Iteration 26400: Loss = 0.43061692929026985\n",
      "Iteration 26500: Loss = 0.4303158095019385\n",
      "Iteration 26600: Loss = 0.4300161860147466\n",
      "Iteration 26700: Loss = 0.4297180485855936\n",
      "Iteration 26800: Loss = 0.42942138706177324\n",
      "Iteration 26900: Loss = 0.4291261913799634\n",
      "Iteration 27000: Loss = 0.42883245156523053\n",
      "Iteration 27100: Loss = 0.42854015773004855\n",
      "Iteration 27200: Loss = 0.4282493000733324\n",
      "Iteration 27300: Loss = 0.42795986887948484\n",
      "Iteration 27400: Loss = 0.4276718545174576\n",
      "Iteration 27500: Loss = 0.42738524743982476\n",
      "Iteration 27600: Loss = 0.4271000381818713\n",
      "Iteration 27700: Loss = 0.4268162173606919\n",
      "Iteration 27800: Loss = 0.42653377567430484\n",
      "Iteration 27900: Loss = 0.42625270390077635\n",
      "Iteration 28000: Loss = 0.42597299289735857\n",
      "Iteration 28100: Loss = 0.4256946335996384\n",
      "Iteration 28200: Loss = 0.42541761702069886\n",
      "Iteration 28300: Loss = 0.4251419342502911\n",
      "Iteration 28400: Loss = 0.42486757645401896\n",
      "Iteration 28500: Loss = 0.4245945348725326\n",
      "Iteration 28600: Loss = 0.4243228008207356\n",
      "Iteration 28700: Loss = 0.4240523656870002\n",
      "Iteration 28800: Loss = 0.423783220932395\n",
      "Iteration 28900: Loss = 0.4235153580899218\n",
      "Iteration 29000: Loss = 0.42324876876376294\n",
      "Iteration 29100: Loss = 0.42298344462853876\n",
      "Iteration 29200: Loss = 0.42271937742857457\n",
      "Iteration 29300: Loss = 0.42245655897717727\n",
      "Iteration 29400: Loss = 0.42219498115592147\n",
      "Iteration 29500: Loss = 0.42193463591394464\n",
      "Iteration 29600: Loss = 0.4216755152672522\n",
      "Iteration 29700: Loss = 0.42141761129803057\n",
      "Iteration 29800: Loss = 0.4211609161539692\n",
      "Iteration 29900: Loss = 0.4209054220475921\n",
      "Iteration 30000: Loss = 0.42065112125559667\n",
      "Iteration 30100: Loss = 0.4203980061182022\n",
      "Iteration 30200: Loss = 0.42014606903850527\n",
      "Iteration 30300: Loss = 0.4198953024818445\n",
      "Iteration 30400: Loss = 0.4196456989751722\n",
      "Iteration 30500: Loss = 0.41939725110643505\n",
      "Iteration 30600: Loss = 0.4191499515239609\n",
      "Iteration 30700: Loss = 0.41890379293585495\n",
      "Iteration 30800: Loss = 0.418658768109402\n",
      "Iteration 30900: Loss = 0.4184148698704769\n",
      "Iteration 31000: Loss = 0.41817209110296155\n",
      "Iteration 31100: Loss = 0.41793042474816994\n",
      "Iteration 31200: Loss = 0.4176898638042792\n",
      "Iteration 31300: Loss = 0.4174504013257681\n",
      "Iteration 31400: Loss = 0.4172120304228627\n",
      "Iteration 31500: Loss = 0.41697474426098746\n",
      "Iteration 31600: Loss = 0.4167385360602248\n",
      "Iteration 31700: Loss = 0.41650339909477874\n",
      "Iteration 31800: Loss = 0.4162693266924478\n",
      "Iteration 31900: Loss = 0.4160363122341016\n",
      "Iteration 32000: Loss = 0.4158043491531647\n",
      "Iteration 32100: Loss = 0.41557343093510746\n",
      "Iteration 32200: Loss = 0.4153435511169412\n",
      "Iteration 32300: Loss = 0.4151147032867208\n",
      "Iteration 32400: Loss = 0.41488688108305166\n",
      "Iteration 32500: Loss = 0.414660078194604\n",
      "Iteration 32600: Loss = 0.41443428835963164\n",
      "Iteration 32700: Loss = 0.4142095053654969\n",
      "Iteration 32800: Loss = 0.4139857230482006\n",
      "Iteration 32900: Loss = 0.4137629352919185\n",
      "Iteration 33000: Loss = 0.4135411360285413\n",
      "Iteration 33100: Loss = 0.41332031923722246\n",
      "Iteration 33200: Loss = 0.4131004789439278\n",
      "Iteration 33300: Loss = 0.4128816092209939\n",
      "Iteration 33400: Loss = 0.4126637041866889\n",
      "Iteration 33500: Loss = 0.41244675800477937\n",
      "Iteration 33600: Loss = 0.412230764884102\n",
      "Iteration 33700: Loss = 0.4120157190781398\n",
      "Iteration 33800: Loss = 0.4118016148846038\n",
      "Iteration 33900: Loss = 0.4115884466450185\n",
      "Iteration 34000: Loss = 0.41137620874431274\n",
      "Iteration 34100: Loss = 0.41116489561041447\n",
      "Iteration 34200: Loss = 0.41095450171385095\n",
      "Iteration 34300: Loss = 0.4107450215673526\n",
      "Iteration 34400: Loss = 0.4105364497254613\n",
      "Iteration 34500: Loss = 0.4103287807841437\n",
      "Iteration 34600: Loss = 0.41012200938040805\n",
      "Iteration 34700: Loss = 0.4099161301919256\n",
      "Iteration 34800: Loss = 0.4097111379366563\n",
      "Iteration 34900: Loss = 0.4095070273724781\n",
      "Iteration 35000: Loss = 0.4093037932968212\n",
      "Iteration 35100: Loss = 0.4091014305463048\n",
      "Iteration 35200: Loss = 0.4088999339963799\n",
      "Iteration 35300: Loss = 0.4086992985609737\n",
      "Iteration 35400: Loss = 0.40849951919214\n",
      "Iteration 35500: Loss = 0.4083005908797114\n",
      "Iteration 35600: Loss = 0.4081025086509568\n",
      "Iteration 35700: Loss = 0.4079052675702422\n",
      "Iteration 35800: Loss = 0.4077088627386943\n",
      "Iteration 35900: Loss = 0.40751328929386893\n",
      "Iteration 36000: Loss = 0.40731854240942217\n",
      "Iteration 36100: Loss = 0.4071246172947851\n",
      "Iteration 36200: Loss = 0.4069315091948431\n",
      "Iteration 36300: Loss = 0.40673921338961627\n",
      "Iteration 36400: Loss = 0.40654772519394594\n",
      "Iteration 36500: Loss = 0.40635703995718214\n",
      "Iteration 36600: Loss = 0.40616715306287654\n",
      "Iteration 36700: Loss = 0.40597805992847624\n",
      "Iteration 36800: Loss = 0.40578975600502304\n",
      "Iteration 36900: Loss = 0.4056022367768546\n",
      "Iteration 37000: Loss = 0.40541549776130886\n",
      "Iteration 37100: Loss = 0.4052295345084318\n",
      "Iteration 37200: Loss = 0.4050443426006884\n",
      "Iteration 37300: Loss = 0.40485991765267615\n",
      "Iteration 37400: Loss = 0.4046762553108417\n",
      "Iteration 37500: Loss = 0.404493351253201\n",
      "Iteration 37600: Loss = 0.4043112011890611\n",
      "Iteration 37700: Loss = 0.40412980085874656\n",
      "Iteration 37800: Loss = 0.40394914603332693\n",
      "Iteration 37900: Loss = 0.40376923251434815\n",
      "Iteration 38000: Loss = 0.40359005613356647\n",
      "Iteration 38100: Loss = 0.40341161275268506\n",
      "Iteration 38200: Loss = 0.4032338982630932\n",
      "Iteration 38300: Loss = 0.4030569085856081\n",
      "Iteration 38400: Loss = 0.40288063967021986\n",
      "Iteration 38500: Loss = 0.4027050874958385\n",
      "Iteration 38600: Loss = 0.40253024807004356\n",
      "Iteration 38700: Loss = 0.4023561174288365\n",
      "Iteration 38800: Loss = 0.402182691636396\n",
      "Iteration 38900: Loss = 0.40200996678483464\n",
      "Iteration 39000: Loss = 0.40183793899395887\n",
      "Iteration 39100: Loss = 0.40166660441103147\n",
      "Iteration 39200: Loss = 0.4014959592105358\n",
      "Iteration 39300: Loss = 0.40132599959394305\n",
      "Iteration 39400: Loss = 0.4011567217894813\n",
      "Iteration 39500: Loss = 0.4009881220519074\n",
      "Iteration 39600: Loss = 0.40082019666228047\n",
      "Iteration 39700: Loss = 0.4006529419277386\n",
      "Iteration 39800: Loss = 0.4004863541812768\n",
      "Iteration 39900: Loss = 0.40032042978152776\n",
      "Iteration 40000: Loss = 0.4001551651125449\n",
      "Iteration 40100: Loss = 0.3999905565835868\n",
      "Iteration 40200: Loss = 0.39982660062890457\n",
      "Iteration 40300: Loss = 0.3996632937075312\n",
      "Iteration 40400: Loss = 0.39950063230307253\n",
      "Iteration 40500: Loss = 0.39933861292350076\n",
      "Iteration 40600: Loss = 0.3991772321009495\n",
      "Iteration 40700: Loss = 0.39901648639151194\n",
      "Iteration 40800: Loss = 0.3988563723750389\n",
      "Iteration 40900: Loss = 0.39869688665494174\n",
      "Iteration 41000: Loss = 0.39853802585799436\n",
      "Iteration 41100: Loss = 0.39837978663413876\n",
      "Iteration 41200: Loss = 0.39822216565629265\n",
      "Iteration 41300: Loss = 0.398065159620158\n",
      "Iteration 41400: Loss = 0.3979087652440317\n",
      "Iteration 41500: Loss = 0.3977529792686186\n",
      "Iteration 41600: Loss = 0.39759779845684573\n",
      "Iteration 41700: Loss = 0.39744321959367923\n",
      "Iteration 41800: Loss = 0.39728923948594125\n",
      "Iteration 41900: Loss = 0.3971358549621311\n",
      "Iteration 42000: Loss = 0.39698306287224566\n",
      "Iteration 42100: Loss = 0.3968308600876036\n",
      "Iteration 42200: Loss = 0.39667924350066985\n",
      "Iteration 42300: Loss = 0.39652821002488203\n",
      "Iteration 42400: Loss = 0.39637775659447966\n",
      "Iteration 42500: Loss = 0.39622788016433325\n",
      "Iteration 42600: Loss = 0.3960785777097762\n",
      "Iteration 42700: Loss = 0.39592984622643834\n",
      "Iteration 42800: Loss = 0.39578168273008024\n",
      "Iteration 42900: Loss = 0.39563408425642965\n",
      "Iteration 43000: Loss = 0.39548704786101957\n",
      "Iteration 43100: Loss = 0.39534057061902783\n",
      "Iteration 43200: Loss = 0.39519464962511797\n",
      "Iteration 43300: Loss = 0.39504928199328126\n",
      "Iteration 43400: Loss = 0.39490446485668146\n",
      "Iteration 43500: Loss = 0.3947601953674998\n",
      "Iteration 43600: Loss = 0.394616470696782\n",
      "Iteration 43700: Loss = 0.39447328803428666\n",
      "Iteration 43800: Loss = 0.3943306445883346\n",
      "Iteration 43900: Loss = 0.39418853758566097\n",
      "Iteration 44000: Loss = 0.3940469642712669\n",
      "Iteration 44100: Loss = 0.3939059219082741\n",
      "Iteration 44200: Loss = 0.39376540777777924\n",
      "Iteration 44300: Loss = 0.39362541917871247\n",
      "Iteration 44400: Loss = 0.3934859534276931\n",
      "Iteration 44500: Loss = 0.39334700785889076\n",
      "Iteration 44600: Loss = 0.39320857982388513\n",
      "Iteration 44700: Loss = 0.39307066669152807\n",
      "Iteration 44800: Loss = 0.392933265847807\n",
      "Iteration 44900: Loss = 0.3927963746957094\n",
      "Iteration 45000: Loss = 0.39265999065508783\n",
      "Iteration 45100: Loss = 0.392524111162528\n",
      "Iteration 45200: Loss = 0.3923887336712162\n",
      "Iteration 45300: Loss = 0.39225385565080917\n",
      "Iteration 45400: Loss = 0.3921194745873041\n",
      "Iteration 45500: Loss = 0.3919855879829113\n",
      "Iteration 45600: Loss = 0.3918521933559263\n",
      "Iteration 45700: Loss = 0.3917192882406045\n",
      "Iteration 45800: Loss = 0.39158687018703653\n",
      "Iteration 45900: Loss = 0.39145493676102433\n",
      "Iteration 46000: Loss = 0.39132348554395907\n",
      "Iteration 46100: Loss = 0.3911925141326994\n",
      "Iteration 46200: Loss = 0.3910620201394518\n",
      "Iteration 46300: Loss = 0.39093200119165084\n",
      "Iteration 46400: Loss = 0.3908024549318416\n",
      "Iteration 46500: Loss = 0.39067337901756266\n",
      "Iteration 46600: Loss = 0.3905447711212296\n",
      "Iteration 46700: Loss = 0.3904166289300211\n",
      "Iteration 46800: Loss = 0.3902889501457643\n",
      "Iteration 46900: Loss = 0.39016173248482255\n",
      "Iteration 47000: Loss = 0.39003497367798295\n",
      "Iteration 47100: Loss = 0.3899086714703463\n",
      "Iteration 47200: Loss = 0.389782823621217\n",
      "Iteration 47300: Loss = 0.38965742790399416\n",
      "Iteration 47400: Loss = 0.3895324821060639\n",
      "Iteration 47500: Loss = 0.3894079840286924\n",
      "Iteration 47600: Loss = 0.38928393148692025\n",
      "Iteration 47700: Loss = 0.38916032230945696\n",
      "Iteration 47800: Loss = 0.3890371543385776\n",
      "Iteration 47900: Loss = 0.388914425430019\n",
      "Iteration 48000: Loss = 0.3887921334528783\n",
      "Iteration 48100: Loss = 0.3886702762895112\n",
      "Iteration 48200: Loss = 0.3885488518354315\n",
      "Iteration 48300: Loss = 0.38842785799921203\n",
      "Iteration 48400: Loss = 0.38830729270238584\n",
      "Iteration 48500: Loss = 0.38818715387934843\n",
      "Iteration 48600: Loss = 0.3880674394772607\n",
      "Iteration 48700: Loss = 0.3879481474559534\n",
      "Iteration 48800: Loss = 0.38782927578783155\n",
      "Iteration 48900: Loss = 0.3877108224577804\n",
      "Iteration 49000: Loss = 0.38759278546307147\n",
      "Iteration 49100: Loss = 0.3874751628132705\n",
      "Iteration 49200: Loss = 0.38735795253014504\n",
      "Iteration 49300: Loss = 0.38724115264757386\n",
      "Iteration 49400: Loss = 0.387124761211456\n",
      "Iteration 49500: Loss = 0.3870087762796219\n",
      "Iteration 49600: Loss = 0.38689319592174454\n",
      "Iteration 49700: Loss = 0.38677801821925145\n",
      "Iteration 49800: Loss = 0.3866632412652374\n",
      "Iteration 49900: Loss = 0.38654886316437814\n",
      "Accuracy: 89.86% for learning rate 0.01 and iterations 50000\n",
      "Iteration 0: Loss = 0.6931471805599453\n",
      "Iteration 100: Loss = 0.6917253381704314\n",
      "Iteration 200: Loss = 0.6909210337856477\n",
      "Iteration 300: Loss = 0.6904565357605086\n",
      "Iteration 400: Loss = 0.6901791089205745\n",
      "Iteration 500: Loss = 0.6900047416707052\n",
      "Iteration 600: Loss = 0.6898871932403304\n",
      "Iteration 700: Loss = 0.6898009927124341\n",
      "Iteration 800: Loss = 0.6897320997124426\n",
      "Iteration 900: Loss = 0.6896727699220915\n",
      "Iteration 1000: Loss = 0.6896187295947861\n",
      "Iteration 1100: Loss = 0.6895676192858956\n",
      "Iteration 1200: Loss = 0.689518135986108\n",
      "Iteration 1300: Loss = 0.6894695599304373\n",
      "Iteration 1400: Loss = 0.6894214934558642\n",
      "Iteration 1500: Loss = 0.6893737168253833\n",
      "Iteration 1600: Loss = 0.6893261086042745\n",
      "Iteration 1700: Loss = 0.6892786016761551\n",
      "Iteration 1800: Loss = 0.6892311589420493\n",
      "Iteration 1900: Loss = 0.6891837598924488\n",
      "Iteration 2000: Loss = 0.6891363931867155\n",
      "Iteration 2100: Loss = 0.6890890525519804\n",
      "Iteration 2200: Loss = 0.6890417345164876\n",
      "Iteration 2300: Loss = 0.688994437156777\n",
      "Iteration 2400: Loss = 0.6889471594052088\n",
      "Iteration 2500: Loss = 0.6888999006672064\n",
      "Iteration 2600: Loss = 0.6888526606096823\n",
      "Iteration 2700: Loss = 0.6888054390440921\n",
      "Iteration 2800: Loss = 0.6887582358617907\n",
      "Iteration 2900: Loss = 0.6887110509982992\n",
      "Iteration 3000: Loss = 0.6886638844135539\n",
      "Iteration 3100: Loss = 0.6886167360809883\n",
      "Iteration 3200: Loss = 0.6885696059814987\n",
      "Iteration 3300: Loss = 0.6885224941001079\n",
      "Iteration 3400: Loss = 0.688475400424122\n",
      "Iteration 3500: Loss = 0.688428324942111\n",
      "Iteration 3600: Loss = 0.6883812676433455\n",
      "Iteration 3700: Loss = 0.6883342285174858\n",
      "Iteration 3800: Loss = 0.6882872075544093\n",
      "Iteration 3900: Loss = 0.6882402047441157\n",
      "Iteration 4000: Loss = 0.688193220076675\n",
      "Iteration 4100: Loss = 0.6881462535421972\n",
      "Iteration 4200: Loss = 0.6880993051308176\n",
      "Iteration 4300: Loss = 0.688052374832687\n",
      "Iteration 4400: Loss = 0.6880054626379677\n",
      "Iteration 4500: Loss = 0.6879585685368298\n",
      "Iteration 4600: Loss = 0.6879116925194502\n",
      "Iteration 4700: Loss = 0.6878648345760122\n",
      "Iteration 4800: Loss = 0.6878179946967042\n",
      "Iteration 4900: Loss = 0.6877711728717203\n",
      "Iteration 5000: Loss = 0.6877243690912592\n",
      "Iteration 5100: Loss = 0.6876775833455254\n",
      "Iteration 5200: Loss = 0.6876308156247275\n",
      "Iteration 5300: Loss = 0.6875840659190803\n",
      "Iteration 5400: Loss = 0.6875373342188025\n",
      "Iteration 5500: Loss = 0.687490620514118\n",
      "Iteration 5600: Loss = 0.6874439247952565\n",
      "Iteration 5700: Loss = 0.6873972470524518\n",
      "Iteration 5800: Loss = 0.6873505872759428\n",
      "Iteration 5900: Loss = 0.6873039454559738\n",
      "Iteration 6000: Loss = 0.6872573215827936\n",
      "Iteration 6100: Loss = 0.6872107156466566\n",
      "Iteration 6200: Loss = 0.6871641276378212\n",
      "Iteration 6300: Loss = 0.6871175575465517\n",
      "Iteration 6400: Loss = 0.687071005363117\n",
      "Iteration 6500: Loss = 0.6870244710777911\n",
      "Iteration 6600: Loss = 0.6869779546808525\n",
      "Iteration 6700: Loss = 0.6869314561625857\n",
      "Iteration 6800: Loss = 0.6868849755132788\n",
      "Iteration 6900: Loss = 0.686838512723226\n",
      "Iteration 7000: Loss = 0.6867920677827258\n",
      "Iteration 7100: Loss = 0.6867456406820824\n",
      "Iteration 7200: Loss = 0.6866992314116038\n",
      "Iteration 7300: Loss = 0.6866528399616041\n",
      "Iteration 7400: Loss = 0.6866064663224021\n",
      "Iteration 7500: Loss = 0.6865601104843208\n",
      "Iteration 7600: Loss = 0.6865137724376892\n",
      "Iteration 7700: Loss = 0.6864674521728407\n",
      "Iteration 7800: Loss = 0.6864211496801137\n",
      "Iteration 7900: Loss = 0.6863748649498516\n",
      "Iteration 8000: Loss = 0.6863285979724029\n",
      "Iteration 8100: Loss = 0.686282348738121\n",
      "Iteration 8200: Loss = 0.6862361172373641\n",
      "Iteration 8300: Loss = 0.6861899034604955\n",
      "Iteration 8400: Loss = 0.6861437073978833\n",
      "Iteration 8500: Loss = 0.686097529039901\n",
      "Iteration 8600: Loss = 0.6860513683769264\n",
      "Iteration 8700: Loss = 0.6860052253993426\n",
      "Iteration 8800: Loss = 0.6859591000975378\n",
      "Iteration 8900: Loss = 0.6859129924619048\n",
      "Iteration 9000: Loss = 0.6858669024828419\n",
      "Iteration 9100: Loss = 0.6858208301507516\n",
      "Iteration 9200: Loss = 0.6857747754560418\n",
      "Iteration 9300: Loss = 0.6857287383891253\n",
      "Iteration 9400: Loss = 0.68568271894042\n",
      "Iteration 9500: Loss = 0.6856367171003482\n",
      "Iteration 9600: Loss = 0.6855907328593379\n",
      "Iteration 9700: Loss = 0.6855447662078215\n",
      "Iteration 9800: Loss = 0.6854988171362364\n",
      "Iteration 9900: Loss = 0.6854528856350249\n",
      "Iteration 10000: Loss = 0.6854069716946349\n",
      "Iteration 10100: Loss = 0.6853610753055182\n",
      "Iteration 10200: Loss = 0.6853151964581322\n",
      "Iteration 10300: Loss = 0.6852693351429393\n",
      "Iteration 10400: Loss = 0.6852234913504065\n",
      "Iteration 10500: Loss = 0.6851776650710056\n",
      "Iteration 10600: Loss = 0.6851318562952139\n",
      "Iteration 10700: Loss = 0.6850860650135133\n",
      "Iteration 10800: Loss = 0.6850402912163905\n",
      "Iteration 10900: Loss = 0.6849945348943374\n",
      "Iteration 11000: Loss = 0.6849487960378506\n",
      "Iteration 11100: Loss = 0.6849030746374319\n",
      "Iteration 11200: Loss = 0.6848573706835879\n",
      "Iteration 11300: Loss = 0.6848116841668298\n",
      "Iteration 11400: Loss = 0.6847660150776741\n",
      "Iteration 11500: Loss = 0.6847203634066424\n",
      "Iteration 11600: Loss = 0.6846747291442609\n",
      "Iteration 11700: Loss = 0.6846291122810606\n",
      "Iteration 11800: Loss = 0.6845835128075777\n",
      "Iteration 11900: Loss = 0.6845379307143534\n",
      "Iteration 12000: Loss = 0.6844923659919333\n",
      "Iteration 12100: Loss = 0.6844468186308686\n",
      "Iteration 12200: Loss = 0.6844012886217149\n",
      "Iteration 12300: Loss = 0.6843557759550329\n",
      "Iteration 12400: Loss = 0.6843102806213883\n",
      "Iteration 12500: Loss = 0.6842648026113515\n",
      "Iteration 12600: Loss = 0.6842193419154982\n",
      "Iteration 12700: Loss = 0.6841738985244084\n",
      "Iteration 12800: Loss = 0.6841284724286675\n",
      "Iteration 12900: Loss = 0.6840830636188656\n",
      "Iteration 13000: Loss = 0.6840376720855981\n",
      "Iteration 13100: Loss = 0.6839922978194647\n",
      "Iteration 13200: Loss = 0.6839469408110703\n",
      "Iteration 13300: Loss = 0.6839016010510245\n",
      "Iteration 13400: Loss = 0.6838562785299426\n",
      "Iteration 13500: Loss = 0.6838109732384435\n",
      "Iteration 13600: Loss = 0.6837656851671522\n",
      "Iteration 13700: Loss = 0.6837204143066978\n",
      "Iteration 13800: Loss = 0.6836751606477146\n",
      "Iteration 13900: Loss = 0.6836299241808422\n",
      "Iteration 14000: Loss = 0.6835847048967242\n",
      "Iteration 14100: Loss = 0.6835395027860098\n",
      "Iteration 14200: Loss = 0.6834943178393529\n",
      "Iteration 14300: Loss = 0.6834491500474122\n",
      "Iteration 14400: Loss = 0.6834039994008514\n",
      "Iteration 14500: Loss = 0.683358865890339\n",
      "Iteration 14600: Loss = 0.6833137495065486\n",
      "Iteration 14700: Loss = 0.6832686502401583\n",
      "Iteration 14800: Loss = 0.6832235680818516\n",
      "Iteration 14900: Loss = 0.6831785030223163\n",
      "Iteration 15000: Loss = 0.6831334550522457\n",
      "Iteration 15100: Loss = 0.6830884241623374\n",
      "Iteration 15200: Loss = 0.6830434103432943\n",
      "Iteration 15300: Loss = 0.6829984135858241\n",
      "Iteration 15400: Loss = 0.6829534338806392\n",
      "Iteration 15500: Loss = 0.682908471218457\n",
      "Iteration 15600: Loss = 0.68286352559\n",
      "Iteration 15700: Loss = 0.682818596985995\n",
      "Iteration 15800: Loss = 0.6827736853971744\n",
      "Iteration 15900: Loss = 0.6827287908142748\n",
      "Iteration 16000: Loss = 0.6826839132280381\n",
      "Iteration 16100: Loss = 0.6826390526292111\n",
      "Iteration 16200: Loss = 0.6825942090085451\n",
      "Iteration 16300: Loss = 0.6825493823567965\n",
      "Iteration 16400: Loss = 0.6825045726647267\n",
      "Iteration 16500: Loss = 0.6824597799231018\n",
      "Iteration 16600: Loss = 0.6824150041226927\n",
      "Iteration 16700: Loss = 0.6823702452542754\n",
      "Iteration 16800: Loss = 0.6823255033086304\n",
      "Iteration 16900: Loss = 0.6822807782765434\n",
      "Iteration 17000: Loss = 0.682236070148805\n",
      "Iteration 17100: Loss = 0.6821913789162105\n",
      "Iteration 17200: Loss = 0.6821467045695598\n",
      "Iteration 17300: Loss = 0.682102047099658\n",
      "Iteration 17400: Loss = 0.6820574064973153\n",
      "Iteration 17500: Loss = 0.682012782753346\n",
      "Iteration 17600: Loss = 0.68196817585857\n",
      "Iteration 17700: Loss = 0.6819235858038115\n",
      "Iteration 17800: Loss = 0.6818790125798999\n",
      "Iteration 17900: Loss = 0.6818344561776696\n",
      "Iteration 18000: Loss = 0.6817899165879592\n",
      "Iteration 18100: Loss = 0.6817453938016126\n",
      "Iteration 18200: Loss = 0.6817008878094787\n",
      "Iteration 18300: Loss = 0.681656398602411\n",
      "Iteration 18400: Loss = 0.6816119261712679\n",
      "Iteration 18500: Loss = 0.6815674705069125\n",
      "Iteration 18600: Loss = 0.6815230316002127\n",
      "Iteration 18700: Loss = 0.6814786094420416\n",
      "Iteration 18800: Loss = 0.6814342040232771\n",
      "Iteration 18900: Loss = 0.6813898153348017\n",
      "Iteration 19000: Loss = 0.6813454433675026\n",
      "Iteration 19100: Loss = 0.6813010881122723\n",
      "Iteration 19200: Loss = 0.6812567495600078\n",
      "Iteration 19300: Loss = 0.681212427701611\n",
      "Iteration 19400: Loss = 0.6811681225279888\n",
      "Iteration 19500: Loss = 0.6811238340300524\n",
      "Iteration 19600: Loss = 0.6810795621987185\n",
      "Iteration 19700: Loss = 0.6810353070249084\n",
      "Iteration 19800: Loss = 0.6809910684995482\n",
      "Iteration 19900: Loss = 0.6809468466135685\n",
      "Iteration 20000: Loss = 0.6809026413579051\n",
      "Iteration 20100: Loss = 0.6808584527234989\n",
      "Iteration 20200: Loss = 0.6808142807012947\n",
      "Iteration 20300: Loss = 0.6807701252822431\n",
      "Iteration 20400: Loss = 0.680725986457299\n",
      "Iteration 20500: Loss = 0.6806818642174223\n",
      "Iteration 20600: Loss = 0.6806377585535774\n",
      "Iteration 20700: Loss = 0.6805936694567339\n",
      "Iteration 20800: Loss = 0.6805495969178661\n",
      "Iteration 20900: Loss = 0.6805055409279529\n",
      "Iteration 21000: Loss = 0.6804615014779785\n",
      "Iteration 21100: Loss = 0.6804174785589312\n",
      "Iteration 21200: Loss = 0.6803734721618048\n",
      "Iteration 21300: Loss = 0.6803294822775976\n",
      "Iteration 21400: Loss = 0.6802855088973125\n",
      "Iteration 21500: Loss = 0.6802415520119577\n",
      "Iteration 21600: Loss = 0.6801976116125458\n",
      "Iteration 21700: Loss = 0.6801536876900941\n",
      "Iteration 21800: Loss = 0.6801097802356254\n",
      "Iteration 21900: Loss = 0.6800658892401665\n",
      "Iteration 22000: Loss = 0.6800220146947494\n",
      "Iteration 22100: Loss = 0.6799781565904109\n",
      "Iteration 22200: Loss = 0.6799343149181926\n",
      "Iteration 22300: Loss = 0.6798904896691405\n",
      "Iteration 22400: Loss = 0.6798466808343058\n",
      "Iteration 22500: Loss = 0.6798028884047447\n",
      "Iteration 22600: Loss = 0.6797591123715174\n",
      "Iteration 22700: Loss = 0.6797153527256897\n",
      "Iteration 22800: Loss = 0.6796716094583319\n",
      "Iteration 22900: Loss = 0.6796278825605191\n",
      "Iteration 23000: Loss = 0.6795841720233309\n",
      "Iteration 23100: Loss = 0.6795404778378519\n",
      "Iteration 23200: Loss = 0.6794967999951718\n",
      "Iteration 23300: Loss = 0.6794531384863844\n",
      "Iteration 23400: Loss = 0.679409493302589\n",
      "Iteration 23500: Loss = 0.6793658644348892\n",
      "Iteration 23600: Loss = 0.6793222518743935\n",
      "Iteration 23700: Loss = 0.6792786556122151\n",
      "Iteration 23800: Loss = 0.6792350756394723\n",
      "Iteration 23900: Loss = 0.6791915119472879\n",
      "Iteration 24000: Loss = 0.6791479645267892\n",
      "Iteration 24100: Loss = 0.6791044333691089\n",
      "Iteration 24200: Loss = 0.679060918465384\n",
      "Iteration 24300: Loss = 0.6790174198067565\n",
      "Iteration 24400: Loss = 0.678973937384373\n",
      "Iteration 24500: Loss = 0.678930471189385\n",
      "Iteration 24600: Loss = 0.6788870212129487\n",
      "Iteration 24700: Loss = 0.6788435874462252\n",
      "Iteration 24800: Loss = 0.6788001698803798\n",
      "Iteration 24900: Loss = 0.6787567685065834\n",
      "Iteration 25000: Loss = 0.6787133833160113\n",
      "Iteration 25100: Loss = 0.678670014299843\n",
      "Iteration 25200: Loss = 0.6786266614492639\n",
      "Iteration 25300: Loss = 0.6785833247554629\n",
      "Iteration 25400: Loss = 0.6785400042096348\n",
      "Iteration 25500: Loss = 0.6784966998029782\n",
      "Iteration 25600: Loss = 0.678453411526697\n",
      "Iteration 25700: Loss = 0.6784101393719998\n",
      "Iteration 25800: Loss = 0.6783668833300998\n",
      "Iteration 25900: Loss = 0.6783236433922148\n",
      "Iteration 26000: Loss = 0.678280419549568\n",
      "Iteration 26100: Loss = 0.6782372117933864\n",
      "Iteration 26200: Loss = 0.6781940201149026\n",
      "Iteration 26300: Loss = 0.6781508445053532\n",
      "Iteration 26400: Loss = 0.6781076849559802\n",
      "Iteration 26500: Loss = 0.6780645414580301\n",
      "Iteration 26600: Loss = 0.6780214140027536\n",
      "Iteration 26700: Loss = 0.6779783025814071\n",
      "Iteration 26800: Loss = 0.677935207185251\n",
      "Iteration 26900: Loss = 0.6778921278055506\n",
      "Iteration 27000: Loss = 0.6778490644335764\n",
      "Iteration 27100: Loss = 0.6778060170606028\n",
      "Iteration 27200: Loss = 0.6777629856779097\n",
      "Iteration 27300: Loss = 0.6777199702767811\n",
      "Iteration 27400: Loss = 0.6776769708485061\n",
      "Iteration 27500: Loss = 0.6776339873843784\n",
      "Iteration 27600: Loss = 0.6775910198756963\n",
      "Iteration 27700: Loss = 0.6775480683137634\n",
      "Iteration 27800: Loss = 0.6775051326898872\n",
      "Iteration 27900: Loss = 0.6774622129953803\n",
      "Iteration 28000: Loss = 0.6774193092215602\n",
      "Iteration 28100: Loss = 0.6773764213597488\n",
      "Iteration 28200: Loss = 0.6773335494012729\n",
      "Iteration 28300: Loss = 0.6772906933374641\n",
      "Iteration 28400: Loss = 0.6772478531596584\n",
      "Iteration 28500: Loss = 0.6772050288591965\n",
      "Iteration 28600: Loss = 0.6771622204274241\n",
      "Iteration 28700: Loss = 0.6771194278556917\n",
      "Iteration 28800: Loss = 0.6770766511353541\n",
      "Iteration 28900: Loss = 0.6770338902577709\n",
      "Iteration 29000: Loss = 0.6769911452143065\n",
      "Iteration 29100: Loss = 0.6769484159963302\n",
      "Iteration 29200: Loss = 0.6769057025952155\n",
      "Iteration 29300: Loss = 0.6768630050023411\n",
      "Iteration 29400: Loss = 0.67682032320909\n",
      "Iteration 29500: Loss = 0.6767776572068502\n",
      "Iteration 29600: Loss = 0.6767350069870143\n",
      "Iteration 29700: Loss = 0.6766923725409792\n",
      "Iteration 29800: Loss = 0.6766497538601473\n",
      "Iteration 29900: Loss = 0.6766071509359248\n",
      "Iteration 30000: Loss = 0.6765645637597233\n",
      "Iteration 30100: Loss = 0.6765219923229587\n",
      "Iteration 30200: Loss = 0.6764794366170518\n",
      "Iteration 30300: Loss = 0.6764368966334277\n",
      "Iteration 30400: Loss = 0.6763943723635165\n",
      "Iteration 30500: Loss = 0.676351863798753\n",
      "Iteration 30600: Loss = 0.6763093709305766\n",
      "Iteration 30700: Loss = 0.6762668937504313\n",
      "Iteration 30800: Loss = 0.6762244322497659\n",
      "Iteration 30900: Loss = 0.6761819864200338\n",
      "Iteration 31000: Loss = 0.6761395562526928\n",
      "Iteration 31100: Loss = 0.6760971417392063\n",
      "Iteration 31200: Loss = 0.6760547428710412\n",
      "Iteration 31300: Loss = 0.6760123596396697\n",
      "Iteration 31400: Loss = 0.6759699920365686\n",
      "Iteration 31500: Loss = 0.6759276400532191\n",
      "Iteration 31600: Loss = 0.6758853036811077\n",
      "Iteration 31700: Loss = 0.6758429829117247\n",
      "Iteration 31800: Loss = 0.6758006777365658\n",
      "Iteration 31900: Loss = 0.6757583881471311\n",
      "Iteration 32000: Loss = 0.6757161141349248\n",
      "Iteration 32100: Loss = 0.6756738556914567\n",
      "Iteration 32200: Loss = 0.6756316128082408\n",
      "Iteration 32300: Loss = 0.6755893854767957\n",
      "Iteration 32400: Loss = 0.6755471736886446\n",
      "Iteration 32500: Loss = 0.6755049774353157\n",
      "Iteration 32600: Loss = 0.6754627967083414\n",
      "Iteration 32700: Loss = 0.675420631499259\n",
      "Iteration 32800: Loss = 0.6753784817996106\n",
      "Iteration 32900: Loss = 0.6753363476009424\n",
      "Iteration 33000: Loss = 0.6752942288948056\n",
      "Iteration 33100: Loss = 0.6752521256727565\n",
      "Iteration 33200: Loss = 0.675210037926355\n",
      "Iteration 33300: Loss = 0.6751679656471667\n",
      "Iteration 33400: Loss = 0.6751259088267607\n",
      "Iteration 33500: Loss = 0.675083867456712\n",
      "Iteration 33600: Loss = 0.6750418415285991\n",
      "Iteration 33700: Loss = 0.6749998310340058\n",
      "Iteration 33800: Loss = 0.6749578359645205\n",
      "Iteration 33900: Loss = 0.674915856311736\n",
      "Iteration 34000: Loss = 0.6748738920672496\n",
      "Iteration 34100: Loss = 0.6748319432226636\n",
      "Iteration 34200: Loss = 0.6747900097695848\n",
      "Iteration 34300: Loss = 0.6747480916996245\n",
      "Iteration 34400: Loss = 0.6747061890043986\n",
      "Iteration 34500: Loss = 0.6746643016755277\n",
      "Iteration 34600: Loss = 0.6746224297046373\n",
      "Iteration 34700: Loss = 0.6745805730833568\n",
      "Iteration 34800: Loss = 0.6745387318033211\n",
      "Iteration 34900: Loss = 0.6744969058561691\n",
      "Iteration 35000: Loss = 0.6744550952335441\n",
      "Iteration 35100: Loss = 0.6744132999270949\n",
      "Iteration 35200: Loss = 0.6743715199284739\n",
      "Iteration 35300: Loss = 0.6743297552293392\n",
      "Iteration 35400: Loss = 0.6742880058213523\n",
      "Iteration 35500: Loss = 0.6742462716961801\n",
      "Iteration 35600: Loss = 0.674204552845494\n",
      "Iteration 35700: Loss = 0.67416284926097\n",
      "Iteration 35800: Loss = 0.6741211609342881\n",
      "Iteration 35900: Loss = 0.6740794878571338\n",
      "Iteration 36000: Loss = 0.6740378300211965\n",
      "Iteration 36100: Loss = 0.6739961874181708\n",
      "Iteration 36200: Loss = 0.6739545600397553\n",
      "Iteration 36300: Loss = 0.6739129478776534\n",
      "Iteration 36400: Loss = 0.6738713509235734\n",
      "Iteration 36500: Loss = 0.6738297691692279\n",
      "Iteration 36600: Loss = 0.6737882026063339\n",
      "Iteration 36700: Loss = 0.6737466512266133\n",
      "Iteration 36800: Loss = 0.6737051150217923\n",
      "Iteration 36900: Loss = 0.6736635939836021\n",
      "Iteration 37000: Loss = 0.6736220881037781\n",
      "Iteration 37100: Loss = 0.6735805973740604\n",
      "Iteration 37200: Loss = 0.6735391217861938\n",
      "Iteration 37300: Loss = 0.6734976613319273\n",
      "Iteration 37400: Loss = 0.6734562160030151\n",
      "Iteration 37500: Loss = 0.6734147857912152\n",
      "Iteration 37600: Loss = 0.6733733706882906\n",
      "Iteration 37700: Loss = 0.6733319706860093\n",
      "Iteration 37800: Loss = 0.6732905857761429\n",
      "Iteration 37900: Loss = 0.673249215950468\n",
      "Iteration 38000: Loss = 0.6732078612007664\n",
      "Iteration 38100: Loss = 0.6731665215188233\n",
      "Iteration 38200: Loss = 0.6731251968964294\n",
      "Iteration 38300: Loss = 0.6730838873253792\n",
      "Iteration 38400: Loss = 0.6730425927974726\n",
      "Iteration 38500: Loss = 0.6730013133045133\n",
      "Iteration 38600: Loss = 0.67296004883831\n",
      "Iteration 38700: Loss = 0.6729187993906757\n",
      "Iteration 38800: Loss = 0.6728775649534281\n",
      "Iteration 38900: Loss = 0.6728363455183894\n",
      "Iteration 39000: Loss = 0.6727951410773865\n",
      "Iteration 39100: Loss = 0.6727539516222505\n",
      "Iteration 39200: Loss = 0.6727127771448173\n",
      "Iteration 39300: Loss = 0.6726716176369274\n",
      "Iteration 39400: Loss = 0.6726304730904255\n",
      "Iteration 39500: Loss = 0.672589343497161\n",
      "Iteration 39600: Loss = 0.6725482288489882\n",
      "Iteration 39700: Loss = 0.6725071291377654\n",
      "Iteration 39800: Loss = 0.672466044355356\n",
      "Iteration 39900: Loss = 0.672424974493627\n",
      "Iteration 40000: Loss = 0.6723839195444509\n",
      "Iteration 40100: Loss = 0.6723428794997044\n",
      "Iteration 40200: Loss = 0.6723018543512683\n",
      "Iteration 40300: Loss = 0.6722608440910289\n",
      "Iteration 40400: Loss = 0.6722198487108759\n",
      "Iteration 40500: Loss = 0.6721788682027041\n",
      "Iteration 40600: Loss = 0.6721379025584129\n",
      "Iteration 40700: Loss = 0.6720969517699062\n",
      "Iteration 40800: Loss = 0.672056015829092\n",
      "Iteration 40900: Loss = 0.6720150947278835\n",
      "Iteration 41000: Loss = 0.6719741884581976\n",
      "Iteration 41100: Loss = 0.6719332970119564\n",
      "Iteration 41200: Loss = 0.671892420381086\n",
      "Iteration 41300: Loss = 0.6718515585575175\n",
      "Iteration 41400: Loss = 0.6718107115331863\n",
      "Iteration 41500: Loss = 0.671769879300032\n",
      "Iteration 41600: Loss = 0.6717290618499991\n",
      "Iteration 41700: Loss = 0.6716882591750366\n",
      "Iteration 41800: Loss = 0.6716474712670978\n",
      "Iteration 41900: Loss = 0.6716066981181403\n",
      "Iteration 42000: Loss = 0.6715659397201268\n",
      "Iteration 42100: Loss = 0.671525196065024\n",
      "Iteration 42200: Loss = 0.6714844671448034\n",
      "Iteration 42300: Loss = 0.6714437529514408\n",
      "Iteration 42400: Loss = 0.6714030534769162\n",
      "Iteration 42500: Loss = 0.671362368713215\n",
      "Iteration 42600: Loss = 0.671321698652326\n",
      "Iteration 42700: Loss = 0.6712810432862433\n",
      "Iteration 42800: Loss = 0.6712404026069649\n",
      "Iteration 42900: Loss = 0.6711997766064941\n",
      "Iteration 43000: Loss = 0.6711591652768375\n",
      "Iteration 43100: Loss = 0.6711185686100071\n",
      "Iteration 43200: Loss = 0.6710779865980192\n",
      "Iteration 43300: Loss = 0.6710374192328943\n",
      "Iteration 43400: Loss = 0.6709968665066576\n",
      "Iteration 43500: Loss = 0.6709563284113386\n",
      "Iteration 43600: Loss = 0.6709158049389716\n",
      "Iteration 43700: Loss = 0.670875296081595\n",
      "Iteration 43800: Loss = 0.670834801831252\n",
      "Iteration 43900: Loss = 0.6707943221799898\n",
      "Iteration 44000: Loss = 0.6707538571198606\n",
      "Iteration 44100: Loss = 0.6707134066429207\n",
      "Iteration 44200: Loss = 0.6706729707412309\n",
      "Iteration 44300: Loss = 0.6706325494068566\n",
      "Iteration 44400: Loss = 0.6705921426318675\n",
      "Iteration 44500: Loss = 0.6705517504083381\n",
      "Iteration 44600: Loss = 0.6705113727283468\n",
      "Iteration 44700: Loss = 0.6704710095839769\n",
      "Iteration 44800: Loss = 0.6704306609673157\n",
      "Iteration 44900: Loss = 0.6703903268704557\n",
      "Iteration 45000: Loss = 0.6703500072854932\n",
      "Iteration 45100: Loss = 0.670309702204529\n",
      "Iteration 45200: Loss = 0.6702694116196687\n",
      "Iteration 45300: Loss = 0.6702291355230217\n",
      "Iteration 45400: Loss = 0.6701888739067028\n",
      "Iteration 45500: Loss = 0.6701486267628304\n",
      "Iteration 45600: Loss = 0.6701083940835274\n",
      "Iteration 45700: Loss = 0.6700681758609217\n",
      "Iteration 45800: Loss = 0.6700279720871453\n",
      "Iteration 45900: Loss = 0.6699877827543345\n",
      "Iteration 46000: Loss = 0.6699476078546301\n",
      "Iteration 46100: Loss = 0.6699074473801776\n",
      "Iteration 46200: Loss = 0.6698673013231264\n",
      "Iteration 46300: Loss = 0.6698271696756309\n",
      "Iteration 46400: Loss = 0.6697870524298496\n",
      "Iteration 46500: Loss = 0.6697469495779454\n",
      "Iteration 46600: Loss = 0.6697068611120858\n",
      "Iteration 46700: Loss = 0.6696667870244425\n",
      "Iteration 46800: Loss = 0.6696267273071916\n",
      "Iteration 46900: Loss = 0.669586681952514\n",
      "Iteration 47000: Loss = 0.6695466509525948\n",
      "Iteration 47100: Loss = 0.6695066342996231\n",
      "Iteration 47200: Loss = 0.669466631985793\n",
      "Iteration 47300: Loss = 0.669426644003303\n",
      "Iteration 47400: Loss = 0.6693866703443554\n",
      "Iteration 47500: Loss = 0.6693467110011575\n",
      "Iteration 47600: Loss = 0.6693067659659208\n",
      "Iteration 47700: Loss = 0.669266835230861\n",
      "Iteration 47800: Loss = 0.6692269187881985\n",
      "Iteration 47900: Loss = 0.669187016630158\n",
      "Iteration 48000: Loss = 0.6691471287489688\n",
      "Iteration 48100: Loss = 0.6691072551368639\n",
      "Iteration 48200: Loss = 0.6690673957860814\n",
      "Iteration 48300: Loss = 0.6690275506888638\n",
      "Iteration 48400: Loss = 0.6689877198374573\n",
      "Iteration 48500: Loss = 0.6689479032241133\n",
      "Iteration 48600: Loss = 0.668908100841087\n",
      "Iteration 48700: Loss = 0.6688683126806382\n",
      "Iteration 48800: Loss = 0.6688285387350313\n",
      "Iteration 48900: Loss = 0.6687887789965345\n",
      "Iteration 49000: Loss = 0.668749033457421\n",
      "Iteration 49100: Loss = 0.668709302109968\n",
      "Iteration 49200: Loss = 0.6686695849464573\n",
      "Iteration 49300: Loss = 0.6686298819591748\n",
      "Iteration 49400: Loss = 0.6685901931404111\n",
      "Iteration 49500: Loss = 0.6685505184824607\n",
      "Iteration 49600: Loss = 0.6685108579776232\n",
      "Iteration 49700: Loss = 0.6684712116182018\n",
      "Iteration 49800: Loss = 0.6684315793965044\n",
      "Iteration 49900: Loss = 0.6683919613048435\n",
      "Iteration 50000: Loss = 0.6683523573355354\n",
      "Iteration 50100: Loss = 0.6683127674809013\n",
      "Iteration 50200: Loss = 0.6682731917332664\n",
      "Iteration 50300: Loss = 0.6682336300849606\n",
      "Iteration 50400: Loss = 0.6681940825283177\n",
      "Iteration 50500: Loss = 0.6681545490556761\n",
      "Iteration 50600: Loss = 0.6681150296593786\n",
      "Iteration 50700: Loss = 0.6680755243317725\n",
      "Iteration 50800: Loss = 0.6680360330652089\n",
      "Iteration 50900: Loss = 0.6679965558520436\n",
      "Iteration 51000: Loss = 0.6679570926846371\n",
      "Iteration 51100: Loss = 0.6679176435553535\n",
      "Iteration 51200: Loss = 0.6678782084565618\n",
      "Iteration 51300: Loss = 0.6678387873806348\n",
      "Iteration 51400: Loss = 0.6677993803199503\n",
      "Iteration 51500: Loss = 0.66775998726689\n",
      "Iteration 51600: Loss = 0.6677206082138404\n",
      "Iteration 51700: Loss = 0.6676812431531913\n",
      "Iteration 51800: Loss = 0.6676418920773379\n",
      "Iteration 51900: Loss = 0.6676025549786794\n",
      "Iteration 52000: Loss = 0.6675632318496189\n",
      "Iteration 52100: Loss = 0.6675239226825646\n",
      "Iteration 52200: Loss = 0.6674846274699282\n",
      "Iteration 52300: Loss = 0.6674453462041263\n",
      "Iteration 52400: Loss = 0.6674060788775797\n",
      "Iteration 52500: Loss = 0.6673668254827134\n",
      "Iteration 52600: Loss = 0.6673275860119569\n",
      "Iteration 52700: Loss = 0.6672883604577436\n",
      "Iteration 52800: Loss = 0.6672491488125116\n",
      "Iteration 52900: Loss = 0.6672099510687035\n",
      "Iteration 53000: Loss = 0.6671707672187652\n",
      "Iteration 53100: Loss = 0.6671315972551484\n",
      "Iteration 53200: Loss = 0.6670924411703079\n",
      "Iteration 53300: Loss = 0.6670532989567032\n",
      "Iteration 53400: Loss = 0.6670141706067981\n",
      "Iteration 53500: Loss = 0.6669750561130611\n",
      "Iteration 53600: Loss = 0.6669359554679641\n",
      "Iteration 53700: Loss = 0.6668968686639841\n",
      "Iteration 53800: Loss = 0.6668577956936022\n",
      "Iteration 53900: Loss = 0.6668187365493033\n",
      "Iteration 54000: Loss = 0.6667796912235775\n",
      "Iteration 54100: Loss = 0.666740659708918\n",
      "Iteration 54200: Loss = 0.6667016419978237\n",
      "Iteration 54300: Loss = 0.6666626380827966\n",
      "Iteration 54400: Loss = 0.6666236479563437\n",
      "Iteration 54500: Loss = 0.6665846716109755\n",
      "Iteration 54600: Loss = 0.6665457090392078\n",
      "Iteration 54700: Loss = 0.6665067602335599\n",
      "Iteration 54800: Loss = 0.6664678251865559\n",
      "Iteration 54900: Loss = 0.6664289038907235\n",
      "Iteration 55000: Loss = 0.6663899963385955\n",
      "Iteration 55100: Loss = 0.6663511025227082\n",
      "Iteration 55200: Loss = 0.6663122224356028\n",
      "Iteration 55300: Loss = 0.6662733560698244\n",
      "Iteration 55400: Loss = 0.6662345034179223\n",
      "Iteration 55500: Loss = 0.6661956644724505\n",
      "Iteration 55600: Loss = 0.6661568392259667\n",
      "Iteration 55700: Loss = 0.6661180276710333\n",
      "Iteration 55800: Loss = 0.6660792298002168\n",
      "Iteration 55900: Loss = 0.6660404456060879\n",
      "Iteration 56000: Loss = 0.6660016750812215\n",
      "Iteration 56100: Loss = 0.6659629182181972\n",
      "Iteration 56200: Loss = 0.6659241750095981\n",
      "Iteration 56300: Loss = 0.6658854454480121\n",
      "Iteration 56400: Loss = 0.6658467295260313\n",
      "Iteration 56500: Loss = 0.6658080272362519\n",
      "Iteration 56600: Loss = 0.6657693385712744\n",
      "Iteration 56700: Loss = 0.6657306635237035\n",
      "Iteration 56800: Loss = 0.665692002086148\n",
      "Iteration 56900: Loss = 0.6656533542512215\n",
      "Iteration 57000: Loss = 0.6656147200115411\n",
      "Iteration 57100: Loss = 0.6655760993597287\n",
      "Iteration 57200: Loss = 0.6655374922884101\n",
      "Iteration 57300: Loss = 0.6654988987902155\n",
      "Iteration 57400: Loss = 0.6654603188577792\n",
      "Iteration 57500: Loss = 0.6654217524837399\n",
      "Iteration 57600: Loss = 0.6653831996607402\n",
      "Iteration 57700: Loss = 0.6653446603814275\n",
      "Iteration 57800: Loss = 0.6653061346384528\n",
      "Iteration 57900: Loss = 0.6652676224244717\n",
      "Iteration 58000: Loss = 0.6652291237321438\n",
      "Iteration 58100: Loss = 0.6651906385541332\n",
      "Iteration 58200: Loss = 0.6651521668831079\n",
      "Iteration 58300: Loss = 0.6651137087117402\n",
      "Iteration 58400: Loss = 0.6650752640327069\n",
      "Iteration 58500: Loss = 0.6650368328386886\n",
      "Iteration 58600: Loss = 0.6649984151223702\n",
      "Iteration 58700: Loss = 0.6649600108764412\n",
      "Iteration 58800: Loss = 0.6649216200935946\n",
      "Iteration 58900: Loss = 0.6648832427665281\n",
      "Iteration 59000: Loss = 0.6648448788879436\n",
      "Iteration 59100: Loss = 0.664806528450547\n",
      "Iteration 59200: Loss = 0.6647681914470487\n",
      "Iteration 59300: Loss = 0.6647298678701626\n",
      "Iteration 59400: Loss = 0.6646915577126079\n",
      "Iteration 59500: Loss = 0.6646532609671068\n",
      "Iteration 59600: Loss = 0.6646149776263864\n",
      "Iteration 59700: Loss = 0.6645767076831781\n",
      "Iteration 59800: Loss = 0.664538451130217\n",
      "Iteration 59900: Loss = 0.6645002079602426\n",
      "Iteration 60000: Loss = 0.6644619781659986\n",
      "Iteration 60100: Loss = 0.664423761740233\n",
      "Iteration 60200: Loss = 0.6643855586756976\n",
      "Iteration 60300: Loss = 0.6643473689651488\n",
      "Iteration 60400: Loss = 0.6643091926013468\n",
      "Iteration 60500: Loss = 0.6642710295770565\n",
      "Iteration 60600: Loss = 0.6642328798850465\n",
      "Iteration 60700: Loss = 0.6641947435180897\n",
      "Iteration 60800: Loss = 0.6641566204689631\n",
      "Iteration 60900: Loss = 0.6641185107304479\n",
      "Iteration 61000: Loss = 0.6640804142953296\n",
      "Iteration 61100: Loss = 0.6640423311563979\n",
      "Iteration 61200: Loss = 0.6640042613064462\n",
      "Iteration 61300: Loss = 0.6639662047382726\n",
      "Iteration 61400: Loss = 0.6639281614446793\n",
      "Iteration 61500: Loss = 0.6638901314184722\n",
      "Iteration 61600: Loss = 0.6638521146524617\n",
      "Iteration 61700: Loss = 0.6638141111394624\n",
      "Iteration 61800: Loss = 0.663776120872293\n",
      "Iteration 61900: Loss = 0.6637381438437762\n",
      "Iteration 62000: Loss = 0.6637001800467386\n",
      "Iteration 62100: Loss = 0.663662229474012\n",
      "Iteration 62200: Loss = 0.663624292118431\n",
      "Iteration 62300: Loss = 0.6635863679728353\n",
      "Iteration 62400: Loss = 0.6635484570300685\n",
      "Iteration 62500: Loss = 0.6635105592829779\n",
      "Iteration 62600: Loss = 0.6634726747244154\n",
      "Iteration 62700: Loss = 0.6634348033472369\n",
      "Iteration 62800: Loss = 0.6633969451443026\n",
      "Iteration 62900: Loss = 0.6633591001084765\n",
      "Iteration 63000: Loss = 0.6633212682326268\n",
      "Iteration 63100: Loss = 0.6632834495096261\n",
      "Iteration 63200: Loss = 0.663245643932351\n",
      "Iteration 63300: Loss = 0.663207851493682\n",
      "Iteration 63400: Loss = 0.6631700721865038\n",
      "Iteration 63500: Loss = 0.6631323060037055\n",
      "Iteration 63600: Loss = 0.6630945529381801\n",
      "Iteration 63700: Loss = 0.6630568129828246\n",
      "Iteration 63800: Loss = 0.6630190861305405\n",
      "Iteration 63900: Loss = 0.6629813723742328\n",
      "Iteration 64000: Loss = 0.6629436717068111\n",
      "Iteration 64100: Loss = 0.6629059841211891\n",
      "Iteration 64200: Loss = 0.6628683096102842\n",
      "Iteration 64300: Loss = 0.6628306481670184\n",
      "Iteration 64400: Loss = 0.6627929997843177\n",
      "Iteration 64500: Loss = 0.6627553644551117\n",
      "Iteration 64600: Loss = 0.6627177421723348\n",
      "Iteration 64700: Loss = 0.662680132928925\n",
      "Iteration 64800: Loss = 0.6626425367178246\n",
      "Iteration 64900: Loss = 0.6626049535319799\n",
      "Iteration 65000: Loss = 0.6625673833643414\n",
      "Iteration 65100: Loss = 0.6625298262078637\n",
      "Iteration 65200: Loss = 0.6624922820555055\n",
      "Iteration 65300: Loss = 0.6624547509002291\n",
      "Iteration 65400: Loss = 0.6624172327350015\n",
      "Iteration 65500: Loss = 0.662379727552794\n",
      "Iteration 65600: Loss = 0.6623422353465809\n",
      "Iteration 65700: Loss = 0.6623047561093416\n",
      "Iteration 65800: Loss = 0.6622672898340589\n",
      "Iteration 65900: Loss = 0.6622298365137205\n",
      "Iteration 66000: Loss = 0.6621923961413171\n",
      "Iteration 66100: Loss = 0.6621549687098441\n",
      "Iteration 66200: Loss = 0.6621175542123012\n",
      "Iteration 66300: Loss = 0.6620801526416916\n",
      "Iteration 66400: Loss = 0.6620427639910229\n",
      "Iteration 66500: Loss = 0.6620053882533067\n",
      "Iteration 66600: Loss = 0.6619680254215584\n",
      "Iteration 66700: Loss = 0.6619306754887981\n",
      "Iteration 66800: Loss = 0.6618933384480494\n",
      "Iteration 66900: Loss = 0.6618560142923399\n",
      "Iteration 67000: Loss = 0.6618187030147016\n",
      "Iteration 67100: Loss = 0.6617814046081706\n",
      "Iteration 67200: Loss = 0.6617441190657866\n",
      "Iteration 67300: Loss = 0.6617068463805935\n",
      "Iteration 67400: Loss = 0.6616695865456398\n",
      "Iteration 67500: Loss = 0.6616323395539772\n",
      "Iteration 67600: Loss = 0.6615951053986621\n",
      "Iteration 67700: Loss = 0.6615578840727545\n",
      "Iteration 67800: Loss = 0.6615206755693187\n",
      "Iteration 67900: Loss = 0.661483479881423\n",
      "Iteration 68000: Loss = 0.6614462970021395\n",
      "Iteration 68100: Loss = 0.661409126924545\n",
      "Iteration 68200: Loss = 0.6613719696417193\n",
      "Iteration 68300: Loss = 0.661334825146747\n",
      "Iteration 68400: Loss = 0.6612976934327166\n",
      "Iteration 68500: Loss = 0.6612605744927205\n",
      "Iteration 68600: Loss = 0.661223468319855\n",
      "Iteration 68700: Loss = 0.6611863749072208\n",
      "Iteration 68800: Loss = 0.6611492942479221\n",
      "Iteration 68900: Loss = 0.6611122263350679\n",
      "Iteration 69000: Loss = 0.6610751711617703\n",
      "Iteration 69100: Loss = 0.6610381287211461\n",
      "Iteration 69200: Loss = 0.6610010990063158\n",
      "Iteration 69300: Loss = 0.660964082010404\n",
      "Iteration 69400: Loss = 0.6609270777265391\n",
      "Iteration 69500: Loss = 0.6608900861478539\n",
      "Iteration 69600: Loss = 0.6608531072674849\n",
      "Iteration 69700: Loss = 0.6608161410785728\n",
      "Iteration 69800: Loss = 0.6607791875742621\n",
      "Iteration 69900: Loss = 0.6607422467477014\n",
      "Iteration 70000: Loss = 0.6607053185920433\n",
      "Iteration 70100: Loss = 0.6606684031004444\n",
      "Iteration 70200: Loss = 0.6606315002660653\n",
      "Iteration 70300: Loss = 0.6605946100820707\n",
      "Iteration 70400: Loss = 0.6605577325416289\n",
      "Iteration 70500: Loss = 0.6605208676379127\n",
      "Iteration 70600: Loss = 0.6604840153640984\n",
      "Iteration 70700: Loss = 0.6604471757133669\n",
      "Iteration 70800: Loss = 0.6604103486789021\n",
      "Iteration 70900: Loss = 0.6603735342538933\n",
      "Iteration 71000: Loss = 0.6603367324315321\n",
      "Iteration 71100: Loss = 0.6602999432050157\n",
      "Iteration 71200: Loss = 0.6602631665675441\n",
      "Iteration 71300: Loss = 0.6602264025123218\n",
      "Iteration 71400: Loss = 0.6601896510325572\n",
      "Iteration 71500: Loss = 0.6601529121214625\n",
      "Iteration 71600: Loss = 0.6601161857722543\n",
      "Iteration 71700: Loss = 0.6600794719781526\n",
      "Iteration 71800: Loss = 0.6600427707323816\n",
      "Iteration 71900: Loss = 0.6600060820281696\n",
      "Iteration 72000: Loss = 0.659969405858749\n",
      "Iteration 72100: Loss = 0.6599327422173554\n",
      "Iteration 72200: Loss = 0.6598960910972295\n",
      "Iteration 72300: Loss = 0.6598594524916145\n",
      "Iteration 72400: Loss = 0.6598228263937591\n",
      "Iteration 72500: Loss = 0.6597862127969151\n",
      "Iteration 72600: Loss = 0.6597496116943382\n",
      "Iteration 72700: Loss = 0.659713023079288\n",
      "Iteration 72800: Loss = 0.6596764469450289\n",
      "Iteration 72900: Loss = 0.6596398832848281\n",
      "Iteration 73000: Loss = 0.6596033320919577\n",
      "Iteration 73100: Loss = 0.6595667933596926\n",
      "Iteration 73200: Loss = 0.6595302670813128\n",
      "Iteration 73300: Loss = 0.6594937532501021\n",
      "Iteration 73400: Loss = 0.6594572518593472\n",
      "Iteration 73500: Loss = 0.65942076290234\n",
      "Iteration 73600: Loss = 0.6593842863723753\n",
      "Iteration 73700: Loss = 0.6593478222627528\n",
      "Iteration 73800: Loss = 0.6593113705667754\n",
      "Iteration 73900: Loss = 0.6592749312777498\n",
      "Iteration 74000: Loss = 0.6592385043889873\n",
      "Iteration 74100: Loss = 0.6592020898938029\n",
      "Iteration 74200: Loss = 0.6591656877855154\n",
      "Iteration 74300: Loss = 0.6591292980574471\n",
      "Iteration 74400: Loss = 0.6590929207029251\n",
      "Iteration 74500: Loss = 0.6590565557152799\n",
      "Iteration 74600: Loss = 0.6590202030878457\n",
      "Iteration 74700: Loss = 0.6589838628139612\n",
      "Iteration 74800: Loss = 0.6589475348869684\n",
      "Iteration 74900: Loss = 0.6589112193002137\n",
      "Iteration 75000: Loss = 0.658874916047047\n",
      "Iteration 75100: Loss = 0.6588386251208224\n",
      "Iteration 75200: Loss = 0.658802346514898\n",
      "Iteration 75300: Loss = 0.6587660802226353\n",
      "Iteration 75400: Loss = 0.6587298262374003\n",
      "Iteration 75500: Loss = 0.6586935845525621\n",
      "Iteration 75600: Loss = 0.6586573551614946\n",
      "Iteration 75700: Loss = 0.6586211380575752\n",
      "Iteration 75800: Loss = 0.6585849332341848\n",
      "Iteration 75900: Loss = 0.6585487406847089\n",
      "Iteration 76000: Loss = 0.6585125604025364\n",
      "Iteration 76100: Loss = 0.6584763923810603\n",
      "Iteration 76200: Loss = 0.658440236613677\n",
      "Iteration 76300: Loss = 0.6584040930937878\n",
      "Iteration 76400: Loss = 0.658367961814797\n",
      "Iteration 76500: Loss = 0.6583318427701128\n",
      "Iteration 76600: Loss = 0.6582957359531477\n",
      "Iteration 76700: Loss = 0.6582596413573181\n",
      "Iteration 76800: Loss = 0.6582235589760438\n",
      "Iteration 76900: Loss = 0.6581874888027486\n",
      "Iteration 77000: Loss = 0.6581514308308606\n",
      "Iteration 77100: Loss = 0.6581153850538112\n",
      "Iteration 77200: Loss = 0.658079351465036\n",
      "Iteration 77300: Loss = 0.6580433300579746\n",
      "Iteration 77400: Loss = 0.6580073208260698\n",
      "Iteration 77500: Loss = 0.6579713237627691\n",
      "Iteration 77600: Loss = 0.6579353388615232\n",
      "Iteration 77700: Loss = 0.6578993661157869\n",
      "Iteration 77800: Loss = 0.657863405519019\n",
      "Iteration 77900: Loss = 0.6578274570646819\n",
      "Iteration 78000: Loss = 0.6577915207462421\n",
      "Iteration 78100: Loss = 0.6577555965571695\n",
      "Iteration 78200: Loss = 0.6577196844909384\n",
      "Iteration 78300: Loss = 0.6576837845410266\n",
      "Iteration 78400: Loss = 0.657647896700916\n",
      "Iteration 78500: Loss = 0.6576120209640918\n",
      "Iteration 78600: Loss = 0.6575761573240437\n",
      "Iteration 78700: Loss = 0.6575403057742648\n",
      "Iteration 78800: Loss = 0.6575044663082522\n",
      "Iteration 78900: Loss = 0.6574686389195069\n",
      "Iteration 79000: Loss = 0.6574328236015334\n",
      "Iteration 79100: Loss = 0.6573970203478405\n",
      "Iteration 79200: Loss = 0.6573612291519405\n",
      "Iteration 79300: Loss = 0.6573254500073494\n",
      "Iteration 79400: Loss = 0.6572896829075876\n",
      "Iteration 79500: Loss = 0.6572539278461786\n",
      "Iteration 79600: Loss = 0.6572181848166504\n",
      "Iteration 79700: Loss = 0.6571824538125343\n",
      "Iteration 79800: Loss = 0.6571467348273654\n",
      "Iteration 79900: Loss = 0.6571110278546832\n",
      "Iteration 80000: Loss = 0.6570753328880302\n",
      "Iteration 80100: Loss = 0.6570396499209537\n",
      "Iteration 80200: Loss = 0.6570039789470037\n",
      "Iteration 80300: Loss = 0.6569683199597346\n",
      "Iteration 80400: Loss = 0.6569326729527049\n",
      "Iteration 80500: Loss = 0.6568970379194761\n",
      "Iteration 80600: Loss = 0.6568614148536142\n",
      "Iteration 80700: Loss = 0.6568258037486888\n",
      "Iteration 80800: Loss = 0.6567902045982731\n",
      "Iteration 80900: Loss = 0.6567546173959442\n",
      "Iteration 81000: Loss = 0.6567190421352831\n",
      "Iteration 81100: Loss = 0.6566834788098745\n",
      "Iteration 81200: Loss = 0.6566479274133069\n",
      "Iteration 81300: Loss = 0.6566123879391725\n",
      "Iteration 81400: Loss = 0.6565768603810677\n",
      "Iteration 81500: Loss = 0.6565413447325918\n",
      "Iteration 81600: Loss = 0.6565058409873489\n",
      "Iteration 81700: Loss = 0.6564703491389462\n",
      "Iteration 81800: Loss = 0.6564348691809949\n",
      "Iteration 81900: Loss = 0.6563994011071099\n",
      "Iteration 82000: Loss = 0.6563639449109101\n",
      "Iteration 82100: Loss = 0.656328500586018\n",
      "Iteration 82200: Loss = 0.6562930681260596\n",
      "Iteration 82300: Loss = 0.6562576475246654\n",
      "Iteration 82400: Loss = 0.6562222387754687\n",
      "Iteration 82500: Loss = 0.6561868418721075\n",
      "Iteration 82600: Loss = 0.656151456808223\n",
      "Iteration 82700: Loss = 0.6561160835774601\n",
      "Iteration 82800: Loss = 0.656080722173468\n",
      "Iteration 82900: Loss = 0.6560453725898988\n",
      "Iteration 83000: Loss = 0.6560100348204094\n",
      "Iteration 83100: Loss = 0.6559747088586596\n",
      "Iteration 83200: Loss = 0.6559393946983134\n",
      "Iteration 83300: Loss = 0.6559040923330383\n",
      "Iteration 83400: Loss = 0.6558688017565057\n",
      "Iteration 83500: Loss = 0.655833522962391\n",
      "Iteration 83600: Loss = 0.6557982559443725\n",
      "Iteration 83700: Loss = 0.6557630006961331\n",
      "Iteration 83800: Loss = 0.6557277572113592\n",
      "Iteration 83900: Loss = 0.6556925254837407\n",
      "Iteration 84000: Loss = 0.6556573055069714\n",
      "Iteration 84100: Loss = 0.655622097274749\n",
      "Iteration 84200: Loss = 0.6555869007807744\n",
      "Iteration 84300: Loss = 0.6555517160187531\n",
      "Iteration 84400: Loss = 0.6555165429823935\n",
      "Iteration 84500: Loss = 0.6554813816654081\n",
      "Iteration 84600: Loss = 0.6554462320615133\n",
      "Iteration 84700: Loss = 0.6554110941644286\n",
      "Iteration 84800: Loss = 0.6553759679678778\n",
      "Iteration 84900: Loss = 0.6553408534655883\n",
      "Iteration 85000: Loss = 0.6553057506512913\n",
      "Iteration 85100: Loss = 0.6552706595187211\n",
      "Iteration 85200: Loss = 0.6552355800616165\n",
      "Iteration 85300: Loss = 0.6552005122737196\n",
      "Iteration 85400: Loss = 0.6551654561487764\n",
      "Iteration 85500: Loss = 0.6551304116805364\n",
      "Iteration 85600: Loss = 0.6550953788627532\n",
      "Iteration 85700: Loss = 0.6550603576891832\n",
      "Iteration 85800: Loss = 0.6550253481535877\n",
      "Iteration 85900: Loss = 0.654990350249731\n",
      "Iteration 86000: Loss = 0.654955363971381\n",
      "Iteration 86100: Loss = 0.6549203893123097\n",
      "Iteration 86200: Loss = 0.6548854262662925\n",
      "Iteration 86300: Loss = 0.6548504748271086\n",
      "Iteration 86400: Loss = 0.654815534988541\n",
      "Iteration 86500: Loss = 0.6547806067443762\n",
      "Iteration 86600: Loss = 0.6547456900884044\n",
      "Iteration 86700: Loss = 0.6547107850144198\n",
      "Iteration 86800: Loss = 0.6546758915162197\n",
      "Iteration 86900: Loss = 0.6546410095876056\n",
      "Iteration 87000: Loss = 0.6546061392223826\n",
      "Iteration 87100: Loss = 0.6545712804143591\n",
      "Iteration 87200: Loss = 0.6545364331573476\n",
      "Iteration 87300: Loss = 0.6545015974451642\n",
      "Iteration 87400: Loss = 0.6544667732716283\n",
      "Iteration 87500: Loss = 0.6544319606305635\n",
      "Iteration 87600: Loss = 0.6543971595157969\n",
      "Iteration 87700: Loss = 0.6543623699211589\n",
      "Iteration 87800: Loss = 0.6543275918404841\n",
      "Iteration 87900: Loss = 0.6542928252676106\n",
      "Iteration 88000: Loss = 0.6542580701963797\n",
      "Iteration 88100: Loss = 0.654223326620637\n",
      "Iteration 88200: Loss = 0.6541885945342315\n",
      "Iteration 88300: Loss = 0.6541538739310159\n",
      "Iteration 88400: Loss = 0.6541191648048464\n",
      "Iteration 88500: Loss = 0.6540844671495829\n",
      "Iteration 88600: Loss = 0.6540497809590892\n",
      "Iteration 88700: Loss = 0.6540151062272324\n",
      "Iteration 88800: Loss = 0.6539804429478835\n",
      "Iteration 88900: Loss = 0.6539457911149167\n",
      "Iteration 89000: Loss = 0.6539111507222106\n",
      "Iteration 89100: Loss = 0.653876521763647\n",
      "Iteration 89200: Loss = 0.6538419042331113\n",
      "Iteration 89300: Loss = 0.6538072981244923\n",
      "Iteration 89400: Loss = 0.6537727034316833\n",
      "Iteration 89500: Loss = 0.65373812014858\n",
      "Iteration 89600: Loss = 0.653703548269083\n",
      "Iteration 89700: Loss = 0.6536689877870954\n",
      "Iteration 89800: Loss = 0.6536344386965249\n",
      "Iteration 89900: Loss = 0.6535999009912822\n",
      "Iteration 90000: Loss = 0.6535653746652815\n",
      "Iteration 90100: Loss = 0.6535308597124416\n",
      "Iteration 90200: Loss = 0.6534963561266837\n",
      "Iteration 90300: Loss = 0.6534618639019331\n",
      "Iteration 90400: Loss = 0.653427383032119\n",
      "Iteration 90500: Loss = 0.6533929135111742\n",
      "Iteration 90600: Loss = 0.6533584553330345\n",
      "Iteration 90700: Loss = 0.6533240084916397\n",
      "Iteration 90800: Loss = 0.6532895729809337\n",
      "Iteration 90900: Loss = 0.653255148794863\n",
      "Iteration 91000: Loss = 0.6532207359273784\n",
      "Iteration 91100: Loss = 0.6531863343724341\n",
      "Iteration 91200: Loss = 0.6531519441239881\n",
      "Iteration 91300: Loss = 0.6531175651760018\n",
      "Iteration 91400: Loss = 0.6530831975224399\n",
      "Iteration 91500: Loss = 0.6530488411572714\n",
      "Iteration 91600: Loss = 0.6530144960744683\n",
      "Iteration 91700: Loss = 0.6529801622680066\n",
      "Iteration 91800: Loss = 0.6529458397318654\n",
      "Iteration 91900: Loss = 0.6529115284600279\n",
      "Iteration 92000: Loss = 0.6528772284464806\n",
      "Iteration 92100: Loss = 0.6528429396852136\n",
      "Iteration 92200: Loss = 0.6528086621702207\n",
      "Iteration 92300: Loss = 0.6527743958954992\n",
      "Iteration 92400: Loss = 0.6527401408550503\n",
      "Iteration 92500: Loss = 0.6527058970428777\n",
      "Iteration 92600: Loss = 0.6526716644529901\n",
      "Iteration 92700: Loss = 0.6526374430793991\n",
      "Iteration 92800: Loss = 0.6526032329161194\n",
      "Iteration 92900: Loss = 0.6525690339571703\n",
      "Iteration 93000: Loss = 0.6525348461965738\n",
      "Iteration 93100: Loss = 0.6525006696283557\n",
      "Iteration 93200: Loss = 0.6524665042465458\n",
      "Iteration 93300: Loss = 0.6524323500451769\n",
      "Iteration 93400: Loss = 0.6523982070182857\n",
      "Iteration 93500: Loss = 0.652364075159912\n",
      "Iteration 93600: Loss = 0.6523299544640999\n",
      "Iteration 93700: Loss = 0.6522958449248962\n",
      "Iteration 93800: Loss = 0.6522617465363523\n",
      "Iteration 93900: Loss = 0.652227659292522\n",
      "Iteration 94000: Loss = 0.6521935831874635\n",
      "Iteration 94100: Loss = 0.652159518215238\n",
      "Iteration 94200: Loss = 0.6521254643699106\n",
      "Iteration 94300: Loss = 0.6520914216455497\n",
      "Iteration 94400: Loss = 0.6520573900362276\n",
      "Iteration 94500: Loss = 0.6520233695360198\n",
      "Iteration 94600: Loss = 0.6519893601390053\n",
      "Iteration 94700: Loss = 0.651955361839267\n",
      "Iteration 94800: Loss = 0.6519213746308912\n",
      "Iteration 94900: Loss = 0.6518873985079673\n",
      "Iteration 95000: Loss = 0.6518534334645886\n",
      "Iteration 95100: Loss = 0.6518194794948521\n",
      "Iteration 95200: Loss = 0.651785536592858\n",
      "Iteration 95300: Loss = 0.6517516047527103\n",
      "Iteration 95400: Loss = 0.6517176839685161\n",
      "Iteration 95500: Loss = 0.6516837742343867\n",
      "Iteration 95600: Loss = 0.6516498755444361\n",
      "Iteration 95700: Loss = 0.6516159878927824\n",
      "Iteration 95800: Loss = 0.6515821112735473\n",
      "Iteration 95900: Loss = 0.6515482456808552\n",
      "Iteration 96000: Loss = 0.651514391108835\n",
      "Iteration 96100: Loss = 0.6514805475516187\n",
      "Iteration 96200: Loss = 0.6514467150033414\n",
      "Iteration 96300: Loss = 0.6514128934581424\n",
      "Iteration 96400: Loss = 0.651379082910164\n",
      "Iteration 96500: Loss = 0.6513452833535524\n",
      "Iteration 96600: Loss = 0.6513114947824571\n",
      "Iteration 96700: Loss = 0.6512777171910309\n",
      "Iteration 96800: Loss = 0.6512439505734304\n",
      "Iteration 96900: Loss = 0.6512101949238156\n",
      "Iteration 97000: Loss = 0.6511764502363497\n",
      "Iteration 97100: Loss = 0.6511427165052003\n",
      "Iteration 97200: Loss = 0.6511089937245372\n",
      "Iteration 97300: Loss = 0.6510752818885349\n",
      "Iteration 97400: Loss = 0.6510415809913704\n",
      "Iteration 97500: Loss = 0.6510078910272248\n",
      "Iteration 97600: Loss = 0.6509742119902824\n",
      "Iteration 97700: Loss = 0.6509405438747312\n",
      "Iteration 97800: Loss = 0.6509068866747624\n",
      "Iteration 97900: Loss = 0.6508732403845711\n",
      "Iteration 98000: Loss = 0.6508396049983552\n",
      "Iteration 98100: Loss = 0.6508059805103169\n",
      "Iteration 98200: Loss = 0.6507723669146611\n",
      "Iteration 98300: Loss = 0.6507387642055968\n",
      "Iteration 98400: Loss = 0.6507051723773359\n",
      "Iteration 98500: Loss = 0.6506715914240944\n",
      "Iteration 98600: Loss = 0.6506380213400909\n",
      "Iteration 98700: Loss = 0.6506044621195484\n",
      "Iteration 98800: Loss = 0.6505709137566927\n",
      "Iteration 98900: Loss = 0.6505373762457535\n",
      "Iteration 99000: Loss = 0.6505038495809635\n",
      "Iteration 99100: Loss = 0.6504703337565593\n",
      "Iteration 99200: Loss = 0.6504368287667808\n",
      "Iteration 99300: Loss = 0.650403334605871\n",
      "Iteration 99400: Loss = 0.6503698512680768\n",
      "Iteration 99500: Loss = 0.6503363787476484\n",
      "Iteration 99600: Loss = 0.6503029170388395\n",
      "Iteration 99700: Loss = 0.6502694661359072\n",
      "Iteration 99800: Loss = 0.6502360260331118\n",
      "Iteration 99900: Loss = 0.6502025967247175\n",
      "Iteration 100000: Loss = 0.6501691782049914\n",
      "Iteration 100100: Loss = 0.6501357704682047\n",
      "Iteration 100200: Loss = 0.6501023735086315\n",
      "Iteration 100300: Loss = 0.6500689873205495\n",
      "Iteration 100400: Loss = 0.6500356118982398\n",
      "Iteration 100500: Loss = 0.6500022472359871\n",
      "Iteration 100600: Loss = 0.6499688933280792\n",
      "Iteration 100700: Loss = 0.6499355501688073\n",
      "Iteration 100800: Loss = 0.6499022177524669\n",
      "Iteration 100900: Loss = 0.6498688960733556\n",
      "Iteration 101000: Loss = 0.6498355851257753\n",
      "Iteration 101100: Loss = 0.6498022849040312\n",
      "Iteration 101200: Loss = 0.6497689954024318\n",
      "Iteration 101300: Loss = 0.649735716615289\n",
      "Iteration 101400: Loss = 0.6497024485369177\n",
      "Iteration 101500: Loss = 0.6496691911616371\n",
      "Iteration 101600: Loss = 0.6496359444837693\n",
      "Iteration 101700: Loss = 0.6496027084976397\n",
      "Iteration 101800: Loss = 0.6495694831975773\n",
      "Iteration 101900: Loss = 0.6495362685779146\n",
      "Iteration 102000: Loss = 0.649503064632987\n",
      "Iteration 102100: Loss = 0.6494698713571341\n",
      "Iteration 102200: Loss = 0.6494366887446981\n",
      "Iteration 102300: Loss = 0.6494035167900252\n",
      "Iteration 102400: Loss = 0.6493703554874645\n",
      "Iteration 102500: Loss = 0.6493372048313688\n",
      "Iteration 102600: Loss = 0.6493040648160943\n",
      "Iteration 102700: Loss = 0.6492709354360006\n",
      "Iteration 102800: Loss = 0.6492378166854502\n",
      "Iteration 102900: Loss = 0.6492047085588099\n",
      "Iteration 103000: Loss = 0.6491716110504487\n",
      "Iteration 103100: Loss = 0.6491385241547406\n",
      "Iteration 103200: Loss = 0.6491054478660611\n",
      "Iteration 103300: Loss = 0.6490723821787903\n",
      "Iteration 103400: Loss = 0.6490393270873114\n",
      "Iteration 103500: Loss = 0.649006282586011\n",
      "Iteration 103600: Loss = 0.648973248669279\n",
      "Iteration 103700: Loss = 0.6489402253315085\n",
      "Iteration 103800: Loss = 0.6489072125670963\n",
      "Iteration 103900: Loss = 0.6488742103704427\n",
      "Iteration 104000: Loss = 0.6488412187359504\n",
      "Iteration 104100: Loss = 0.6488082376580265\n",
      "Iteration 104200: Loss = 0.6487752671310812\n",
      "Iteration 104300: Loss = 0.648742307149528\n",
      "Iteration 104400: Loss = 0.6487093577077834\n",
      "Iteration 104500: Loss = 0.6486764188002678\n",
      "Iteration 104600: Loss = 0.6486434904214046\n",
      "Iteration 104700: Loss = 0.6486105725656208\n",
      "Iteration 104800: Loss = 0.6485776652273464\n",
      "Iteration 104900: Loss = 0.6485447684010153\n",
      "Iteration 105000: Loss = 0.6485118820810641\n",
      "Iteration 105100: Loss = 0.6484790062619332\n",
      "Iteration 105200: Loss = 0.6484461409380664\n",
      "Iteration 105300: Loss = 0.64841328610391\n",
      "Iteration 105400: Loss = 0.6483804417539151\n",
      "Iteration 105500: Loss = 0.6483476078825346\n",
      "Iteration 105600: Loss = 0.648314784484226\n",
      "Iteration 105700: Loss = 0.6482819715534492\n",
      "Iteration 105800: Loss = 0.6482491690846679\n",
      "Iteration 105900: Loss = 0.6482163770723491\n",
      "Iteration 106000: Loss = 0.648183595510963\n",
      "Iteration 106100: Loss = 0.6481508243949834\n",
      "Iteration 106200: Loss = 0.648118063718887\n",
      "Iteration 106300: Loss = 0.6480853134771539\n",
      "Iteration 106400: Loss = 0.648052573664268\n",
      "Iteration 106500: Loss = 0.648019844274716\n",
      "Iteration 106600: Loss = 0.647987125302988\n",
      "Iteration 106700: Loss = 0.6479544167435778\n",
      "Iteration 106800: Loss = 0.6479217185909819\n",
      "Iteration 106900: Loss = 0.6478890308397006\n",
      "Iteration 107000: Loss = 0.6478563534842373\n",
      "Iteration 107100: Loss = 0.6478236865190985\n",
      "Iteration 107200: Loss = 0.6477910299387949\n",
      "Iteration 107300: Loss = 0.6477583837378392\n",
      "Iteration 107400: Loss = 0.6477257479107482\n",
      "Iteration 107500: Loss = 0.6476931224520421\n",
      "Iteration 107600: Loss = 0.647660507356244\n",
      "Iteration 107700: Loss = 0.6476279026178804\n",
      "Iteration 107800: Loss = 0.6475953082314811\n",
      "Iteration 107900: Loss = 0.6475627241915795\n",
      "Iteration 108000: Loss = 0.6475301504927116\n",
      "Iteration 108100: Loss = 0.6474975871294175\n",
      "Iteration 108200: Loss = 0.6474650340962401\n",
      "Iteration 108300: Loss = 0.6474324913877256\n",
      "Iteration 108400: Loss = 0.6473999589984236\n",
      "Iteration 108500: Loss = 0.647367436922887\n",
      "Iteration 108600: Loss = 0.6473349251556719\n",
      "Iteration 108700: Loss = 0.6473024236913378\n",
      "Iteration 108800: Loss = 0.6472699325244472\n",
      "Iteration 108900: Loss = 0.6472374516495661\n",
      "Iteration 109000: Loss = 0.6472049810612639\n",
      "Iteration 109100: Loss = 0.6471725207541131\n",
      "Iteration 109200: Loss = 0.6471400707226892\n",
      "Iteration 109300: Loss = 0.6471076309615715\n",
      "Iteration 109400: Loss = 0.647075201465342\n",
      "Iteration 109500: Loss = 0.6470427822285869\n",
      "Iteration 109600: Loss = 0.6470103732458943\n",
      "Iteration 109700: Loss = 0.6469779745118567\n",
      "Iteration 109800: Loss = 0.6469455860210692\n",
      "Iteration 109900: Loss = 0.646913207768131\n",
      "Iteration 110000: Loss = 0.6468808397476432\n",
      "Iteration 110100: Loss = 0.6468484819542112\n",
      "Iteration 110200: Loss = 0.6468161343824436\n",
      "Iteration 110300: Loss = 0.6467837970269517\n",
      "Iteration 110400: Loss = 0.6467514698823504\n",
      "Iteration 110500: Loss = 0.646719152943258\n",
      "Iteration 110600: Loss = 0.6466868462042956\n",
      "Iteration 110700: Loss = 0.646654549660088\n",
      "Iteration 110800: Loss = 0.6466222633052631\n",
      "Iteration 110900: Loss = 0.6465899871344515\n",
      "Iteration 111000: Loss = 0.646557721142288\n",
      "Iteration 111100: Loss = 0.64652546532341\n",
      "Iteration 111200: Loss = 0.6464932196724581\n",
      "Iteration 111300: Loss = 0.6464609841840765\n",
      "Iteration 111400: Loss = 0.6464287588529125\n",
      "Iteration 111500: Loss = 0.6463965436736164\n",
      "Iteration 111600: Loss = 0.6463643386408419\n",
      "Iteration 111700: Loss = 0.6463321437492459\n",
      "Iteration 111800: Loss = 0.6462999589934885\n",
      "Iteration 111900: Loss = 0.6462677843682332\n",
      "Iteration 112000: Loss = 0.6462356198681466\n",
      "Iteration 112100: Loss = 0.6462034654878982\n",
      "Iteration 112200: Loss = 0.6461713212221614\n",
      "Iteration 112300: Loss = 0.6461391870656119\n",
      "Iteration 112400: Loss = 0.6461070630129296\n",
      "Iteration 112500: Loss = 0.646074949058797\n",
      "Iteration 112600: Loss = 0.6460428451978999\n",
      "Iteration 112700: Loss = 0.6460107514249275\n",
      "Iteration 112800: Loss = 0.6459786677345717\n",
      "Iteration 112900: Loss = 0.6459465941215282\n",
      "Iteration 113000: Loss = 0.6459145305804956\n",
      "Iteration 113100: Loss = 0.645882477106176\n",
      "Iteration 113200: Loss = 0.645850433693274\n",
      "Iteration 113300: Loss = 0.6458184003364983\n",
      "Iteration 113400: Loss = 0.6457863770305601\n",
      "Iteration 113500: Loss = 0.645754363770174\n",
      "Iteration 113600: Loss = 0.6457223605500579\n",
      "Iteration 113700: Loss = 0.6456903673649329\n",
      "Iteration 113800: Loss = 0.6456583842095229\n",
      "Iteration 113900: Loss = 0.6456264110785556\n",
      "Iteration 114000: Loss = 0.6455944479667615\n",
      "Iteration 114100: Loss = 0.6455624948688743\n",
      "Iteration 114200: Loss = 0.6455305517796308\n",
      "Iteration 114300: Loss = 0.6454986186937712\n",
      "Iteration 114400: Loss = 0.6454666956060391\n",
      "Iteration 114500: Loss = 0.6454347825111805\n",
      "Iteration 114600: Loss = 0.6454028794039451\n",
      "Iteration 114700: Loss = 0.6453709862790858\n",
      "Iteration 114800: Loss = 0.6453391031313584\n",
      "Iteration 114900: Loss = 0.6453072299555224\n",
      "Iteration 115000: Loss = 0.6452753667463398\n",
      "Iteration 115100: Loss = 0.645243513498576\n",
      "Iteration 115200: Loss = 0.6452116702069998\n",
      "Iteration 115300: Loss = 0.6451798368663828\n",
      "Iteration 115400: Loss = 0.6451480134715002\n",
      "Iteration 115500: Loss = 0.6451162000171297\n",
      "Iteration 115600: Loss = 0.645084396498053\n",
      "Iteration 115700: Loss = 0.6450526029090543\n",
      "Iteration 115800: Loss = 0.6450208192449208\n",
      "Iteration 115900: Loss = 0.6449890455004437\n",
      "Iteration 116000: Loss = 0.6449572816704168\n",
      "Iteration 116100: Loss = 0.6449255277496366\n",
      "Iteration 116200: Loss = 0.6448937837329038\n",
      "Iteration 116300: Loss = 0.6448620496150216\n",
      "Iteration 116400: Loss = 0.644830325390796\n",
      "Iteration 116500: Loss = 0.644798611055037\n",
      "Iteration 116600: Loss = 0.644766906602557\n",
      "Iteration 116700: Loss = 0.6447352120281717\n",
      "Iteration 116800: Loss = 0.6447035273267007\n",
      "Iteration 116900: Loss = 0.6446718524929654\n",
      "Iteration 117000: Loss = 0.6446401875217913\n",
      "Iteration 117100: Loss = 0.6446085324080069\n",
      "Iteration 117200: Loss = 0.6445768871464432\n",
      "Iteration 117300: Loss = 0.6445452517319352\n",
      "Iteration 117400: Loss = 0.6445136261593207\n",
      "Iteration 117500: Loss = 0.6444820104234401\n",
      "Iteration 117600: Loss = 0.6444504045191376\n",
      "Iteration 117700: Loss = 0.6444188084412603\n",
      "Iteration 117800: Loss = 0.6443872221846585\n",
      "Iteration 117900: Loss = 0.6443556457441851\n",
      "Iteration 118000: Loss = 0.644324079114697\n",
      "Iteration 118100: Loss = 0.6442925222910533\n",
      "Iteration 118200: Loss = 0.6442609752681169\n",
      "Iteration 118300: Loss = 0.6442294380407533\n",
      "Iteration 118400: Loss = 0.6441979106038316\n",
      "Iteration 118500: Loss = 0.6441663929522236\n",
      "Iteration 118600: Loss = 0.6441348850808042\n",
      "Iteration 118700: Loss = 0.6441033869844518\n",
      "Iteration 118800: Loss = 0.6440718986580476\n",
      "Iteration 118900: Loss = 0.6440404200964758\n",
      "Iteration 119000: Loss = 0.644008951294624\n",
      "Iteration 119100: Loss = 0.6439774922473825\n",
      "Iteration 119200: Loss = 0.6439460429496452\n",
      "Iteration 119300: Loss = 0.6439146033963085\n",
      "Iteration 119400: Loss = 0.6438831735822723\n",
      "Iteration 119500: Loss = 0.6438517535024396\n",
      "Iteration 119600: Loss = 0.6438203431517162\n",
      "Iteration 119700: Loss = 0.6437889425250113\n",
      "Iteration 119800: Loss = 0.6437575516172368\n",
      "Iteration 119900: Loss = 0.643726170423308\n",
      "Iteration 120000: Loss = 0.6436947989381432\n",
      "Iteration 120100: Loss = 0.6436634371566637\n",
      "Iteration 120200: Loss = 0.6436320850737938\n",
      "Iteration 120300: Loss = 0.6436007426844614\n",
      "Iteration 120400: Loss = 0.6435694099835966\n",
      "Iteration 120500: Loss = 0.6435380869661332\n",
      "Iteration 120600: Loss = 0.6435067736270081\n",
      "Iteration 120700: Loss = 0.6434754699611606\n",
      "Iteration 120800: Loss = 0.643444175963534\n",
      "Iteration 120900: Loss = 0.6434128916290738\n",
      "Iteration 121000: Loss = 0.6433816169527292\n",
      "Iteration 121100: Loss = 0.6433503519294521\n",
      "Iteration 121200: Loss = 0.6433190965541974\n",
      "Iteration 121300: Loss = 0.6432878508219234\n",
      "Iteration 121400: Loss = 0.6432566147275913\n",
      "Iteration 121500: Loss = 0.643225388266165\n",
      "Iteration 121600: Loss = 0.6431941714326121\n",
      "Iteration 121700: Loss = 0.6431629642219028\n",
      "Iteration 121800: Loss = 0.6431317666290104\n",
      "Iteration 121900: Loss = 0.6431005786489111\n",
      "Iteration 122000: Loss = 0.6430694002765848\n",
      "Iteration 122100: Loss = 0.6430382315070136\n",
      "Iteration 122200: Loss = 0.6430070723351831\n",
      "Iteration 122300: Loss = 0.6429759227560816\n",
      "Iteration 122400: Loss = 0.6429447827647011\n",
      "Iteration 122500: Loss = 0.6429136523560361\n",
      "Iteration 122600: Loss = 0.6428825315250841\n",
      "Iteration 122700: Loss = 0.6428514202668457\n",
      "Iteration 122800: Loss = 0.6428203185763248\n",
      "Iteration 122900: Loss = 0.6427892264485281\n",
      "Iteration 123000: Loss = 0.6427581438784655\n",
      "Iteration 123100: Loss = 0.6427270708611494\n",
      "Iteration 123200: Loss = 0.6426960073915957\n",
      "Iteration 123300: Loss = 0.6426649534648233\n",
      "Iteration 123400: Loss = 0.6426339090758542\n",
      "Iteration 123500: Loss = 0.6426028742197128\n",
      "Iteration 123600: Loss = 0.6425718488914273\n",
      "Iteration 123700: Loss = 0.6425408330860284\n",
      "Iteration 123800: Loss = 0.64250982679855\n",
      "Iteration 123900: Loss = 0.642478830024029\n",
      "Iteration 124000: Loss = 0.6424478427575053\n",
      "Iteration 124100: Loss = 0.6424168649940218\n",
      "Iteration 124200: Loss = 0.6423858967286245\n",
      "Iteration 124300: Loss = 0.6423549379563621\n",
      "Iteration 124400: Loss = 0.6423239886722866\n",
      "Iteration 124500: Loss = 0.6422930488714528\n",
      "Iteration 124600: Loss = 0.6422621185489188\n",
      "Iteration 124700: Loss = 0.6422311976997453\n",
      "Iteration 124800: Loss = 0.6422002863189962\n",
      "Iteration 124900: Loss = 0.6421693844017385\n",
      "Iteration 125000: Loss = 0.6421384919430421\n",
      "Iteration 125100: Loss = 0.6421076089379795\n",
      "Iteration 125200: Loss = 0.6420767353816268\n",
      "Iteration 125300: Loss = 0.6420458712690629\n",
      "Iteration 125400: Loss = 0.6420150165953694\n",
      "Iteration 125500: Loss = 0.641984171355631\n",
      "Iteration 125600: Loss = 0.6419533355449358\n",
      "Iteration 125700: Loss = 0.6419225091583743\n",
      "Iteration 125800: Loss = 0.6418916921910403\n",
      "Iteration 125900: Loss = 0.6418608846380304\n",
      "Iteration 126000: Loss = 0.6418300864944443\n",
      "Iteration 126100: Loss = 0.6417992977553846\n",
      "Iteration 126200: Loss = 0.6417685184159567\n",
      "Iteration 126300: Loss = 0.6417377484712697\n",
      "Iteration 126400: Loss = 0.6417069879164347\n",
      "Iteration 126500: Loss = 0.6416762367465663\n",
      "Iteration 126600: Loss = 0.641645494956782\n",
      "Iteration 126700: Loss = 0.6416147625422022\n",
      "Iteration 126800: Loss = 0.6415840394979501\n",
      "Iteration 126900: Loss = 0.6415533258191521\n",
      "Iteration 127000: Loss = 0.6415226215009376\n",
      "Iteration 127100: Loss = 0.641491926538439\n",
      "Iteration 127200: Loss = 0.641461240926791\n",
      "Iteration 127300: Loss = 0.6414305646611321\n",
      "Iteration 127400: Loss = 0.6413998977366032\n",
      "Iteration 127500: Loss = 0.6413692401483486\n",
      "Iteration 127600: Loss = 0.6413385918915149\n",
      "Iteration 127700: Loss = 0.6413079529612522\n",
      "Iteration 127800: Loss = 0.6412773233527135\n",
      "Iteration 127900: Loss = 0.6412467030610544\n",
      "Iteration 128000: Loss = 0.6412160920814337\n",
      "Iteration 128100: Loss = 0.641185490409013\n",
      "Iteration 128200: Loss = 0.641154898038957\n",
      "Iteration 128300: Loss = 0.6411243149664332\n",
      "Iteration 128400: Loss = 0.6410937411866121\n",
      "Iteration 128500: Loss = 0.6410631766946671\n",
      "Iteration 128600: Loss = 0.6410326214857743\n",
      "Iteration 128700: Loss = 0.6410020755551133\n",
      "Iteration 128800: Loss = 0.6409715388978661\n",
      "Iteration 128900: Loss = 0.6409410115092179\n",
      "Iteration 129000: Loss = 0.6409104933843566\n",
      "Iteration 129100: Loss = 0.6408799845184732\n",
      "Iteration 129200: Loss = 0.6408494849067613\n",
      "Iteration 129300: Loss = 0.6408189945444183\n",
      "Iteration 129400: Loss = 0.6407885134266431\n",
      "Iteration 129500: Loss = 0.6407580415486388\n",
      "Iteration 129600: Loss = 0.6407275789056108\n",
      "Iteration 129700: Loss = 0.6406971254927675\n",
      "Iteration 129800: Loss = 0.6406666813053201\n",
      "Iteration 129900: Loss = 0.6406362463384829\n",
      "Iteration 130000: Loss = 0.6406058205874733\n",
      "Iteration 130100: Loss = 0.640575404047511\n",
      "Iteration 130200: Loss = 0.6405449967138189\n",
      "Iteration 130300: Loss = 0.6405145985816231\n",
      "Iteration 130400: Loss = 0.6404842096461522\n",
      "Iteration 130500: Loss = 0.6404538299026377\n",
      "Iteration 130600: Loss = 0.6404234593463141\n",
      "Iteration 130700: Loss = 0.6403930979724191\n",
      "Iteration 130800: Loss = 0.6403627457761929\n",
      "Iteration 130900: Loss = 0.6403324027528784\n",
      "Iteration 131000: Loss = 0.6403020688977219\n",
      "Iteration 131100: Loss = 0.6402717442059723\n",
      "Iteration 131200: Loss = 0.6402414286728816\n",
      "Iteration 131300: Loss = 0.6402111222937044\n",
      "Iteration 131400: Loss = 0.640180825063698\n",
      "Iteration 131500: Loss = 0.6401505369781235\n",
      "Iteration 131600: Loss = 0.6401202580322437\n",
      "Iteration 131700: Loss = 0.6400899882213252\n",
      "Iteration 131800: Loss = 0.640059727540637\n",
      "Iteration 131900: Loss = 0.6400294759854511\n",
      "Iteration 132000: Loss = 0.6399992335510422\n",
      "Iteration 132100: Loss = 0.639969000232688\n",
      "Iteration 132200: Loss = 0.6399387760256692\n",
      "Iteration 132300: Loss = 0.6399085609252693\n",
      "Iteration 132400: Loss = 0.6398783549267745\n",
      "Iteration 132500: Loss = 0.6398481580254739\n",
      "Iteration 132600: Loss = 0.6398179702166596\n",
      "Iteration 132700: Loss = 0.6397877914956266\n",
      "Iteration 132800: Loss = 0.6397576218576726\n",
      "Iteration 132900: Loss = 0.639727461298098\n",
      "Iteration 133000: Loss = 0.6396973098122064\n",
      "Iteration 133100: Loss = 0.6396671673953042\n",
      "Iteration 133200: Loss = 0.6396370340427004\n",
      "Iteration 133300: Loss = 0.639606909749707\n",
      "Iteration 133400: Loss = 0.6395767945116388\n",
      "Iteration 133500: Loss = 0.6395466883238138\n",
      "Iteration 133600: Loss = 0.639516591181552\n",
      "Iteration 133700: Loss = 0.6394865030801773\n",
      "Iteration 133800: Loss = 0.6394564240150157\n",
      "Iteration 133900: Loss = 0.639426353981396\n",
      "Iteration 134000: Loss = 0.6393962929746505\n",
      "Iteration 134100: Loss = 0.6393662409901137\n",
      "Iteration 134200: Loss = 0.6393361980231231\n",
      "Iteration 134300: Loss = 0.6393061640690193\n",
      "Iteration 134400: Loss = 0.6392761391231453\n",
      "Iteration 134500: Loss = 0.6392461231808472\n",
      "Iteration 134600: Loss = 0.6392161162374741\n",
      "Iteration 134700: Loss = 0.6391861182883772\n",
      "Iteration 134800: Loss = 0.6391561293289115\n",
      "Iteration 134900: Loss = 0.6391261493544341\n",
      "Iteration 135000: Loss = 0.6390961783603052\n",
      "Iteration 135100: Loss = 0.6390662163418878\n",
      "Iteration 135200: Loss = 0.6390362632945475\n",
      "Iteration 135300: Loss = 0.6390063192136533\n",
      "Iteration 135400: Loss = 0.6389763840945761\n",
      "Iteration 135500: Loss = 0.6389464579326906\n",
      "Iteration 135600: Loss = 0.6389165407233736\n",
      "Iteration 135700: Loss = 0.6388866324620049\n",
      "Iteration 135800: Loss = 0.6388567331439672\n",
      "Iteration 135900: Loss = 0.6388268427646461\n",
      "Iteration 136000: Loss = 0.6387969613194299\n",
      "Iteration 136100: Loss = 0.6387670888037092\n",
      "Iteration 136200: Loss = 0.6387372252128782\n",
      "Iteration 136300: Loss = 0.6387073705423337\n",
      "Iteration 136400: Loss = 0.6386775247874749\n",
      "Iteration 136500: Loss = 0.6386476879437041\n",
      "Iteration 136600: Loss = 0.6386178600064264\n",
      "Iteration 136700: Loss = 0.6385880409710496\n",
      "Iteration 136800: Loss = 0.6385582308329842\n",
      "Iteration 136900: Loss = 0.6385284295876439\n",
      "Iteration 137000: Loss = 0.6384986372304444\n",
      "Iteration 137100: Loss = 0.6384688537568053\n",
      "Iteration 137200: Loss = 0.638439079162148\n",
      "Iteration 137300: Loss = 0.638409313441897\n",
      "Iteration 137400: Loss = 0.6383795565914798\n",
      "Iteration 137500: Loss = 0.6383498086063265\n",
      "Iteration 137600: Loss = 0.6383200694818697\n",
      "Iteration 137700: Loss = 0.6382903392135455\n",
      "Iteration 137800: Loss = 0.6382606177967919\n",
      "Iteration 137900: Loss = 0.6382309052270503\n",
      "Iteration 138000: Loss = 0.6382012014997647\n",
      "Iteration 138100: Loss = 0.6381715066103818\n",
      "Iteration 138200: Loss = 0.638141820554351\n",
      "Iteration 138300: Loss = 0.6381121433271247\n",
      "Iteration 138400: Loss = 0.6380824749241579\n",
      "Iteration 138500: Loss = 0.6380528153409083\n",
      "Iteration 138600: Loss = 0.6380231645728365\n",
      "Iteration 138700: Loss = 0.6379935226154058\n",
      "Iteration 138800: Loss = 0.6379638894640823\n",
      "Iteration 138900: Loss = 0.6379342651143349\n",
      "Iteration 139000: Loss = 0.6379046495616351\n",
      "Iteration 139100: Loss = 0.6378750428014571\n",
      "Iteration 139200: Loss = 0.6378454448292781\n",
      "Iteration 139300: Loss = 0.6378158556405781\n",
      "Iteration 139400: Loss = 0.6377862752308393\n",
      "Iteration 139500: Loss = 0.6377567035955471\n",
      "Iteration 139600: Loss = 0.6377271407301898\n",
      "Iteration 139700: Loss = 0.6376975866302581\n",
      "Iteration 139800: Loss = 0.6376680412912454\n",
      "Iteration 139900: Loss = 0.6376385047086479\n",
      "Iteration 140000: Loss = 0.637608976877965\n",
      "Iteration 140100: Loss = 0.6375794577946983\n",
      "Iteration 140200: Loss = 0.637549947454352\n",
      "Iteration 140300: Loss = 0.6375204458524338\n",
      "Iteration 140400: Loss = 0.6374909529844531\n",
      "Iteration 140500: Loss = 0.6374614688459231\n",
      "Iteration 140600: Loss = 0.6374319934323588\n",
      "Iteration 140700: Loss = 0.6374025267392787\n",
      "Iteration 140800: Loss = 0.6373730687622033\n",
      "Iteration 140900: Loss = 0.6373436194966564\n",
      "Iteration 141000: Loss = 0.6373141789381642\n",
      "Iteration 141100: Loss = 0.6372847470822558\n",
      "Iteration 141200: Loss = 0.6372553239244627\n",
      "Iteration 141300: Loss = 0.6372259094603198\n",
      "Iteration 141400: Loss = 0.6371965036853637\n",
      "Iteration 141500: Loss = 0.6371671065951348\n",
      "Iteration 141600: Loss = 0.6371377181851754\n",
      "Iteration 141700: Loss = 0.6371083384510307\n",
      "Iteration 141800: Loss = 0.637078967388249\n",
      "Iteration 141900: Loss = 0.6370496049923808\n",
      "Iteration 142000: Loss = 0.6370202512589794\n",
      "Iteration 142100: Loss = 0.6369909061836014\n",
      "Iteration 142200: Loss = 0.636961569761805\n",
      "Iteration 142300: Loss = 0.6369322419891523\n",
      "Iteration 142400: Loss = 0.6369029228612071\n",
      "Iteration 142500: Loss = 0.6368736123735363\n",
      "Iteration 142600: Loss = 0.6368443105217098\n",
      "Iteration 142700: Loss = 0.6368150173012997\n",
      "Iteration 142800: Loss = 0.6367857327078811\n",
      "Iteration 142900: Loss = 0.6367564567370315\n",
      "Iteration 143000: Loss = 0.6367271893843315\n",
      "Iteration 143100: Loss = 0.636697930645364\n",
      "Iteration 143200: Loss = 0.6366686805157147\n",
      "Iteration 143300: Loss = 0.6366394389909723\n",
      "Iteration 143400: Loss = 0.6366102060667276\n",
      "Iteration 143500: Loss = 0.6365809817385745\n",
      "Iteration 143600: Loss = 0.6365517660021095\n",
      "Iteration 143700: Loss = 0.6365225588529317\n",
      "Iteration 143800: Loss = 0.6364933602866427\n",
      "Iteration 143900: Loss = 0.6364641702988475\n",
      "Iteration 144000: Loss = 0.6364349888851528\n",
      "Iteration 144100: Loss = 0.6364058160411686\n",
      "Iteration 144200: Loss = 0.6363766517625076\n",
      "Iteration 144300: Loss = 0.6363474960447845\n",
      "Iteration 144400: Loss = 0.6363183488836176\n",
      "Iteration 144500: Loss = 0.6362892102746271\n",
      "Iteration 144600: Loss = 0.6362600802134364\n",
      "Iteration 144700: Loss = 0.6362309586956711\n",
      "Iteration 144800: Loss = 0.6362018457169598\n",
      "Iteration 144900: Loss = 0.6361727412729336\n",
      "Iteration 145000: Loss = 0.6361436453592263\n",
      "Iteration 145100: Loss = 0.6361145579714744\n",
      "Iteration 145200: Loss = 0.636085479105317\n",
      "Iteration 145300: Loss = 0.6360564087563956\n",
      "Iteration 145400: Loss = 0.6360273469203549\n",
      "Iteration 145500: Loss = 0.635998293592842\n",
      "Iteration 145600: Loss = 0.6359692487695064\n",
      "Iteration 145700: Loss = 0.6359402124460006\n",
      "Iteration 145800: Loss = 0.6359111846179795\n",
      "Iteration 145900: Loss = 0.6358821652811005\n",
      "Iteration 146000: Loss = 0.6358531544310244\n",
      "Iteration 146100: Loss = 0.6358241520634137\n",
      "Iteration 146200: Loss = 0.6357951581739341\n",
      "Iteration 146300: Loss = 0.6357661727582535\n",
      "Iteration 146400: Loss = 0.6357371958120432\n",
      "Iteration 146500: Loss = 0.6357082273309762\n",
      "Iteration 146600: Loss = 0.6356792673107289\n",
      "Iteration 146700: Loss = 0.6356503157469799\n",
      "Iteration 146800: Loss = 0.6356213726354105\n",
      "Iteration 146900: Loss = 0.6355924379717046\n",
      "Iteration 147000: Loss = 0.6355635117515488\n",
      "Iteration 147100: Loss = 0.6355345939706325\n",
      "Iteration 147200: Loss = 0.6355056846246473\n",
      "Iteration 147300: Loss = 0.6354767837092877\n",
      "Iteration 147400: Loss = 0.6354478912202508\n",
      "Iteration 147500: Loss = 0.6354190071532363\n",
      "Iteration 147600: Loss = 0.6353901315039465\n",
      "Iteration 147700: Loss = 0.6353612642680864\n",
      "Iteration 147800: Loss = 0.6353324054413633\n",
      "Iteration 147900: Loss = 0.6353035550194873\n",
      "Iteration 148000: Loss = 0.6352747129981714\n",
      "Iteration 148100: Loss = 0.6352458793731307\n",
      "Iteration 148200: Loss = 0.6352170541400834\n",
      "Iteration 148300: Loss = 0.6351882372947499\n",
      "Iteration 148400: Loss = 0.6351594288328534\n",
      "Iteration 148500: Loss = 0.6351306287501195\n",
      "Iteration 148600: Loss = 0.6351018370422767\n",
      "Iteration 148700: Loss = 0.6350730537050562\n",
      "Iteration 148800: Loss = 0.6350442787341911\n",
      "Iteration 148900: Loss = 0.6350155121254178\n",
      "Iteration 149000: Loss = 0.634986753874475\n",
      "Iteration 149100: Loss = 0.6349580039771039\n",
      "Iteration 149200: Loss = 0.6349292624290485\n",
      "Iteration 149300: Loss = 0.6349005292260554\n",
      "Iteration 149400: Loss = 0.6348718043638737\n",
      "Iteration 149500: Loss = 0.6348430878382549\n",
      "Iteration 149600: Loss = 0.6348143796449534\n",
      "Iteration 149700: Loss = 0.6347856797797261\n",
      "Iteration 149800: Loss = 0.634756988238332\n",
      "Iteration 149900: Loss = 0.6347283050165338\n",
      "Iteration 150000: Loss = 0.6346996301100954\n",
      "Iteration 150100: Loss = 0.6346709635147845\n",
      "Iteration 150200: Loss = 0.6346423052263704\n",
      "Iteration 150300: Loss = 0.6346136552406256\n",
      "Iteration 150400: Loss = 0.634585013553325\n",
      "Iteration 150500: Loss = 0.634556380160246\n",
      "Iteration 150600: Loss = 0.6345277550571685\n",
      "Iteration 150700: Loss = 0.6344991382398752\n",
      "Iteration 150800: Loss = 0.6344705297041512\n",
      "Iteration 150900: Loss = 0.6344419294457843\n",
      "Iteration 151000: Loss = 0.6344133374605645\n",
      "Iteration 151100: Loss = 0.6343847537442849\n",
      "Iteration 151200: Loss = 0.6343561782927407\n",
      "Iteration 151300: Loss = 0.6343276111017301\n",
      "Iteration 151400: Loss = 0.6342990521670533\n",
      "Iteration 151500: Loss = 0.6342705014845132\n",
      "Iteration 151600: Loss = 0.6342419590499159\n",
      "Iteration 151700: Loss = 0.6342134248590692\n",
      "Iteration 151800: Loss = 0.6341848989077838\n",
      "Iteration 151900: Loss = 0.6341563811918732\n",
      "Iteration 152000: Loss = 0.634127871707153\n",
      "Iteration 152100: Loss = 0.6340993704494415\n",
      "Iteration 152200: Loss = 0.6340708774145597\n",
      "Iteration 152300: Loss = 0.6340423925983307\n",
      "Iteration 152400: Loss = 0.634013915996581\n",
      "Iteration 152500: Loss = 0.6339854476051388\n",
      "Iteration 152600: Loss = 0.6339569874198349\n",
      "Iteration 152700: Loss = 0.6339285354365033\n",
      "Iteration 152800: Loss = 0.6339000916509798\n",
      "Iteration 152900: Loss = 0.6338716560591031\n",
      "Iteration 153000: Loss = 0.6338432286567145\n",
      "Iteration 153100: Loss = 0.6338148094396576\n",
      "Iteration 153200: Loss = 0.6337863984037788\n",
      "Iteration 153300: Loss = 0.6337579955449265\n",
      "Iteration 153400: Loss = 0.633729600858952\n",
      "Iteration 153500: Loss = 0.6337012143417093\n",
      "Iteration 153600: Loss = 0.6336728359890547\n",
      "Iteration 153700: Loss = 0.633644465796847\n",
      "Iteration 153800: Loss = 0.6336161037609476\n",
      "Iteration 153900: Loss = 0.6335877498772201\n",
      "Iteration 154000: Loss = 0.6335594041415311\n",
      "Iteration 154100: Loss = 0.6335310665497496\n",
      "Iteration 154200: Loss = 0.6335027370977468\n",
      "Iteration 154300: Loss = 0.6334744157813967\n",
      "Iteration 154400: Loss = 0.6334461025965759\n",
      "Iteration 154500: Loss = 0.6334177975391629\n",
      "Iteration 154600: Loss = 0.6333895006050395\n",
      "Iteration 154700: Loss = 0.6333612117900896\n",
      "Iteration 154800: Loss = 0.6333329310901997\n",
      "Iteration 154900: Loss = 0.6333046585012585\n",
      "Iteration 155000: Loss = 0.6332763940191576\n",
      "Iteration 155100: Loss = 0.6332481376397909\n",
      "Iteration 155200: Loss = 0.6332198893590552\n",
      "Iteration 155300: Loss = 0.6331916491728489\n",
      "Iteration 155400: Loss = 0.6331634170770737\n",
      "Iteration 155500: Loss = 0.6331351930676333\n",
      "Iteration 155600: Loss = 0.6331069771404346\n",
      "Iteration 155700: Loss = 0.633078769291386\n",
      "Iteration 155800: Loss = 0.633050569516399\n",
      "Iteration 155900: Loss = 0.6330223778113879\n",
      "Iteration 156000: Loss = 0.6329941941722685\n",
      "Iteration 156100: Loss = 0.63296601859496\n",
      "Iteration 156200: Loss = 0.6329378510753835\n",
      "Iteration 156300: Loss = 0.632909691609463\n",
      "Iteration 156400: Loss = 0.6328815401931247\n",
      "Iteration 156500: Loss = 0.6328533968222974\n",
      "Iteration 156600: Loss = 0.6328252614929122\n",
      "Iteration 156700: Loss = 0.632797134200903\n",
      "Iteration 156800: Loss = 0.6327690149422057\n",
      "Iteration 156900: Loss = 0.6327409037127595\n",
      "Iteration 157000: Loss = 0.6327128005085048\n",
      "Iteration 157100: Loss = 0.6326847053253856\n",
      "Iteration 157200: Loss = 0.6326566181593479\n",
      "Iteration 157300: Loss = 0.6326285390063404\n",
      "Iteration 157400: Loss = 0.6326004678623139\n",
      "Iteration 157500: Loss = 0.632572404723222\n",
      "Iteration 157600: Loss = 0.63254434958502\n",
      "Iteration 157700: Loss = 0.6325163024436671\n",
      "Iteration 157800: Loss = 0.6324882632951236\n",
      "Iteration 157900: Loss = 0.6324602321353531\n",
      "Iteration 158000: Loss = 0.6324322089603209\n",
      "Iteration 158100: Loss = 0.6324041937659958\n",
      "Iteration 158200: Loss = 0.6323761865483478\n",
      "Iteration 158300: Loss = 0.6323481873033504\n",
      "Iteration 158400: Loss = 0.6323201960269789\n",
      "Iteration 158500: Loss = 0.6322922127152114\n",
      "Iteration 158600: Loss = 0.6322642373640284\n",
      "Iteration 158700: Loss = 0.6322362699694126\n",
      "Iteration 158800: Loss = 0.6322083105273493\n",
      "Iteration 158900: Loss = 0.6321803590338266\n",
      "Iteration 159000: Loss = 0.6321524154848341\n",
      "Iteration 159100: Loss = 0.632124479876365\n",
      "Iteration 159200: Loss = 0.6320965522044141\n",
      "Iteration 159300: Loss = 0.6320686324649788\n",
      "Iteration 159400: Loss = 0.6320407206540593\n",
      "Iteration 159500: Loss = 0.6320128167676577\n",
      "Iteration 159600: Loss = 0.6319849208017789\n",
      "Iteration 159700: Loss = 0.6319570327524302\n",
      "Iteration 159800: Loss = 0.6319291526156211\n",
      "Iteration 159900: Loss = 0.6319012803873638\n",
      "Iteration 160000: Loss = 0.6318734160636728\n",
      "Iteration 160100: Loss = 0.6318455596405648\n",
      "Iteration 160200: Loss = 0.6318177111140595\n",
      "Iteration 160300: Loss = 0.6317898704801783\n",
      "Iteration 160400: Loss = 0.6317620377349458\n",
      "Iteration 160500: Loss = 0.631734212874388\n",
      "Iteration 160600: Loss = 0.6317063958945346\n",
      "Iteration 160700: Loss = 0.6316785867914165\n",
      "Iteration 160800: Loss = 0.6316507855610679\n",
      "Iteration 160900: Loss = 0.6316229921995247\n",
      "Iteration 161000: Loss = 0.6315952067028259\n",
      "Iteration 161100: Loss = 0.6315674290670122\n",
      "Iteration 161200: Loss = 0.6315396592881274\n",
      "Iteration 161300: Loss = 0.6315118973622172\n",
      "Iteration 161400: Loss = 0.63148414328533\n",
      "Iteration 161500: Loss = 0.6314563970535164\n",
      "Iteration 161600: Loss = 0.6314286586628295\n",
      "Iteration 161700: Loss = 0.6314009281093247\n",
      "Iteration 161800: Loss = 0.63137320538906\n",
      "Iteration 161900: Loss = 0.6313454904980957\n",
      "Iteration 162000: Loss = 0.6313177834324943\n",
      "Iteration 162100: Loss = 0.631290084188321\n",
      "Iteration 162200: Loss = 0.6312623927616432\n",
      "Iteration 162300: Loss = 0.6312347091485307\n",
      "Iteration 162400: Loss = 0.6312070333450559\n",
      "Iteration 162500: Loss = 0.6311793653472932\n",
      "Iteration 162600: Loss = 0.6311517051513198\n",
      "Iteration 162700: Loss = 0.6311240527532149\n",
      "Iteration 162800: Loss = 0.6310964081490603\n",
      "Iteration 162900: Loss = 0.6310687713349404\n",
      "Iteration 163000: Loss = 0.6310411423069416\n",
      "Iteration 163100: Loss = 0.6310135210611526\n",
      "Iteration 163200: Loss = 0.630985907593665\n",
      "Iteration 163300: Loss = 0.6309583019005721\n",
      "Iteration 163400: Loss = 0.6309307039779702\n",
      "Iteration 163500: Loss = 0.6309031138219579\n",
      "Iteration 163600: Loss = 0.6308755314286354\n",
      "Iteration 163700: Loss = 0.6308479567941063\n",
      "Iteration 163800: Loss = 0.6308203899144761\n",
      "Iteration 163900: Loss = 0.6307928307858525\n",
      "Iteration 164000: Loss = 0.6307652794043458\n",
      "Iteration 164100: Loss = 0.6307377357660686\n",
      "Iteration 164200: Loss = 0.630710199867136\n",
      "Iteration 164300: Loss = 0.6306826717036651\n",
      "Iteration 164400: Loss = 0.6306551512717758\n",
      "Iteration 164500: Loss = 0.6306276385675901\n",
      "Iteration 164600: Loss = 0.6306001335872324\n",
      "Iteration 164700: Loss = 0.6305726363268295\n",
      "Iteration 164800: Loss = 0.6305451467825104\n",
      "Iteration 164900: Loss = 0.6305176649504067\n",
      "Iteration 165000: Loss = 0.6304901908266521\n",
      "Iteration 165100: Loss = 0.6304627244073828\n",
      "Iteration 165200: Loss = 0.6304352656887373\n",
      "Iteration 165300: Loss = 0.6304078146668566\n",
      "Iteration 165400: Loss = 0.6303803713378836\n",
      "Iteration 165500: Loss = 0.6303529356979641\n",
      "Iteration 165600: Loss = 0.6303255077432461\n",
      "Iteration 165700: Loss = 0.6302980874698795\n",
      "Iteration 165800: Loss = 0.6302706748740169\n",
      "Iteration 165900: Loss = 0.6302432699518135\n",
      "Iteration 166000: Loss = 0.6302158726994264\n",
      "Iteration 166100: Loss = 0.6301884831130149\n",
      "Iteration 166200: Loss = 0.6301611011887412\n",
      "Iteration 166300: Loss = 0.6301337269227696\n",
      "Iteration 166400: Loss = 0.6301063603112665\n",
      "Iteration 166500: Loss = 0.630079001350401\n",
      "Iteration 166600: Loss = 0.630051650036344\n",
      "Iteration 166700: Loss = 0.6300243063652693\n",
      "Iteration 166800: Loss = 0.6299969703333526\n",
      "Iteration 166900: Loss = 0.6299696419367722\n",
      "Iteration 167000: Loss = 0.6299423211717087\n",
      "Iteration 167100: Loss = 0.6299150080343449\n",
      "Iteration 167200: Loss = 0.6298877025208658\n",
      "Iteration 167300: Loss = 0.6298604046274591\n",
      "Iteration 167400: Loss = 0.6298331143503144\n",
      "Iteration 167500: Loss = 0.629805831685624\n",
      "Iteration 167600: Loss = 0.629778556629582\n",
      "Iteration 167700: Loss = 0.6297512891783855\n",
      "Iteration 167800: Loss = 0.6297240293282332\n",
      "Iteration 167900: Loss = 0.6296967770753268\n",
      "Iteration 168000: Loss = 0.6296695324158695\n",
      "Iteration 168100: Loss = 0.6296422953460677\n",
      "Iteration 168200: Loss = 0.6296150658621296\n",
      "Iteration 168300: Loss = 0.6295878439602656\n",
      "Iteration 168400: Loss = 0.6295606296366886\n",
      "Iteration 168500: Loss = 0.6295334228876136\n",
      "Iteration 168600: Loss = 0.6295062237092585\n",
      "Iteration 168700: Loss = 0.6294790320978427\n",
      "Iteration 168800: Loss = 0.6294518480495883\n",
      "Iteration 168900: Loss = 0.6294246715607198\n",
      "Iteration 169000: Loss = 0.6293975026274639\n",
      "Iteration 169100: Loss = 0.6293703412460493\n",
      "Iteration 169200: Loss = 0.6293431874127073\n",
      "Iteration 169300: Loss = 0.6293160411236715\n",
      "Iteration 169400: Loss = 0.6292889023751775\n",
      "Iteration 169500: Loss = 0.6292617711634637\n",
      "Iteration 169600: Loss = 0.6292346474847701\n",
      "Iteration 169700: Loss = 0.6292075313353398\n",
      "Iteration 169800: Loss = 0.6291804227114174\n",
      "Iteration 169900: Loss = 0.6291533216092502\n",
      "Iteration 170000: Loss = 0.6291262280250878\n",
      "Iteration 170100: Loss = 0.6290991419551817\n",
      "Iteration 170200: Loss = 0.629072063395786\n",
      "Iteration 170300: Loss = 0.6290449923431575\n",
      "Iteration 170400: Loss = 0.6290179287935541\n",
      "Iteration 170500: Loss = 0.6289908727432372\n",
      "Iteration 170600: Loss = 0.6289638241884695\n",
      "Iteration 170700: Loss = 0.6289367831255169\n",
      "Iteration 170800: Loss = 0.6289097495506466\n",
      "Iteration 170900: Loss = 0.6288827234601287\n",
      "Iteration 171000: Loss = 0.6288557048502356\n",
      "Iteration 171100: Loss = 0.6288286937172416\n",
      "Iteration 171200: Loss = 0.6288016900574233\n",
      "Iteration 171300: Loss = 0.6287746938670599\n",
      "Iteration 171400: Loss = 0.6287477051424323\n",
      "Iteration 171500: Loss = 0.6287207238798245\n",
      "Iteration 171600: Loss = 0.6286937500755218\n",
      "Iteration 171700: Loss = 0.6286667837258125\n",
      "Iteration 171800: Loss = 0.6286398248269868\n",
      "Iteration 171900: Loss = 0.6286128733753371\n",
      "Iteration 172000: Loss = 0.6285859293671583\n",
      "Iteration 172100: Loss = 0.6285589927987473\n",
      "Iteration 172200: Loss = 0.6285320636664035\n",
      "Iteration 172300: Loss = 0.6285051419664281\n",
      "Iteration 172400: Loss = 0.6284782276951254\n",
      "Iteration 172500: Loss = 0.6284513208488008\n",
      "Iteration 172600: Loss = 0.6284244214237631\n",
      "Iteration 172700: Loss = 0.6283975294163222\n",
      "Iteration 172800: Loss = 0.6283706448227913\n",
      "Iteration 172900: Loss = 0.6283437676394851\n",
      "Iteration 173000: Loss = 0.6283168978627206\n",
      "Iteration 173100: Loss = 0.6282900354888178\n",
      "Iteration 173200: Loss = 0.6282631805140977\n",
      "Iteration 173300: Loss = 0.6282363329348848\n",
      "Iteration 173400: Loss = 0.6282094927475047\n",
      "Iteration 173500: Loss = 0.628182659948286\n",
      "Iteration 173600: Loss = 0.6281558345335594\n",
      "Iteration 173700: Loss = 0.6281290164996575\n",
      "Iteration 173800: Loss = 0.6281022058429153\n",
      "Iteration 173900: Loss = 0.6280754025596702\n",
      "Iteration 174000: Loss = 0.6280486066462615\n",
      "Iteration 174100: Loss = 0.6280218180990311\n",
      "Iteration 174200: Loss = 0.6279950369143228\n",
      "Iteration 174300: Loss = 0.6279682630884826\n",
      "Iteration 174400: Loss = 0.6279414966178591\n",
      "Iteration 174500: Loss = 0.6279147374988028\n",
      "Iteration 174600: Loss = 0.6278879857276662\n",
      "Iteration 174700: Loss = 0.6278612413008049\n",
      "Iteration 174800: Loss = 0.6278345042145755\n",
      "Iteration 174900: Loss = 0.6278077744653378\n",
      "Iteration 175000: Loss = 0.627781052049453\n",
      "Iteration 175100: Loss = 0.6277543369632853\n",
      "Iteration 175200: Loss = 0.6277276292032005\n",
      "Iteration 175300: Loss = 0.627700928765567\n",
      "Iteration 175400: Loss = 0.6276742356467553\n",
      "Iteration 175500: Loss = 0.6276475498431379\n",
      "Iteration 175600: Loss = 0.6276208713510895\n",
      "Iteration 175700: Loss = 0.6275942001669874\n",
      "Iteration 175800: Loss = 0.6275675362872107\n",
      "Iteration 175900: Loss = 0.627540879708141\n",
      "Iteration 176000: Loss = 0.6275142304261616\n",
      "Iteration 176100: Loss = 0.6274875884376585\n",
      "Iteration 176200: Loss = 0.6274609537390199\n",
      "Iteration 176300: Loss = 0.6274343263266358\n",
      "Iteration 176400: Loss = 0.6274077061968985\n",
      "Iteration 176500: Loss = 0.6273810933462027\n",
      "Iteration 176600: Loss = 0.6273544877709453\n",
      "Iteration 176700: Loss = 0.6273278894675248\n",
      "Iteration 176800: Loss = 0.627301298432343\n",
      "Iteration 176900: Loss = 0.6272747146618026\n",
      "Iteration 177000: Loss = 0.6272481381523095\n",
      "Iteration 177100: Loss = 0.6272215689002711\n",
      "Iteration 177200: Loss = 0.6271950069020975\n",
      "Iteration 177300: Loss = 0.6271684521542005\n",
      "Iteration 177400: Loss = 0.6271419046529945\n",
      "Iteration 177500: Loss = 0.6271153643948957\n",
      "Iteration 177600: Loss = 0.6270888313763225\n",
      "Iteration 177700: Loss = 0.6270623055936962\n",
      "Iteration 177800: Loss = 0.6270357870434392\n",
      "Iteration 177900: Loss = 0.6270092757219768\n",
      "Iteration 178000: Loss = 0.6269827716257361\n",
      "Iteration 178100: Loss = 0.6269562747511466\n",
      "Iteration 178200: Loss = 0.6269297850946397\n",
      "Iteration 178300: Loss = 0.6269033026526493\n",
      "Iteration 178400: Loss = 0.6268768274216111\n",
      "Iteration 178500: Loss = 0.6268503593979633\n",
      "Iteration 178600: Loss = 0.626823898578146\n",
      "Iteration 178700: Loss = 0.6267974449586016\n",
      "Iteration 178800: Loss = 0.6267709985357746\n",
      "Iteration 178900: Loss = 0.6267445593061118\n",
      "Iteration 179000: Loss = 0.6267181272660619\n",
      "Iteration 179100: Loss = 0.6266917024120758\n",
      "Iteration 179200: Loss = 0.6266652847406068\n",
      "Iteration 179300: Loss = 0.62663887424811\n",
      "Iteration 179400: Loss = 0.6266124709310429\n",
      "Iteration 179500: Loss = 0.6265860747858654\n",
      "Iteration 179600: Loss = 0.6265596858090384\n",
      "Iteration 179700: Loss = 0.6265333039970267\n",
      "Iteration 179800: Loss = 0.6265069293462957\n",
      "Iteration 179900: Loss = 0.6264805618533137\n",
      "Iteration 180000: Loss = 0.6264542015145511\n",
      "Iteration 180100: Loss = 0.62642784832648\n",
      "Iteration 180200: Loss = 0.6264015022855753\n",
      "Iteration 180300: Loss = 0.6263751633883138\n",
      "Iteration 180400: Loss = 0.6263488316311739\n",
      "Iteration 180500: Loss = 0.6263225070106369\n",
      "Iteration 180600: Loss = 0.6262961895231857\n",
      "Iteration 180700: Loss = 0.6262698791653056\n",
      "Iteration 180800: Loss = 0.6262435759334839\n",
      "Iteration 180900: Loss = 0.6262172798242103\n",
      "Iteration 181000: Loss = 0.6261909908339761\n",
      "Iteration 181100: Loss = 0.6261647089592753\n",
      "Iteration 181200: Loss = 0.6261384341966038\n",
      "Iteration 181300: Loss = 0.6261121665424592\n",
      "Iteration 181400: Loss = 0.6260859059933419\n",
      "Iteration 181500: Loss = 0.626059652545754\n",
      "Iteration 181600: Loss = 0.6260334061962001\n",
      "Iteration 181700: Loss = 0.6260071669411863\n",
      "Iteration 181800: Loss = 0.6259809347772214\n",
      "Iteration 181900: Loss = 0.6259547097008161\n",
      "Iteration 182000: Loss = 0.6259284917084831\n",
      "Iteration 182100: Loss = 0.6259022807967375\n",
      "Iteration 182200: Loss = 0.6258760769620959\n",
      "Iteration 182300: Loss = 0.6258498802010778\n",
      "Iteration 182400: Loss = 0.6258236905102045\n",
      "Iteration 182500: Loss = 0.6257975078859991\n",
      "Iteration 182600: Loss = 0.6257713323249873\n",
      "Iteration 182700: Loss = 0.6257451638236963\n",
      "Iteration 182800: Loss = 0.6257190023786561\n",
      "Iteration 182900: Loss = 0.6256928479863983\n",
      "Iteration 183000: Loss = 0.6256667006434569\n",
      "Iteration 183100: Loss = 0.6256405603463677\n",
      "Iteration 183200: Loss = 0.625614427091669\n",
      "Iteration 183300: Loss = 0.6255883008759004\n",
      "Iteration 183400: Loss = 0.625562181695605\n",
      "Iteration 183500: Loss = 0.6255360695473264\n",
      "Iteration 183600: Loss = 0.6255099644276114\n",
      "Iteration 183700: Loss = 0.6254838663330083\n",
      "Iteration 183800: Loss = 0.625457775260068\n",
      "Iteration 183900: Loss = 0.6254316912053431\n",
      "Iteration 184000: Loss = 0.6254056141653882\n",
      "Iteration 184100: Loss = 0.6253795441367604\n",
      "Iteration 184200: Loss = 0.6253534811160185\n",
      "Iteration 184300: Loss = 0.6253274250997237\n",
      "Iteration 184400: Loss = 0.6253013760844388\n",
      "Iteration 184500: Loss = 0.6252753340667296\n",
      "Iteration 184600: Loss = 0.6252492990431627\n",
      "Iteration 184700: Loss = 0.6252232710103081\n",
      "Iteration 184800: Loss = 0.6251972499647367\n",
      "Iteration 184900: Loss = 0.6251712359030223\n",
      "Iteration 185000: Loss = 0.6251452288217403\n",
      "Iteration 185100: Loss = 0.6251192287174684\n",
      "Iteration 185200: Loss = 0.6250932355867865\n",
      "Iteration 185300: Loss = 0.6250672494262763\n",
      "Iteration 185400: Loss = 0.6250412702325217\n",
      "Iteration 185500: Loss = 0.6250152980021084\n",
      "Iteration 185600: Loss = 0.6249893327316247\n",
      "Iteration 185700: Loss = 0.6249633744176606\n",
      "Iteration 185800: Loss = 0.624937423056808\n",
      "Iteration 185900: Loss = 0.6249114786456613\n",
      "Iteration 186000: Loss = 0.6248855411808167\n",
      "Iteration 186100: Loss = 0.6248596106588725\n",
      "Iteration 186200: Loss = 0.6248336870764292\n",
      "Iteration 186300: Loss = 0.624807770430089\n",
      "Iteration 186400: Loss = 0.6247818607164564\n",
      "Iteration 186500: Loss = 0.6247559579321381\n",
      "Iteration 186600: Loss = 0.6247300620737424\n",
      "Iteration 186700: Loss = 0.6247041731378802\n",
      "Iteration 186800: Loss = 0.6246782911211641\n",
      "Iteration 186900: Loss = 0.6246524160202088\n",
      "Iteration 187000: Loss = 0.6246265478316311\n",
      "Iteration 187100: Loss = 0.6246006865520497\n",
      "Iteration 187200: Loss = 0.624574832178086\n",
      "Iteration 187300: Loss = 0.6245489847063621\n",
      "Iteration 187400: Loss = 0.6245231441335035\n",
      "Iteration 187500: Loss = 0.6244973104561371\n",
      "Iteration 187600: Loss = 0.6244714836708919\n",
      "Iteration 187700: Loss = 0.6244456637743989\n",
      "Iteration 187800: Loss = 0.6244198507632914\n",
      "Iteration 187900: Loss = 0.6243940446342042\n",
      "Iteration 188000: Loss = 0.6243682453837751\n",
      "Iteration 188100: Loss = 0.6243424530086426\n",
      "Iteration 188200: Loss = 0.6243166675054485\n",
      "Iteration 188300: Loss = 0.6242908888708356\n",
      "Iteration 188400: Loss = 0.6242651171014497\n",
      "Iteration 188500: Loss = 0.6242393521939378\n",
      "Iteration 188600: Loss = 0.6242135941449495\n",
      "Iteration 188700: Loss = 0.6241878429511358\n",
      "Iteration 188800: Loss = 0.6241620986091506\n",
      "Iteration 188900: Loss = 0.6241363611156491\n",
      "Iteration 189000: Loss = 0.6241106304672888\n",
      "Iteration 189100: Loss = 0.624084906660729\n",
      "Iteration 189200: Loss = 0.6240591896926313\n",
      "Iteration 189300: Loss = 0.6240334795596594\n",
      "Iteration 189400: Loss = 0.6240077762584786\n",
      "Iteration 189500: Loss = 0.6239820797857566\n",
      "Iteration 189600: Loss = 0.6239563901381627\n",
      "Iteration 189700: Loss = 0.6239307073123688\n",
      "Iteration 189800: Loss = 0.6239050313050484\n",
      "Iteration 189900: Loss = 0.6238793621128769\n",
      "Iteration 190000: Loss = 0.6238536997325321\n",
      "Iteration 190100: Loss = 0.6238280441606937\n",
      "Iteration 190200: Loss = 0.623802395394043\n",
      "Iteration 190300: Loss = 0.6237767534292638\n",
      "Iteration 190400: Loss = 0.6237511182630417\n",
      "Iteration 190500: Loss = 0.6237254898920644\n",
      "Iteration 190600: Loss = 0.6236998683130215\n",
      "Iteration 190700: Loss = 0.6236742535226046\n",
      "Iteration 190800: Loss = 0.6236486455175073\n",
      "Iteration 190900: Loss = 0.6236230442944254\n",
      "Iteration 191000: Loss = 0.6235974498500563\n",
      "Iteration 191100: Loss = 0.6235718621810996\n",
      "Iteration 191200: Loss = 0.623546281284257\n",
      "Iteration 191300: Loss = 0.6235207071562323\n",
      "Iteration 191400: Loss = 0.6234951397937307\n",
      "Iteration 191500: Loss = 0.6234695791934601\n",
      "Iteration 191600: Loss = 0.62344402535213\n",
      "Iteration 191700: Loss = 0.623418478266452\n",
      "Iteration 191800: Loss = 0.6233929379331393\n",
      "Iteration 191900: Loss = 0.6233674043489079\n",
      "Iteration 192000: Loss = 0.6233418775104751\n",
      "Iteration 192100: Loss = 0.6233163574145604\n",
      "Iteration 192200: Loss = 0.6232908440578852\n",
      "Iteration 192300: Loss = 0.623265337437173\n",
      "Iteration 192400: Loss = 0.6232398375491495\n",
      "Iteration 192500: Loss = 0.6232143443905417\n",
      "Iteration 192600: Loss = 0.6231888579580792\n",
      "Iteration 192700: Loss = 0.6231633782484933\n",
      "Iteration 192800: Loss = 0.6231379052585174\n",
      "Iteration 192900: Loss = 0.6231124389848868\n",
      "Iteration 193000: Loss = 0.6230869794243388\n",
      "Iteration 193100: Loss = 0.6230615265736126\n",
      "Iteration 193200: Loss = 0.6230360804294494\n",
      "Iteration 193300: Loss = 0.6230106409885924\n",
      "Iteration 193400: Loss = 0.6229852082477868\n",
      "Iteration 193500: Loss = 0.6229597822037797\n",
      "Iteration 193600: Loss = 0.62293436285332\n",
      "Iteration 193700: Loss = 0.6229089501931591\n",
      "Iteration 193800: Loss = 0.6228835442200495\n",
      "Iteration 193900: Loss = 0.6228581449307466\n",
      "Iteration 194000: Loss = 0.6228327523220074\n",
      "Iteration 194100: Loss = 0.6228073663905903\n",
      "Iteration 194200: Loss = 0.6227819871332566\n",
      "Iteration 194300: Loss = 0.6227566145467687\n",
      "Iteration 194400: Loss = 0.6227312486278915\n",
      "Iteration 194500: Loss = 0.6227058893733918\n",
      "Iteration 194600: Loss = 0.6226805367800381\n",
      "Iteration 194700: Loss = 0.622655190844601\n",
      "Iteration 194800: Loss = 0.6226298515638532\n",
      "Iteration 194900: Loss = 0.622604518934569\n",
      "Iteration 195000: Loss = 0.6225791929535248\n",
      "Iteration 195100: Loss = 0.6225538736174993\n",
      "Iteration 195200: Loss = 0.6225285609232725\n",
      "Iteration 195300: Loss = 0.6225032548676268\n",
      "Iteration 195400: Loss = 0.6224779554473463\n",
      "Iteration 195500: Loss = 0.6224526626592173\n",
      "Iteration 195600: Loss = 0.6224273765000279\n",
      "Iteration 195700: Loss = 0.622402096966568\n",
      "Iteration 195800: Loss = 0.6223768240556294\n",
      "Iteration 195900: Loss = 0.6223515577640065\n",
      "Iteration 196000: Loss = 0.6223262980884945\n",
      "Iteration 196100: Loss = 0.6223010450258918\n",
      "Iteration 196200: Loss = 0.6222757985729974\n",
      "Iteration 196300: Loss = 0.6222505587266135\n",
      "Iteration 196400: Loss = 0.6222253254835434\n",
      "Iteration 196500: Loss = 0.6222000988405925\n",
      "Iteration 196600: Loss = 0.6221748787945685\n",
      "Iteration 196700: Loss = 0.6221496653422804\n",
      "Iteration 196800: Loss = 0.6221244584805397\n",
      "Iteration 196900: Loss = 0.6220992582061594\n",
      "Iteration 197000: Loss = 0.6220740645159547\n",
      "Iteration 197100: Loss = 0.6220488774067425\n",
      "Iteration 197200: Loss = 0.6220236968753418\n",
      "Iteration 197300: Loss = 0.6219985229185735\n",
      "Iteration 197400: Loss = 0.6219733555332604\n",
      "Iteration 197500: Loss = 0.621948194716227\n",
      "Iteration 197600: Loss = 0.6219230404643\n",
      "Iteration 197700: Loss = 0.621897892774308\n",
      "Iteration 197800: Loss = 0.6218727516430813\n",
      "Iteration 197900: Loss = 0.6218476170674523\n",
      "Iteration 198000: Loss = 0.6218224890442552\n",
      "Iteration 198100: Loss = 0.6217973675703261\n",
      "Iteration 198200: Loss = 0.621772252642503\n",
      "Iteration 198300: Loss = 0.6217471442576262\n",
      "Iteration 198400: Loss = 0.6217220424125373\n",
      "Iteration 198500: Loss = 0.6216969471040801\n",
      "Iteration 198600: Loss = 0.6216718583291002\n",
      "Iteration 198700: Loss = 0.6216467760844453\n",
      "Iteration 198800: Loss = 0.6216217003669648\n",
      "Iteration 198900: Loss = 0.62159663117351\n",
      "Iteration 199000: Loss = 0.6215715685009346\n",
      "Iteration 199100: Loss = 0.6215465123460931\n",
      "Iteration 199200: Loss = 0.6215214627058429\n",
      "Iteration 199300: Loss = 0.6214964195770428\n",
      "Iteration 199400: Loss = 0.6214713829565539\n",
      "Iteration 199500: Loss = 0.621446352841239\n",
      "Iteration 199600: Loss = 0.6214213292279621\n",
      "Iteration 199700: Loss = 0.6213963121135904\n",
      "Iteration 199800: Loss = 0.6213713014949919\n",
      "Iteration 199900: Loss = 0.621346297369037\n",
      "Accuracy: 66.67% for learning rate 0.0001 and iterations 200000\n"
     ]
    }
   ],
   "source": [
    "for l_r, iters in hyperparameters: #iterating through the values of learning rate and number of iterations\n",
    "    accuracy, model = train_and_evaluate(l_r, iters) #finding various values of accuracies for different hyperparameters\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pikcle to save the weights of the best model\n",
    "with open('ganeshdi_sushreen_assignment1_part2.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model.weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 89.86%\n"
     ]
    }
   ],
   "source": [
    "#printing the best accuracy\n",
    "print(f\"Best Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the loss over the iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model.losses)\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
